{
 "metadata": {
  "name": "",
  "signature": "sha256:5d011eb61fd43ef5dfe20aae589582e8a66b7f08f129e52e5fc9141fe9a50c27"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# W207 Final Project Submission\n",
      "### Ross Boberg, Sarah Neff, Sam Zaiss\n",
      "\n",
      "This notebook documents our exploration for the <a href=\"http://www.kaggle.com/c/random-acts-of-pizza\">Random Acts of Pizza</a> kaggle competition as part of the W207 Machine Learning course for UC Berkeley's MIDS program. We document the individual areas of exploration that we completed for this project, followed by the larger model that pulled these explorations together.\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "#### Table of Contents\n",
      "\n",
      "<ol>\n",
      "<li><a href=\"#part1\">Data Import and Base Methods</a></li>\n",
      "<li><a href=\"#part2\">Activity Features</a></li>\n",
      "<li><a href=\"#part3\">Text Bag of Words</a>\n",
      "<ul>\n",
      "<li>Simple</li>\n",
      "<li>L1 Feature Regularization</li>\n",
      "<li>Time</li><br/>\n",
      "</ul>\n",
      "</li>\n",
      "<li><a href=\"#part4\">Time Features</a></li>\n",
      "<li><a href=\"#part5\">Interesting Words &amp; Category Tags</a></li>\n",
      "<li><a href=\"#part6\">Spelling Mistakes</a></li>\n",
      "<li><a href=\"#part7\">Text Summary Features</a>\n",
      "<ul>\n",
      "<li>Text Length</li>\n",
      "<li>Word Case</li>\n",
      "<li>Paragraph Analysis</li><br/>\n",
      "</ul>\n",
      "</li>\n",
      "<li><a href=\"#part8\">Location Features</a></li>\n",
      "<li><a href=\"#part9\">Parts of Speech</a></li>\n",
      "<li><a href=\"#part10\">Final, Composite Model</a></li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part1\"></a>\n",
      "## 1. Data Import and Base Methods"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import pandas as pd\n",
      "import scipy as scipy\n",
      "import datetime as dt\n",
      "import time\n",
      "import re\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gensim\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import make_scorer\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "#useful for text processing\n",
      "from nltk import word_tokenize\n",
      "from nltk.tokenize import WhitespaceTokenizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.tokenize import RegexpTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Helper methods that will be used often in this notebook\n",
      "\n",
      "# Helper methods to pull and submit data:\n",
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "            \n",
      "            \n",
      "# Helper methods for pulling columns from the dataset:\n",
      "def name2index(df, names):\n",
      "    return_single = False\n",
      "    \n",
      "    if type(names) == type([]):\n",
      "       names = np.array(names)\n",
      "    elif type(names) != type(np.array([])):\n",
      "        names = np.array([names])\n",
      "        return_single = True \n",
      "    \n",
      "    inds = np.where(np.in1d(df.columns, np.array(names)))[0]\n",
      "    \n",
      "    return inds[0] if return_single else inds\n",
      "\n",
      "            \n",
      "# Helper methods that are focused on class rebalancing:\n",
      "def balance_samples(y, method='oversample'):\n",
      "    class_counts = np.bincount(y)\n",
      "    \n",
      "    maxi = np.argmax(class_counts)\n",
      "    new_idx = np.argwhere(y==maxi)\n",
      "    \n",
      "    for i in range(len(class_counts)):\n",
      "        if i != maxi:\n",
      "            mult = class_counts[maxi]/class_counts[i]\n",
      "            rem = class_counts[maxi] - class_counts[i]*mult\n",
      "            idxi = np.argwhere(y==i)\n",
      "            np.random.shuffle(idxi)\n",
      "            for j in range(mult):\n",
      "                new_idx = np.vstack((new_idx,idxi))\n",
      "            new_idx = np.vstack((new_idx,idxi[:rem]))\n",
      "        \n",
      "    np.random.shuffle(new_idx)\n",
      "\n",
      "    return np.reshape(new_idx, (new_idx.shape[0],))\n",
      "\n",
      "def oversample_kfold(kf, y):\n",
      "    kf_over = []\n",
      "    for ti, di in kf:\n",
      "        yt = y[ti]\n",
      "        ti_over = ti[balance_samples(yt)]\n",
      "        kf_over.append((ti_over, di))\n",
      "    return kf_over\n",
      "\n",
      "# Helper methods for printing and scoring:\n",
      "def test_kfolds(X, y, kf, model, verbose=1, balance=False):\n",
      "    roc_auc_list = []\n",
      "    \n",
      "    for train_i, dev_i in kf:\n",
      "        if balance:\n",
      "            train_i_orig = train_i\n",
      "            y_train = y[train_i_orig]\n",
      "            train_i = train_i_orig[balance_samples(y_train)]\n",
      "        \n",
      "        \n",
      "        X_train = X[train_i]\n",
      "        X_dev = X[dev_i]\n",
      "\n",
      "        model.fit(X_train, y[train_i])\n",
      "\n",
      "        dev_pred = model.predict(X_dev)\n",
      "        \n",
      "        roc_auc_i = roc_auc_score(y[dev_i], dev_pred)\n",
      "        roc_auc_list.append(roc_auc_i)\n",
      "        if verbose > 1:\n",
      "            print('ROC AUC:',roc_auc_i)\n",
      "            \n",
      "    if verbose > 0:\n",
      "        print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(kf), np.mean(roc_auc_list), np.median(roc_auc_list), np.std(roc_auc_list))\n",
      "        \n",
      "    return roc_auc_list\n",
      "\n",
      "def print_scores(scores):\n",
      "    print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(scores), np.mean(scores), np.median(scores), np.std(scores))\n",
      "\n",
      "\n",
      "# Lastly, some classes to handle string tokenizing that we will use in multiple sections:\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class SnowballStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = SnowballStemmer('english')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "class PorterStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class PuncTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.reg = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
      "    def __call__(self, doc):\n",
      "        return self.reg.tokenize(doc)\n",
      "    \n",
      "class SpaceTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tknzr = WhitespaceTokenizer()\n",
      "    def __call__(self, doc):\n",
      "        return [t for t in self.tknzr.tokenize(doc)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Set up the training and test data to work with throughout the notebook:\n",
      "\n",
      "# load data from JSON file as list of dicts\n",
      "all_train_dict_list = load_json_file('../data/train.json')\n",
      "submit_dict_list =  load_json_file('../data/test.json')\n",
      "\n",
      "n_all = len(all_train_dict_list)\n",
      "n_submit = len(submit_dict_list)\n",
      "\n",
      "# shuffle data to avoid biased split of train / dev data\n",
      "rand.shuffle(all_train_dict_list)\n",
      "\n",
      "# process labels\n",
      "all_train_labels = np.array([x['requester_received_pizza'] for x in all_train_dict_list])\n",
      "\n",
      "# pandas is useful for turning dicts in to matrix-like objects\n",
      "# where each column is an numpy array\n",
      "submit_df = pd.DataFrame(submit_dict_list)\n",
      "all_train_df = pd.DataFrame(all_train_dict_list)\n",
      "\n",
      "# limit train to columns available in submit_df\n",
      "submit_cols = submit_df.columns\n",
      "all_train_df = all_train_df[submit_cols]\n",
      "\n",
      "# useful for sklearn scoring\n",
      "roc_scorer = make_scorer(roc_auc_score)\n",
      "\n",
      "# set up kFolds\n",
      "kf = KFold(n_all, n_folds = 10)\n",
      "\n",
      "y = all_train_labels\n",
      "kf_over = []\n",
      "for ti, di in kf:\n",
      "    yt = y[ti]\n",
      "    ti_over = ti[balance_samples(yt)]\n",
      "    kf_over.append((ti_over, di))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part2\"></a>\n",
      "## 2. Activity Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Set up Numeric Activity Variables, including processor class for all activity feature analysis.\n",
      "\n",
      "ACTIVITY_VARS = ['requester_account_age_in_days_at_request',\n",
      "                'requester_days_since_first_post_on_raop_at_request',\n",
      "                'requester_number_of_comments_at_request',\n",
      "                'requester_number_of_comments_in_raop_at_request',\n",
      "                'requester_number_of_posts_at_request',\n",
      "                'requester_number_of_posts_on_raop_at_request',\n",
      "                'requester_number_of_subreddits_at_request',\n",
      "                'requester_upvotes_minus_downvotes_at_request',\n",
      "                'requester_upvotes_plus_downvotes_at_request'\n",
      "                ]\n",
      "ACTIVITY_COLUMNS = name2index(all_train_df, ACTIVITY_VARS)\n",
      "\n",
      "class ExtractColumnsTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, cols=[0]):\n",
      "        self.cols = cols\n",
      "        \n",
      "    def fit(self, *args, **kwargs):\n",
      "        return self\n",
      "        \n",
      "    def transform(self, X):\n",
      "        cols = self.cols\n",
      "        return X[:,cols]\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "class ExtractActivities(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, ACTIVITY_COLUMNS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Explore models using the activity features only.\n",
      "\n",
      "# The main concern here is weighting classes appropriately, so we do an investigation of different\n",
      "# approaches and see how well the resulting model performs on 5 k-folds of the training data.\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print 'Equal Class Weights'\n",
      "pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC())])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nReweighted Classes'\n",
      "wt_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC(class_weight='auto'))])\n",
      "print_scores(cross_val_score(wt_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nRebalanced Sample'\n",
      "rebal_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc',SVC())])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(rebal_pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Equal Class Weights\n",
        "N: 10, Mean: 0.509546, Median: 0.507994, SD: 0.008061"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Reweighted Classes\n",
        "N: 10, Mean: 0.555346, Median: 0.553994, SD: 0.025812"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Rebalanced Sample\n",
        "N: 10, Mean: 0.551692, Median: 0.552735, SD: 0.032479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part3\"></a>\n",
      "## 3. Bag of Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Define quick classes that we can use to isolate the title and body columns in our data.\n",
      "TITLE_COLUMN = name2index(all_train_df, 'request_title')\n",
      "BODY_COLUMN = name2index(all_train_df, 'request_text_edit_aware')\n",
      "\n",
      "class ExtractBody(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, BODY_COLUMN)\n",
      "        \n",
      "class ExtractTitle(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, TITLE_COLUMN)\n",
      "        \n",
      "class ExtractAllText(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, np.array([TITLE_COLUMN, BODY_COLUMN]))\n",
      "\n",
      "\n",
      "class ConcatStringTransformer(TransformerMixin):\n",
      "    def __init__(self):\n",
      "        return None\n",
      "        \n",
      "    def fit(self, *args, **kwargs):\n",
      "        return self\n",
      "        \n",
      "    def transform(self, X):\n",
      "        \n",
      "        if len(X.shape) == 1:\n",
      "            return X\n",
      "        else:\n",
      "            n_feat = X.shape[1]\n",
      "            new_list = []\n",
      "            for i in range(X.shape[0]):\n",
      "                new_list.append('.'.join(X[i,:]))\n",
      "            \n",
      "            return np.array(new_list)\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "# A simple text processing pipeline that we will use in the composite model:\n",
      "text_pipe = Pipeline([('union', ExtractAllText()), ('concat',ConcatStringTransformer())])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reusable method for quick BOW investigations:\n",
      "def simple_text(do_all=True, do_count=False,do_tfidf=False, do_titles=False, do_bodies=False, do_both=False, lowercase=False, tokenizer=None, stop_words=None):\n",
      "\n",
      "    # Notes\n",
      "    # results slightly better w/ lowercase = False (when unigrams only)\n",
      "    # bigrams added no value on unigrams\n",
      "\n",
      "    tv = TfidfVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    cv = CountVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "    \n",
      "    body_cv = Pipeline([('body',ExtractBody()),('cv', cv)])\n",
      "    body_tv = Pipeline([('body',ExtractBody()),('tv', tv)])\n",
      "    \n",
      "    title_cv = Pipeline([('title',ExtractTitle()),('cv', cv)])\n",
      "    title_tv = Pipeline([('title',ExtractTitle()),('tv', tv)])\n",
      "\n",
      "    if do_titles or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Titles\n",
      "            print '\\nCount Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer TItles\n",
      "            print '\\nTFIDF Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "    if do_bodies or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Bodies\n",
      "            print '\\nCount Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Bodies\n",
      "            print '\\nTFIDF Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        \n",
      "    if do_both or do_all:\n",
      "        if do_count or do_all:\n",
      "\n",
      "            # Count Vectorizer Titles and Bodies\n",
      "            print '\\nCount Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_cv),\n",
      "                    ('tranform_body',body_cv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "            \n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Titles and Bodies\n",
      "            print '\\nTFIDF Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_tv),\n",
      "                    ('tranform_body',body_tv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Experimentation with different vectorizers and tokenizers\n",
      "\n",
      "print \"Examination of best vectorizer across titles, bodies, or titles + bodies:\"\n",
      "simple_text(do_all = True)\n",
      "\n",
      "print \"==========\"\n",
      "print \"Examination of tfidf vectorizer on titles + bodies with Snowball Stem Tokenizer:\"\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=SnowballStemTokenizer())\n",
      "print \"Examination of tfidf vectorizer on titles + bodies with Lemma Tokenizer:\"\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Examination of best vectorizer across titles, bodies, or titles + bodies:\n",
        "\n",
        "Count Vectorizer on Titles\n",
        "N: 10, Mean: 0.519474, Median: 0.520740, SD: 0.024555"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles\n",
        "N: 10, Mean: 0.518167, Median: 0.521154, SD: 0.026890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Bodies\n",
        "N: 10, Mean: 0.538654, Median: 0.529119, SD: 0.021140"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Bodies\n",
        "N: 10, Mean: 0.546304, Median: 0.540143, SD: 0.019159"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.542050, Median: 0.546315, SD: 0.016378"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.534769, Median: 0.534403, SD: 0.028518"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "==========\n",
        "Examination of tfidf vectorizer on titles + bodies with Snowball Stem Tokenizer:\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.554693, Median: 0.543610, SD: 0.028433"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Examination of tfidf vectorizer on titles + bodies with Lemma Tokenizer:\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.548386, Median: 0.546751, SD: 0.030142"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Bag of Words\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>TF-IDF Vectorizer on Bodies Only</td>\n",
      "<td>0.5463</td>\n",
      "<td>0.5401</td>\n",
      "<td>0.0191</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part4\"></a>\n",
      "## 4. Time Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Time Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part5\"></a>\n",
      "## 5. Interesting Words & Category Tags"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part6\"></a>\n",
      "## 6. Spelling Mistakes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part7\"></a>\n",
      "## 7. Text Summary Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part8\"></a>\n",
      "## 8. Location Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part9\"></a>\n",
      "## 9. Parts of Speech"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part10\"></a>\n",
      "## 10. Final, Composite Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}