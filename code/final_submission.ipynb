{
 "metadata": {
  "name": "",
  "signature": "sha256:1bfe0bb81a22cc169d7aa73ef5d9b191c9068eb0cd6dc03191c8fb67b47b6f85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# W207 Final Project Submission\n",
      "### Ross Boberg, Sarah Neff, Sam Zaiss\n",
      "\n",
      "This notebook documents our exploration for the <a href=\"http://www.kaggle.com/c/random-acts-of-pizza\">Random Acts of Pizza</a> kaggle competition as part of the W207 Machine Learning course for UC Berkeley's MIDS program. We document the individual areas of exploration that we completed for this project, followed by the larger model that pulled these explorations together.\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "#### Table of Contents\n",
      "<ol>\n",
      "<li><a href=\"#part1\">Data Import and Base Methods</a></li>\n",
      "<li><a href=\"#part2\">Activity Features</a></li>\n",
      "<li><a href=\"#part3\">Text Bag of Words</a>\n",
      "<ul>\n",
      "<li>Simple</li>\n",
      "<li>L1 Feature Regularization</li>\n",
      "<li>Time</li><br/>\n",
      "</ul>\n",
      "</li>\n",
      "<li><a href=\"#part4\">Time Features</a></li>\n",
      "<li><a href=\"#part5\">Interesting Words &amp; Category Tags</a></li>\n",
      "<li><a href=\"#part6\">Spelling Mistakes</a></li>\n",
      "<li><a href=\"#part7\">Text Summary Features</a>\n",
      "<ul>\n",
      "<li>Text Length</li>\n",
      "<li>Word Case</li>\n",
      "<li>Paragraph Analysis</li><br/>\n",
      "</ul>\n",
      "</li>\n",
      "<li><a href=\"#part8\">Location Features</a></li>\n",
      "<li><a href=\"#part9\">Parts of Speech</a></li>\n",
      "<li><a href=\"#part10\">Final, Composite Model</a></li>\n",
      "<br/>\n",
      "<li><a href=\"#part11\">Appendix - additional goodness</a></li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part1\"></a>\n",
      "## 1. Data Import and Base Methods"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import pandas as pd\n",
      "import scipy as scipy\n",
      "import datetime as dt\n",
      "import time\n",
      "import BeautifulSoup\n",
      "from urllib import urlopen\n",
      "import re\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import gensim\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import make_scorer\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "#useful for text processing\n",
      "from nltk import word_tokenize\n",
      "from nltk.tokenize import WhitespaceTokenizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.tokenize import RegexpTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Helper methods that will be used often in this notebook\n",
      "\n",
      "# Helper methods to pull and submit data:\n",
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "            \n",
      "            \n",
      "# Helper methods for pulling columns from the dataset:\n",
      "def name2index(df, names):\n",
      "    return_single = False\n",
      "    \n",
      "    if type(names) == type([]):\n",
      "       names = np.array(names)\n",
      "    elif type(names) != type(np.array([])):\n",
      "        names = np.array([names])\n",
      "        return_single = True \n",
      "    \n",
      "    inds = np.where(np.in1d(df.columns, np.array(names)))[0]\n",
      "    \n",
      "    return inds[0] if return_single else inds\n",
      "\n",
      "            \n",
      "# Helper methods that are focused on class rebalancing:\n",
      "def balance_samples(y, method='oversample'):\n",
      "    class_counts = np.bincount(y)\n",
      "    \n",
      "    maxi = np.argmax(class_counts)\n",
      "    new_idx = np.argwhere(y==maxi)\n",
      "    \n",
      "    for i in range(len(class_counts)):\n",
      "        if i != maxi:\n",
      "            mult = class_counts[maxi]/class_counts[i]\n",
      "            rem = class_counts[maxi] - class_counts[i]*mult\n",
      "            idxi = np.argwhere(y==i)\n",
      "            np.random.shuffle(idxi)\n",
      "            for j in range(mult):\n",
      "                new_idx = np.vstack((new_idx,idxi))\n",
      "            new_idx = np.vstack((new_idx,idxi[:rem]))\n",
      "        \n",
      "    np.random.shuffle(new_idx)\n",
      "\n",
      "    return np.reshape(new_idx, (new_idx.shape[0],))\n",
      "\n",
      "def oversample_kfold(kf, y):\n",
      "    kf_over = []\n",
      "    for ti, di in kf:\n",
      "        yt = y[ti]\n",
      "        ti_over = ti[balance_samples(yt)]\n",
      "        kf_over.append((ti_over, di))\n",
      "    return kf_over\n",
      "\n",
      "# Helper methods for printing and scoring:\n",
      "def test_kfolds(X, y, kf, model, verbose=1, balance=False):\n",
      "    roc_auc_list = []\n",
      "    \n",
      "    for train_i, dev_i in kf:\n",
      "        if balance:\n",
      "            train_i_orig = train_i\n",
      "            y_train = y[train_i_orig]\n",
      "            train_i = train_i_orig[balance_samples(y_train)]\n",
      "        \n",
      "        \n",
      "        X_train = X[train_i]\n",
      "        X_dev = X[dev_i]\n",
      "\n",
      "        model.fit(X_train, y[train_i])\n",
      "\n",
      "        dev_pred = model.predict(X_dev)\n",
      "        \n",
      "        roc_auc_i = roc_auc_score(y[dev_i], dev_pred)\n",
      "        roc_auc_list.append(roc_auc_i)\n",
      "        if verbose > 1:\n",
      "            print('ROC AUC:',roc_auc_i)\n",
      "            \n",
      "    if verbose > 0:\n",
      "        print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(kf), np.mean(roc_auc_list), np.median(roc_auc_list), np.std(roc_auc_list))\n",
      "        \n",
      "    return roc_auc_list\n",
      "\n",
      "def print_scores(scores):\n",
      "    print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(scores), np.mean(scores), np.median(scores), np.std(scores))\n",
      "\n",
      "\n",
      "# Lastly, some classes to handle string tokenizing that we will use in multiple sections:\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class SnowballStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = SnowballStemmer('english')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "class PorterStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class PuncTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.reg = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
      "    def __call__(self, doc):\n",
      "        return self.reg.tokenize(doc)\n",
      "    \n",
      "class SpaceTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tknzr = WhitespaceTokenizer()\n",
      "    def __call__(self, doc):\n",
      "        return [t for t in self.tknzr.tokenize(doc)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Set up the training and test data to work with throughout the notebook:\n",
      "\n",
      "# load data from JSON file as list of dicts\n",
      "all_train_dict_list = load_json_file('../data/train.json')\n",
      "submit_dict_list =  load_json_file('../data/test.json')\n",
      "\n",
      "n_all = len(all_train_dict_list)\n",
      "n_submit = len(submit_dict_list)\n",
      "\n",
      "# shuffle data to avoid biased split of train / dev data\n",
      "rand.shuffle(all_train_dict_list)\n",
      "\n",
      "# process labels\n",
      "all_train_labels = np.array([x['requester_received_pizza'] for x in all_train_dict_list])\n",
      "\n",
      "# pandas is useful for turning dicts in to matrix-like objects\n",
      "# where each column is an numpy array\n",
      "submit_df = pd.DataFrame(submit_dict_list)\n",
      "all_train_df = pd.DataFrame(all_train_dict_list)\n",
      "\n",
      "# limit train to columns available in submit_df\n",
      "submit_cols = submit_df.columns\n",
      "all_train_df = all_train_df[submit_cols]\n",
      "\n",
      "# useful for sklearn scoring\n",
      "roc_scorer = make_scorer(roc_auc_score)\n",
      "\n",
      "# set up kFolds\n",
      "kf = KFold(n_all, n_folds = 10)\n",
      "\n",
      "y = all_train_labels\n",
      "kf_over = []\n",
      "for ti, di in kf:\n",
      "    yt = y[ti]\n",
      "    ti_over = ti[balance_samples(yt)]\n",
      "    kf_over.append((ti_over, di))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part2\"></a>\n",
      "## 2. Activity Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Set up Numeric Activity Variables, including processor class for all activity feature analysis.\n",
      "\n",
      "ACTIVITY_VARS = ['requester_account_age_in_days_at_request',\n",
      "                'requester_days_since_first_post_on_raop_at_request',\n",
      "                'requester_number_of_comments_at_request',\n",
      "                'requester_number_of_comments_in_raop_at_request',\n",
      "                'requester_number_of_posts_at_request',\n",
      "                'requester_number_of_posts_on_raop_at_request',\n",
      "                'requester_number_of_subreddits_at_request',\n",
      "                'requester_upvotes_minus_downvotes_at_request',\n",
      "                'requester_upvotes_plus_downvotes_at_request'\n",
      "                ]\n",
      "ACTIVITY_COLUMNS = name2index(all_train_df, ACTIVITY_VARS)\n",
      "\n",
      "class ExtractColumnsTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, cols=[0]):\n",
      "        self.cols = cols\n",
      "        \n",
      "    def fit(self, *args, **kwargs):\n",
      "        return self\n",
      "        \n",
      "    def transform(self, X):\n",
      "        cols = self.cols\n",
      "        return X[:,cols]\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "class ExtractActivities(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, ACTIVITY_COLUMNS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Explore models using the activity features only.\n",
      "\n",
      "# The main concern here is weighting classes appropriately, so we do an investigation of different\n",
      "# approaches and see how well the resulting model performs on 5 k-folds of the training data.\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print 'Equal Class Weights'\n",
      "pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC())])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nReweighted Classes'\n",
      "wt_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC(class_weight='auto'))])\n",
      "print_scores(cross_val_score(wt_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nRebalanced Sample'\n",
      "rebal_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc',SVC())])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(rebal_pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Equal Class Weights\n",
        "N: 10, Mean: 0.509546, Median: 0.507994, SD: 0.008061"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Reweighted Classes\n",
        "N: 10, Mean: 0.555346, Median: 0.553994, SD: 0.025812"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Rebalanced Sample\n",
        "N: 10, Mean: 0.551692, Median: 0.552735, SD: 0.032479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part3\"></a>\n",
      "## 3. Bag of Words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3a. Simple"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Define quick classes that we can use to isolate the title and body columns in our data.\n",
      "TITLE_COLUMN = name2index(all_train_df, 'request_title')\n",
      "BODY_COLUMN = name2index(all_train_df, 'request_text_edit_aware')\n",
      "\n",
      "class ExtractBody(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, BODY_COLUMN)\n",
      "        \n",
      "class ExtractTitle(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, TITLE_COLUMN)\n",
      "        \n",
      "class ExtractAllText(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, np.array([TITLE_COLUMN, BODY_COLUMN]))\n",
      "\n",
      "\n",
      "class ConcatStringTransformer(TransformerMixin):\n",
      "    def __init__(self):\n",
      "        return None\n",
      "        \n",
      "    def fit(self, *args, **kwargs):\n",
      "        return self\n",
      "        \n",
      "    def transform(self, X):\n",
      "        \n",
      "        if len(X.shape) == 1:\n",
      "            return X\n",
      "        else:\n",
      "            n_feat = X.shape[1]\n",
      "            new_list = []\n",
      "            for i in range(X.shape[0]):\n",
      "                new_list.append('.'.join(X[i,:]))\n",
      "            \n",
      "            return np.array(new_list)\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "# A simple text processing pipeline that we will use in the composite model:\n",
      "text_pipe = Pipeline([('union', ExtractAllText()), ('concat',ConcatStringTransformer())])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reusable method for quick BOW investigations:\n",
      "def simple_text(do_all=True, do_count=False,do_tfidf=False, do_titles=False, do_bodies=False, do_both=False, lowercase=False, tokenizer=None, stop_words=None):\n",
      "\n",
      "    # Notes\n",
      "    # results slightly better w/ lowercase = False (when unigrams only)\n",
      "    # bigrams added no value on unigrams\n",
      "\n",
      "    tv = TfidfVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    cv = CountVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "    \n",
      "    body_cv = Pipeline([('body',ExtractBody()),('cv', cv)])\n",
      "    body_tv = Pipeline([('body',ExtractBody()),('tv', tv)])\n",
      "    \n",
      "    title_cv = Pipeline([('title',ExtractTitle()),('cv', cv)])\n",
      "    title_tv = Pipeline([('title',ExtractTitle()),('tv', tv)])\n",
      "\n",
      "    if do_titles or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Titles\n",
      "            print '\\nCount Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer TItles\n",
      "            print '\\nTFIDF Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "    if do_bodies or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Bodies\n",
      "            print '\\nCount Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Bodies\n",
      "            print '\\nTFIDF Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        \n",
      "    if do_both or do_all:\n",
      "        if do_count or do_all:\n",
      "\n",
      "            # Count Vectorizer Titles and Bodies\n",
      "            print '\\nCount Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_cv),\n",
      "                    ('tranform_body',body_cv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "            \n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Titles and Bodies\n",
      "            print '\\nTFIDF Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_tv),\n",
      "                    ('tranform_body',body_tv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Experimentation with different vectorizers and tokenizers\n",
      "\n",
      "print \"Examination of best vectorizer across titles, bodies, or titles + bodies:\"\n",
      "simple_text(do_all = True)\n",
      "\n",
      "print \"==========\"\n",
      "print \"Examination of tfidf vectorizer on titles + bodies with Snowball Stem Tokenizer:\"\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=SnowballStemTokenizer())\n",
      "print \"\\nExamination of tfidf vectorizer on titles + bodies with Lemma Tokenizer:\"\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Examination of best vectorizer across titles, bodies, or titles + bodies:\n",
        "\n",
        "Count Vectorizer on Titles\n",
        "N: 10, Mean: 0.519474, Median: 0.520740, SD: 0.024555"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles\n",
        "N: 10, Mean: 0.518167, Median: 0.521154, SD: 0.026890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Bodies\n",
        "N: 10, Mean: 0.538654, Median: 0.529119, SD: 0.021140"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Bodies\n",
        "N: 10, Mean: 0.546304, Median: 0.540143, SD: 0.019159"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.542050, Median: 0.546315, SD: 0.016378"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.534769, Median: 0.534403, SD: 0.028518"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "==========\n",
        "Examination of tfidf vectorizer on titles + bodies with Snowball Stem Tokenizer:\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.554693, Median: 0.543610, SD: 0.028433"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Examination of tfidf vectorizer on titles + bodies with Lemma Tokenizer:\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.548386, Median: 0.546751, SD: 0.030142"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Simple Bag of Words\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
      "<td>0.5547</td>\n",
      "<td>0.5436</td>\n",
      "<td>0.0284</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3b. L1 Feature Regularization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reusable class to process important terms in BOW\n",
      "class LinearWeightFeatureThreshold(TransformerMixin):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model = LinearSVC(class_weight='auto', loss='squared_hinge', penalty='l1', dual=False),\n",
      "        return_dense = True,\n",
      "        C = 1,\n",
      "        threshold = 0.01,\n",
      "        verbose = 1\n",
      "        ):\n",
      "        self.model = model\n",
      "        self.return_dense = return_dense\n",
      "        self.C = C\n",
      "        self.threshold = threshold\n",
      "        self.verbose = verbose\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        model = self.model\n",
      "        threshold = self.threshold\n",
      "        verbose = self.verbose\n",
      "        C = self.C\n",
      "        \n",
      "        model.set_params(C=C)\n",
      "        \n",
      "        model.fit(X, y)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > threshold)[0]\n",
      "        n_coef = np.sum(sig_coef)\n",
      "        if verbose > 0:\n",
      "            print 'kept %d/%d features' % (n_coef, coef.shape[1])\n",
      "        \n",
      "        if n_coef == 0:\n",
      "            sig_coef[0] = 1\n",
      "        \n",
      "        self.sig_coef_  = sig_coef\n",
      "        return self\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        sig_coef = self.sig_coef_\n",
      "        return_dense = self.return_dense\n",
      "        \n",
      "        X_new = X[:,sig_coef]\n",
      "        \n",
      "        if return_dense and (type(X_new) != type(np.array(1))):\n",
      "            X_new = X_new.toarray()\n",
      "            \n",
      "        return X_new\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {'C':self.C, 'threshold':self.threshold}\n",
      "    \n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            setattr(self, parameter, value)\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### APPENDIX? - the whole section\n",
      "\n",
      "### Examination of different title + body combination with feature reduction:\n",
      "\n",
      "# Grid search on C put optimal value between 0.1 and 0.3.\n",
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=False, tokenizer=SnowballStemTokenizer())\n",
      "l1 = LinearWeightFeatureThreshold(C=0.3)\n",
      "lsvc = LinearSVC(class_weight='auto')\n",
      "\n",
      "pipe_lsvc = Pipeline([('tv',tv), ('features',l1), ('lsvc',lsvc)])\n",
      "\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv), ('features',l1)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv), ('features',l1)])\n",
      "\n",
      "print '\\nL1 Feature Reduction on Titles w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([('title', title_pipe), ('lsvc', lsvc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([('body', body_pipe), ('lsvc', lsvc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "print '\\nL1 Feature Reduction on Both w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Union w/ LinearSVC'\n",
      "\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv)])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('l1', l1),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Both Individually and on Union w/ LinearSVC'\n",
      "\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv), ('l1',l1)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv), ('l1',l1)])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('l1', l1),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "L1 Feature Reduction on Titles w/ LinearSVC\n",
        "kept 169/3881 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/3914 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 169/3885 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 179/3892 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 187/3867 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 175/3858 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/3913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 176/3921 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/3892 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/3902 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.526429, Median: 0.527779, SD: 0.022890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Bodies w/ LinearSVC\n",
        "kept 148/9938 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 164/9873 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 150/9853 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 153/9866 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 149/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 139/9894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 156/9918 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 152/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 147/9946 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 154/9813 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.573187, Median: 0.575517, SD: 0.024980"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Both w/ LinearSVC\n",
        "kept 169/3881 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 148/9938 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/3914 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 164/9873 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 169/3885 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 150/9853 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 179/3892 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 153/9866 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 187/3867 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 149/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 175/3858 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 139/9894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/3913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 156/9918 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 176/3921 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 152/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/3892 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 147/9946 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/3902 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 154/9813 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.581588, Median: 0.585658, SD: 0.031463"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Union w/ LinearSVC\n",
        "kept 267/13819 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 289/13787 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 275/13738 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 288/13758 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 278/13767 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 263/13752 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 279/13831 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 296/13834 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 278/13838 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 289/13715 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.573828, Median: 0.574240, SD: 0.025762"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Both Individually and on Union w/ LinearSVC\n",
        "kept 169/3881 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 148/9938 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 243/317 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/3914 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 164/9873 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 264/338 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 169/3885 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 150/9853 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 254/319 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 179/3892 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 153/9866 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 260/332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 187/3867 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 149/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 255/336 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 175/3858 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 139/9894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 241/314 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/3913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 156/9918 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 261/330 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 176/3921 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 152/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 260/328 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/3892 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 147/9946 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 254/324 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/3902 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 154/9813 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 264/336 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.575145, Median: 0.578758, SD: 0.031717"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Try GradientBoostingClassifier and ExtraTreesClassifier for the BOW models:\n",
      "\n",
      "l1 = LinearWeightFeatureThreshold()\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "pipe_etc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',etc)])\n",
      "pipe_gbc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',gbc)])\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ ETC'\n",
      "print_scores(cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "### APPENDIX? - GBC portion, focus on ETC\n",
      "print '\\nL1 Feature Reduction on Bodies w/ GBC'\n",
      "#need to oversample GBC b/c no class_weight\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "print_scores(cross_val_score(pipe_gbc, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "L1 Feature Reduction on Bodies w/ ETC\n",
        "kept 775/9938 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 782/9873 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 772/9853 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 797/9866 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 789/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 798/9894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 775/9918 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 769/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 813/9946 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 788/9813 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.595034, Median: 0.598702, SD: 0.020847"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Bodies w/ GBC\n",
        "kept 1395/9938 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1365/9873 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1358/9853 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1387/9866 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1386/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1405/9894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1360/9918 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1378/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1385/9946 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1377/9813 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.565562, Median: 0.561362, SD: 0.022070"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Bag of Words w/ Feature Reduction\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
      "<td>0.5547</td>\n",
      "<td>0.5436</td>\n",
      "<td>0.0284</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
      "<td>0.5950</td>\n",
      "<td>0.5987</td>\n",
      "<td>0.0208</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part4\"></a>\n",
      "## 4. Time Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reusable class for time features\n",
      "DATE_TIME_COLUMN_DEFAULT = np.where(all_train_df.columns == 'unix_timestamp_of_request')[0][0]\n",
      "\n",
      "class TimeTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, date_time_column=DATE_TIME_COLUMN_DEFAULT, do_hour=True, do_dow=True, do_month=True):\n",
      "        self.date_time_column = date_time_column\n",
      "        self.do_hour = do_hour\n",
      "        self.do_dow = do_dow\n",
      "        self.do_month = do_month\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        return self\n",
      "    \n",
      "    def extract_from_date_time_(self, dt, do_hour, do_dow, do_month):\n",
      "        extract = []\n",
      "        if do_hour:\n",
      "            extract.append(dt.hour)\n",
      "            \n",
      "        if do_dow:\n",
      "            extract.append(dt.weekday())\n",
      "            \n",
      "        if do_month:\n",
      "            extract.append(dt.month)\n",
      "            \n",
      "        return extract\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        date_time_column = self.date_time_column\n",
      "        do_hour = self.do_hour\n",
      "        do_dow = self.do_dow\n",
      "        do_month = self.do_month\n",
      "        extract_from_date_time = self.extract_from_date_time_\n",
      "        \n",
      "        features = np.array([\n",
      "            extract_from_date_time(dt.datetime.fromtimestamp(timei),\n",
      "                                   do_hour=do_hour,\n",
      "                                   do_dow=do_dow,\n",
      "                                   do_month=do_month) for timei in X[:,date_time_column]\n",
      "        ])\n",
      "        \n",
      "        return features\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Visualizations to inspect time variables\n",
      "# Exploring time features, it looks like requests are not as succesful at late nights /\n",
      "# early mornings or on Mondays / Fridays...\n",
      "# Though that could be because there's more requests on those days.\n",
      "\n",
      "# look at hourly success\n",
      "hour = TimeTransformer(do_dow=False, do_month=False).transform(all_train_df.values).flatten()\n",
      "hour_pos = hour[all_train_labels]\n",
      "hour_neg = hour[np.logical_not(all_train_labels)]\n",
      "pd.Series(hour_pos).hist(bins=24, alpha=0.2, normed=True)\n",
      "pd.Series(hour_neg).hist(bins=24, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x11387b210>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJEmSJEmSJEmSJC3u/wO+1xxjrBNuZAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11387b9d0>"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# look at day of week success\n",
      "dow = TimeTransformer(do_hour=False, do_month=False).transform(all_train_df.values).flatten()\n",
      "dow_pos = dow[all_train_labels]\n",
      "dow_neg = dow[np.logical_not(all_train_labels)]\n",
      "pd.Series(dow_pos).hist(bins=7, alpha=0.2, normed=True)\n",
      "pd.Series(dow_neg).hist(bins=7, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x112397750>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE+JJREFUeJzt3X9sXWd9x/F3Grdr05RkUyqilNIgpYOgjZYt61LoIFsr\nWiii+69DE4PBuqKtbGIIddXQ0v+maENIrBqNGEwdTLQS+6GU0RkqLWoFpvR2sQPEaZ02iZOY4rYh\nmDRt4qzeH+emM8b2eXzvfXLu8837JV3lHp97bp4PoR8ff++51yBJkiRJkiRJkiRJkiRJknTOugnY\nC4wBd86z//eBEWA38C3gLUs4VpLUkOXAPmA9cD4wDGyc85hrgVXt+zcB31nCsZKkTM6r2X8NVUkf\nAKaB+4Fb5jxmCPhJ+/5jwOuWcKwkKZO6gr8MODRr+3D7awv5CPD1Do+VJPXQQM3+mSU8128DHwbe\n3sGxkqQeqyv4I8Dls7YvpzoTn+stwOepZvA/Xsqx69atm5mYmEhdrySp8jSwoZsnGGg/yXrgAuZ/\nofT1VLP2zR0cCzAT2datW5teQlbmK1fkbDMz8fORMCWpO4M/DdwBDFJdFfMFYBS4vb1/O/DXwC8C\nn2t/bZrqBdaFjj2nHDhwoOklZGW+ckXOBvHzpagreICH2rfZts+6/0ftW+qxkqSzoO4qGnXpQx/6\nUNNLyMp85YqcDeLnS7Gs6QVQzeCbXoMkFWXZsmVQ0+GewWe2c+fOppeQlfnKFTkbxM+XwoKXpKAc\n0UhSgRzRSNI5zILPLPoc0HzlipwN4udLYcFLUlDO4CWpQCkz+JR3siqwoaERpqZON72Mjr3mNQNc\ne+1VTS9D6ksWfGY7d+5ky5YtTS9jQVNTp1mz5tc7Pr7V2smmTVt6t6Alev75J7I+f7//+3UjcjaI\nny+FM3hJCsoZ/DlucPCJrs7gm/b8809w443lrl/qlNfBS9I5zBl8l4ZaQ0ydmFpw/8iuEa56a/++\nCLhnbIJ3FDyDzy3yHDdyNoifL4UF36WpE1Os2bBmwf2rJlctur9pozu+wfkXDXZ8/Ni+EaYHTvZw\nRUsz/dKEIxppARZ8ZpvetqnpJSzq1P++xOr1nX8D+o311/dwNUt3cPdY1uePfAYYORvEz5fCGbwk\nBeUZfGatb7f6/iy+G6PDLTZe3Vy+8fEjDA7muxZ+ZKTFVVfly9fkG7Wiz6ij50vRFwX/+ON7m15C\nx6Z+OsUa+nfGHt3Jl2eyXua5atVPsz5/7jdq6dzWFwV/6tTrm15CR44de4GTJ08t+pjIZ+9Ao2fv\nZ0PkK4Sin93W5Sv9YzpS9EXBX3jhiqaX0JGBgSlo7gISSV3o9mM6SuCLrJm1vt1qeglZjQ7Hztdq\n7Wx6CdlE/7z06PlSWPCSFJQFn5kz+LI5gy9X9Hwp+mIGL0ln256x3Zx/+Pmml5GVBZ+Z18GXLfJn\n7US/Trwu34npF7liY+xLnB3RSFJQFnxmkc/ewRl8ySKfvUP8fCkseEkKyoLPzOvgy+Z18OWKni+F\nBS9JQVnwmTmDL5sz+HJFz5eiLy6T/MFYmT/mHzv2PL+w4njTy5CkefVFwZ9a+XLTS+jIsWPPsfrU\n4p825nXwZfM6+HJFz5eiLwr+wosuanoJHRkY6Iv/+SRpXs7gM4t89g7O4EsW/ew2er4UFrwkBeWM\noUv79x/hvKHRBfePfX+UK39l41lc0dJMTh7t6nhn8OWKPqOOni+FBd+ll16aYfXqhQt85coXF93f\ntOlTO5peggo11Bpi6sRU08tY0MjICCfPW/giiPGJZ7jiLdeexRWdfRZ8ZpHPbiF+vqhn79D9jHrq\nxBRrNvTvpzFev+H6RfefPF3m1XtLYcGraJMvHGFoeLDpZXRs+qUJbrwx9u8FVXMs+Myiz6ibzjc9\nc5LV6/OdRebOd3D3WLbnrhN9Rh39PSgpvIpGkoKy4DOLfPYO5itZ5LN3iP8elBQWvCQFZcFnFv3z\n0s1Xruiflx79dzGkSCn4m4C9wBhw5zz73wQMAS8Dn5iz7wCwG9gFfLfjVUqSlqzuKprlwD3ADcAR\n4HFgBzD7rZsvAB8Dfnee42eALUB3b5csWOQZLpivZM7g46s7g78G2Ed1Jj4N3A/cMucxzwGt9v75\nLOtifZKkDtUV/GXAoVnbh9tfSzUDPEz1DeC2pS0thsgzXDBfyZzBx1c3opnp8vnfDvwQuBT4JtUs\n/9G5D9q+bSuXrl0HwIqLV3LFhje++qPxmf/A+nV74tD4z7wZZu7+g/ue7Kv1zt0+Ojm56PrrtpvO\n1+36m873zL6xn3nD0ZnSLWX7TImeGYeUtt30f39L2R4dbvHI4IMAr/ZlnbrxyWbgbqoXWgHuAl4B\nts3z2K3AceDTCzzXQvtnvvRwmd9px/c/xfieXVx3861NL6Vj9927jQ9+dL7XzstQ+voP7h7irz5+\nR9PL6MjgI4N9/Vk0df7+b7/ADe/9SNPL6NgHbtgENR1eN6JpAVcC64ELgFupXmSdz9y/aAVwSfv+\nxcC7gO/V/H2SpB6pK/jTwB3AILAHeIDqCprb2zeAtVRz+o8DnwLGgZXtrz8KDAOPAV8DvtHb5fe/\nyDNcMF/JnMHHl/JhYw+1b7Ntn3X/WeDyeY47Dlzd4bqkc8L4+BEGB59o5O8eGXmSkycvqX/gAvYc\neJp3FDyiORf4aZKZRb6OGszXrZMvz7BmTTMfF3z99d39vSf2DPVoJXl4HbwfVSBJYVnwmUWe4YL5\nStZq7Wx6CVk5g7fgJSksCz4zZ9Rli5wv8u+bBWfw4IusUqNK/p2y4xPPANc2vQwtwoLPrOnfWZqb\n+bqT+3fKLqbbbCe//3IPV9N7/k5WRzSSFJYFn1nks1swX8kiZwNn8GDBS1JYFnxmka+jBvOVLHI2\n8Dp4sOAlKSwLPrPoc07zlStyNnAGD14mKalDk5MvMDQ02vQyOjY5ebTpJWRnwWfmdeJli5yv22zT\np2ZYvXpjD1fUW3X5pk8t9LuL4nBEI0lBWfCZRT37O8N85YqcDeLnS2HBS1JQFnxm0a81Nl+5ImeD\n+PlSWPCSFJQFn1n0OaD5yhU5G8TPl8KCl6SgLPjMos8BzVeuyNkgfr4UFrwkBWXBZxZ9Dmi+ckXO\nBvHzpbDgJSkoCz6z6HNA85UrcjaIny+FBS9JQVnwmUWfA5qvXJGzQfx8KSx4SQrKgs8s+hzQfOWK\nnA3i50thwUtSUBZ8ZtHngOYrV+RsED9fCgtekoKy4DOLPgc0X7kiZ4P4+VJY8JIUlAWfWfQ5oPnK\nFTkbxM+XwoKXpKAs+MyizwHNV67I2SB+vhQWvCQFZcFnFn0OaL5yRc4G8fOlsOAlKSgLPrPoc0Dz\nlStyNoifL4UFL0lBWfCZRZ8Dmq9ckbNB/HwpLHhJCsqCzyz6HNB85YqcDeLnS5FS8DcBe4Ex4M55\n9r8JGAJeBj6xxGMlSZnUFfxy4B6qon4z8H5g45zHvAB8DPi7Do4NL/oc0HzlipwN4udLUVfw1wD7\ngAPANHA/cMucxzwHtNr7l3qsJCmTuoK/DDg0a/tw+2spujk2jOhzQPOVK3I2iJ8vxUDN/pkunjv5\n2O3btnLp2nUArLh4JVdseOOrP16d+Ufq1+2JQ+OMDrcW3H9w35N9td6520cnJxddf9120/m6XX/T\n+XKv3+0426PDLR4ZfBDg1b6ss6xm/2bgbqo5OsBdwCvAtnkeuxU4Dnx6icfOfOnhMr/Tju9/ivE9\nu7ju5lubXkrH7rt3Gx/8aLmvf7v+5pS8dih//R+4YRPUdHjdiKYFXAmsBy4AbgV2LPDYuX/RUo6V\nJPVYXcGfBu4ABoE9wAPAKHB7+wawlmrW/nHgU8A4sHKRY88p0eeA5itX5GwQP1+Kuhk8wEPt22zb\nZ91/Frh8CcdKks4C38maWfRrcc1XrsjZIH6+FBa8JAVlwWcWfQ5ovnJFzgbx86Ww4CUpKAs+s+hz\nQPOVK3I2iJ8vhQUvSUFZ8JlFnwOar1yRs0H8fCkseEkKyoLPLPoc0HzlipwN4udLYcFLUlAWfGbR\n54DmK1fkbBA/XwoLXpKCsuAziz4HNF+5ImeD+PlSWPCSFJQFn1n0OaD5yhU5G8TPl8KCl6SgLPjM\nos8BzVeuyNkgfr4UFrwkBWXBZxZ9Dmi+ckXOBvHzpbDgJSkoCz6z6HNA85UrcjaIny+FBS9JQVnw\nmUWfA5qvXJGzQfx8KSx4SQrKgs8s+hzQfOWKnA3i50thwUtSUBZ8ZtHngOYrV+RsED9fCgtekoKy\n4DOLPgc0X7kiZ4P4+VJY8JIUlAWfWfQ5oPnKFTkbxM+XwoKXpKAs+MyizwHNV67I2SB+vhQWvCQF\nZcFnFn0OaL5yRc4G8fOlsOAlKSgLPrPoc0DzlStyNoifL4UFL0lBWfCZRZ8Dmq9ckbNB/HwpLHhJ\nCsqCzyz6HNB85YqcDeLnS2HBS1JQFnxm0eeA5itX5GwQP18KC16SgrLgM4s+BzRfuSJng/j5Uljw\nkhSUBZ9Z9Dmg+coVORvEz5cipeBvAvYCY8CdCzzms+39I8BbZ339ALAb2AV8t+NVSpKWbKBm/3Lg\nHuAG4AjwOLADGJ31mPcAG4Argd8EPgdsbu+bAbYAR3u24sJEnwOar1yRs0H8fCnqzuCvAfZRnYlP\nA/cDt8x5zPuA+9r3HwNWA6+dtX9Z16uUJC1ZXcFfBhyatX24/bXUx8wADwMt4LbOl1mu6HNA85Ur\ncjaIny9F3YhmJvF5FjpLvw6YAC4Fvkk1y3907oO2b9vKpWvXAbDi4pVcseGNr/54deYfqV+3Jw6N\nMzrcWnD/wX1P9tV6524fnZxcdP11203n63b9TefLvX6342yPDrd4ZPBBgFf7sk7d+GQzcDfVC60A\ndwGvANtmPeZeYCfV+AaqEn8n8KM5z7UVOA58es7XZ770cJnfacf3P8X4nl1cd/OtTS+lY/fdu40P\nfnSh1877n+tvTslrh/LX/4EbNkFNh9eNaFpUL56uBy4AbqV6kXW2HcAftO9vBo5RlfsK4JL21y8G\n3gV8L2nlkqSu1RX8aeAOYBDYAzxAdQXN7e0bwNeBZ6hejN0O/En762upxjHDVC++fg34Rg/XXoTo\nc0DzlStyNoifL0XdDB7gofZttu1ztu+Y57hngKs7WZQkqXu+kzWz6Nfimq9ckbNB/HwpLHhJCsqC\nzyz6HNB85YqcDeLnS2HBS1JQFnxm0eeA5itX5GwQP18KC16SgrLgM4s+BzRfuSJng/j5UljwkhSU\nBZ9Z9Dmg+coVORvEz5fCgpekoCz4zKLPAc1XrsjZIH6+FBa8JAVlwWcWfQ5ovnJFzgbx86Ww4CUp\nKAs+s+hzQPOVK3I2iJ8vhQUvSUFZ8JlFnwOar1yRs0H8fCkseEkKyoLPLPoc0HzlipwN4udLYcFL\nUlAWfGbR54DmK1fkbBA/XwoLXpKCsuAziz4HNF+5ImeD+PlSWPCSFJQFn1n0OaD5yhU5G8TPl8KC\nl6SgLPjMos8BzVeuyNkgfr4UFrwkBWXBZxZ9Dmi+ckXOBvHzpbDgJSkoCz6z6HNA85UrcjaIny+F\nBS9JQVnwmUWfA5qvXJGzQfx8KSx4SQrKgs8s+hzQfOWKnA3i50thwUtSUBZ8ZtHngOYrV+RsED9f\nCgtekoKy4DOLPgc0X7kiZ4P4+VJY8JIUlAWfWfQ5oPnKFTkbxM+XwoKXpKAs+MyizwHNV67I2SB+\nvhQWvCQFZcFnFn0OaL5yRc4G8fOlsOAlKaiUgr8J2AuMAXcu8JjPtvePAG9d4rGhRZ8Dmq9ckbNB\n/Hwp6gp+OXAPVVG/GXg/sHHOY94DbACuBP4Y+NwSjg3v4L4nm15CVuYrV+RsED9firqCvwbYBxwA\npoH7gVvmPOZ9wH3t+48Bq4G1iceGd+LF400vISvzlStyNoifL0VdwV8GHJq1fbj9tZTHrEs4VpKU\nyUDN/pnE51nWzSIOPrW7m8Mb89KJF1lW8y3yuWcnzs5iGmK+ckXOBvHz9cJm4L9mbd/Fz79Yei/w\ne7O29wKvTTwWqjHOjDdv3rx5W9JtH10aAJ4G1gMXAMPM/yLr19v3NwPfWcKxkqQGvRt4kuq7xV3t\nr93evp1xT3v/CPBrNcdKkiRJKlXkN0J9EfgR8L2mF5LB5cB/Az8Avg/8WbPL6bkLqS75HQb2AH/T\n7HKyWQ7sAh5seiEZHAB2U+X7brNL6bnVwFeBUar/f25udjnzW041ulkPnE+8Gf1vUb2rN2LBrwWu\nbt9fSTWGi/RvB7Ci/ecA1etK1zW4llz+AvgXYEfTC8lgP/BLTS8ik/uAD7fvDwCrFnpgk59FE/2N\nUI8CP256EZk8S/UNGeA41ZnEuuaWk8WJ9p8XUJ2MHG1wLTm8juoCiX+ky8uc+1jEXKuoTh6/2N4+\nDfxkoQc3WfApb6JS/1tP9ZPKYw2vo9fOo/om9iOqcdSeZpfTc58BPgm80vRCMpkBHgZawG0Nr6WX\n3gA8B/wT8D/A5/n/nzZ/TpMFP9Pg363eWEk1C/xzqjP5SF6hGkO9DngHsKXR1fTWe4FJqvl0xLNc\ngLdTnXi8G/hTqrPeCAaorlT8h/afLwJ/udCDmyz4I1Qv1p1xOdVZvMpwPvCvwJeB/2h4LTn9BPhP\nINKHi7+N6jOk9gNfAX4H+OdGV9R7P2z/+Rzw71Qj4QgOt2+Pt7e/ys9emt43zoU3Qq0n5ousy6gK\n4TNNLySTNVRXKgBcBDwCXN/ccrJ6J/GuolkBXNK+fzHwLeBdzS2n5x4Bfrl9/25gW3NLWVzkN0J9\nBZgATlK91vCHzS6np66jGmEMU/2Yv4vqktcofpVqvjlMdandJ5tdTlbvJN5VNG+g+rcbprqMN1q3\nXEV1Bj8C/BuLXEUjSZIkSZIkSZIkSZIkSZIkSZIkSY37PxN9uJr2t5IqAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11387be90>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# look at month success\n",
      "month = TimeTransformer(do_hour=False, do_dow=False).transform(all_train_df.values).flatten()\n",
      "month_pos = month[all_train_labels]\n",
      "month_neg = month[np.logical_not(all_train_labels)]\n",
      "pd.Series(month_pos).hist(bins=12, alpha=0.2, normed=True)\n",
      "pd.Series(month_neg).hist(bins=12, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x111d75b10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREREREGs3/Bx0X+CHxQya1AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x111d85690>"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Try a couple classifiers for time features to find a good choice.\n",
      "# Turns out time features don't perform well by themselves.\n",
      "\n",
      "svc = Pipeline([('scaler',StandardScaler()), ('svc',SVC(class_weight='auto'))])\n",
      "\n",
      "print '\\nSVC'\n",
      "svc_pipe = Pipeline([\n",
      "    ('time',TimeTransformer()),\n",
      "    ('model',svc)\n",
      "    ])\n",
      "print_scores(cross_val_score(svc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "print '\\nExtra Tree Ensemble'\n",
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_pipe = Pipeline([\n",
      "    ('time',TimeTransformer()),\n",
      "    ('model',etc)\n",
      "    ])\n",
      "\n",
      "print_scores(cross_val_score(etc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC\n",
        "N: 10, Mean: 0.519620, Median: 0.515254, SD: 0.015947"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Tree Ensemble\n",
        "N: 10, Mean: 0.525189, Median: 0.522283, SD: 0.022853"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Time Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
      "<td>0.5547</td>\n",
      "<td>0.5436</td>\n",
      "<td>0.0284</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
      "<td>0.5950</td>\n",
      "<td>0.5987</td>\n",
      "<td>0.0208</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Time Features</td>\n",
      "<td>0.5252</td>\n",
      "<td>0.5223</td>\n",
      "<td>0.0229</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part5\"></a>\n",
      "## 5. Interesting Words & Category Tags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reusable class for interesting words\n",
      "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "# Useful method for parsing individual words:\n",
      "def lenArray(text, no_zero = True):\n",
      "    lens = np.array([[float(len(x.encode('utf-8'))) for x in text]]).T\n",
      "    lens[lens==0]=1\n",
      "    return lens\n",
      "\n",
      "class InterestingWordsTransformer(TransformerMixin):\n",
      "    def __init__(self, title_col = TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True, do_tags=True, do_words=True):\n",
      "        self.do_title = do_title\n",
      "        self.do_body = do_body\n",
      "        self.do_tags = do_tags\n",
      "        self.do_words = do_words\n",
      "        self.title_col = title_col\n",
      "        self.body_col = body_col\n",
      "        \n",
      "        self.keywords = {\n",
      "            'sad_food': ['hungry', 'starving', 'no food', 'grocer', 'eaten', 'hunger', 'ramen', 'empty', 'fridge', 'refrig'],\n",
      "            'money': ['broke', 'paid', 'money', 'unemployed', 'lost', 'job', 'bill', 'wage', 'work', 'payday', 'paycheck', 'funds', 'cash', 'bank', 'laid off', 'poor', 'payroll'],\n",
      "            'sad': ['worst', 'awful', 'sick', 'problem', 'catch a break', 'cheer', 'hospital', 'bad', 'shitty', 'stress', 'luck', ':(', 'rough', 'tough'],\n",
      "            'military': ['military', 'veteran', 'soldier', 'army', 'navy', 'marine', 'air force', 'iraq', 'afghanis'],\n",
      "            'happy': ['celebrate', 'birthday', 'party', 'new year', 'bday', 'engage', 'annivers'],\n",
      "            'nice': ['please', 'help', 'thank you', ':)', ':D', ':-)', ';)', 'thank', 'apprec'],\n",
      "            'honest': ['sob story', 'honest', 'just want', 'just because'],\n",
      "            'parent': ['family', 'kid', 'parent', 'mom', 'mother', 'dad', 'father', 'baby', 'boy', 'girl'],\n",
      "            'relationship': ['husband', 'wife', 'girlfriend', 'boyfriend', 'fianc'],\n",
      "            'student': ['study', 'test', 'final', 'midterm', 'student'],\n",
      "            'payback': ['pay it forward', 'pay the pizza back', 'pay it back/forward', 'pay it back', 'pay back']\n",
      "        }\n",
      "    \n",
      "    def find_tag_words(self, keywords, text):\n",
      "        word_dict = {}\n",
      "        tag_dict = {}\n",
      "\n",
      "        for tag, words in keywords.iteritems():\n",
      "\n",
      "            tag_count = None\n",
      "\n",
      "            for word in words:\n",
      "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
      "                word_dict[word] = has_word\n",
      "\n",
      "                if tag_count is None:\n",
      "                    tag_count = has_word\n",
      "                else:\n",
      "                    tag_count = tag_count +  has_word\n",
      "\n",
      "            tag_dict[tag] = tag_count\n",
      "\n",
      "        return (tag_dict, word_dict)\n",
      "    \n",
      "    # manually create keywords with categories\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        do_title = self.do_title\n",
      "        do_tags = self.do_tags\n",
      "        do_words = self.do_words\n",
      "        do_body = self.do_body\n",
      "        keywords = self.keywords\n",
      "        find_tag_words = self.find_tag_words\n",
      "        body_col = self.body_col\n",
      "        title_col = self.title_col\n",
      "        \n",
      "        features = []\n",
      "        feature_names = []\n",
      "\n",
      "        # find keywords and tags\n",
      "        if do_title:\n",
      "            title_unicode = np.array([x.lower() for x in X[:,title_col]])\n",
      "            title_tag_dict, title_word_dict = find_tag_words(keywords, title_unicode)\n",
      "            \n",
      "            lens = lenArray(X[:,body_col])\n",
      "            \n",
      "            if do_tags:\n",
      "                features.append(pd.DataFrame(title_tag_dict).values/lens)\n",
      "                feature_names.append('title_tags')\n",
      "            if do_words:\n",
      "                features.append(pd.DataFrame(title_word_dict).values/lens)\n",
      "                feature_names.append('title_words')\n",
      "\n",
      "        if do_body:\n",
      "            body_unicode = np.array([x.lower() for x in X[:,body_col]])\n",
      "            body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
      "\n",
      "            lens = lenArray(X[:,body_col])\n",
      "            #print body_len\n",
      "            \n",
      "            if do_tags:\n",
      "                features.append(pd.DataFrame(body_tag_dict).values/lens)\n",
      "                feature_names.append('body_tags')\n",
      "\n",
      "            if do_words:\n",
      "                features.append(pd.DataFrame(body_word_dict).values/lens)\n",
      "                feature_names.append('body_words')\n",
      "\n",
      "        return np.hstack(tuple(features))\n",
      "    \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {'do_words': self.do_words, 'do_tags':self.do_tags, 'do_body':self.do_body, 'do_title':self.do_title}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Explore different models focusing on interesting words in tags and text:\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_oob = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           oob_score = True,\n",
      "                           bootstrap=True,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "\n",
      "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_pca = Pipeline([('scale', StandardScaler()),('pca', RandomizedPCA(n_components=3)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_l1 = Pipeline([('scale', StandardScaler()),('l1', LinearWeightFeatureThreshold(C=0.05)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "### APPENDIX?? - remove Linear SVC, both with and w/o PCA, since perf is bad.\n",
      "\n",
      "models = {'Extra Trees':etc, 'Gradient Boosting':gbc, 'Linear SVC':lsvc, 'Linear SVC PCA 3':lsvc_pca}\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Tags'\n",
      "trans = InterestingWordsTransformer(do_words=False)\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "    \n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n",
      "\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Words'\n",
      "trans = InterestingWordsTransformer(do_tags=False)\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "\n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n",
      "\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Words & Tags'\n",
      "trans = InterestingWordsTransformer()\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "\n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "##############\n",
        "Body & Title Tags\n",
        "\n",
        "Linear SVC PCA 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.520237, Median: 0.521332, SD: 0.029144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.512548, Median: 0.519883, SD: 0.026329"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 10, Mean: 0.496333, Median: 0.497730, SD: 0.027104"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.498539, Median: 0.496973, SD: 0.028433"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting\n",
        "N: 10, Mean: 0.559693, Median: 0.571710, SD: 0.026278"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.560550, Median: 0.563709, SD: 0.018093"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 10, Mean: 0.551301, Median: 0.556353, SD: 0.021962"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.554511, Median: 0.560185, SD: 0.023092"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "##############\n",
        "Body & Title Words\n",
        "\n",
        "Linear SVC PCA 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.489260, Median: 0.496667, SD: 0.029718"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.515717, Median: 0.508281, SD: 0.021669"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 10, Mean: 0.509117, Median: 0.500618, SD: 0.028559"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.510415, Median: 0.507585, SD: 0.027677"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting\n",
        "N: 10, Mean: 0.547426, Median: 0.541228, SD: 0.020665"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.548792, Median: 0.547379, SD: 0.025514"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 10, Mean: 0.563883, Median: 0.568168, SD: 0.022157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.559496, Median: 0.560109, SD: 0.022653"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "##############\n",
        "Body & Title Words & Tags\n",
        "\n",
        "Linear SVC PCA 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.475488, Median: 0.476664, SD: 0.030382"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.497049, Median: 0.500655, SD: 0.031975"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 10, Mean: 0.511267, Median: 0.503705, SD: 0.031924"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.509466, Median: 0.498373, SD: 0.031066"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting\n",
        "N: 10, Mean: 0.551670, Median: 0.557716, SD: 0.030437"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.557066, Median: 0.561948, SD: 0.022624"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 10, Mean: 0.560490, Median: 0.569074, SD: 0.020859"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.561783, Median: 0.566197, SD: 0.022942"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Interesting Words\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
      "<td>0.5547</td>\n",
      "<td>0.5436</td>\n",
      "<td>0.0284</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
      "<td>0.5950</td>\n",
      "<td>0.5987</td>\n",
      "<td>0.0208</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Time Features</td>\n",
      "<td>0.5252</td>\n",
      "<td>0.5223</td>\n",
      "<td>0.0229</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Interesting Words in Body</td>\n",
      "<td>0.5639</td>\n",
      "<td>0.5682</td>\n",
      "<td>0.0222</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part6\"></a>\n",
      "## 6. Spelling Mistakes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### NOT SURE WHERE THIS IS IN OUR PRIOR WORK"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Spelling Mistakes\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
      "<td>0.5547</td>\n",
      "<td>0.5436</td>\n",
      "<td>0.0284</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
      "<td>0.5950</td>\n",
      "<td>0.5987</td>\n",
      "<td>0.0208</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Time Features</td>\n",
      "<td>0.5252</td>\n",
      "<td>0.5223</td>\n",
      "<td>0.0229</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Interesting Words in Body</td>\n",
      "<td>0.5639</td>\n",
      "<td>0.5682</td>\n",
      "<td>0.0222</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>[Spelling Analysis here]</td>\n",
      "<td>1</td>\n",
      "<td>1</td>\n",
      "<td>1</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part7\"></a>\n",
      "## 7. Text Summary Features: Text Length"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Reusable class for text length:\n",
      "\n",
      "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "class TextSummaryTransformer(TransformerMixin):\n",
      "    def __init__(self, title_col=TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True):\n",
      "        self.do_title = do_title\n",
      "        self.do_body = do_body\n",
      "        self.title_col = title_col\n",
      "        self.body_col = body_col\n",
      "\n",
      "    def transform(self, X, **transform_params):\n",
      "        do_title = self.do_title\n",
      "        do_body = self.do_body\n",
      "        title_col = self.title_col\n",
      "        body_col = self.body_col\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        if do_title:\n",
      "            title_unicode = X[:, title_col]\n",
      "            title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "            features.append(title_len)\n",
      "            \n",
      "        if do_body:\n",
      "            body_unicode = X[:, body_col]\n",
      "            body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "            features.append(body_len)\n",
      "        \n",
      "        return np.hstack(tuple(features))\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self \n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "### APPENDIX? - SVC again has a low score, could cut it.\n",
      "print '\\nSVC on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('svc', svc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nExtra Trees Classifier on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('etc', etc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nGradient Boosting on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('gbc', gbc)])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC on title and body length\n",
        "N: 10, Mean: 0.487884, Median: 0.484572, SD: 0.013913"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees Classifier on title and body length\n",
        "N: 10, Mean: 0.571502, Median: 0.572088, SD: 0.026467"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting on title and body length\n",
        "N: 10, Mean: 0.568278, Median: 0.565342, SD: 0.019419"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Text Summary Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5553</td>\n",
      "<td>0.5540</td>\n",
      "<td>0.0258</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
      "<td>0.5547</td>\n",
      "<td>0.5436</td>\n",
      "<td>0.0284</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
      "<td>0.5950</td>\n",
      "<td>0.5987</td>\n",
      "<td>0.0208</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Time Features</td>\n",
      "<td>0.5252</td>\n",
      "<td>0.5223</td>\n",
      "<td>0.0229</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Interesting Words in Body</td>\n",
      "<td>0.5639</td>\n",
      "<td>0.5682</td>\n",
      "<td>0.0222</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>[Spelling Analysis here]</td>\n",
      "<td>1</td>\n",
      "<td>1</td>\n",
      "<td>1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Extra Trees on Title and Body Length</td>\n",
      "<td>0.5715</td>\n",
      "<td>0.5721</td>\n",
      "<td>0.0265</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part8\"></a>\n",
      "## 8. Location Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Method for pulling out geographies from RAOP entries\n",
      "\n",
      "MANUAL_GEOS = [\n",
      "{'loc':'nyc', 'g1':'ny', 'g2':'us'},\n",
      "{'loc':'sf', 'g1':'ca', 'g2':'us'},\n",
      "{'loc':'uk', 'g1':'uk', 'g2':'non_us'},\n",
      "{'loc':'australia', 'g1':'aus', 'g2':'non_us'},\n",
      "{'loc':'canada', 'g1':'can', 'g2':'non_us'},\n",
      "{'loc':'ottawa', 'g1':'can', 'g2':'non_us'},\n",
      "{'loc':'toronto', 'g1':'can', 'g2':'non_us'},\n",
      "{'loc':'vancouver', 'g1':'can', 'g2':'non_us'},\n",
      "{'loc':'montreal', 'g1':'can', 'g2':'non_us'}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def make_geo(other_geos=MANUAL_GEOS, filter_loc=[]):\n",
      "\n",
      "    from bs4 import BeautifulSoup\n",
      "    from urllib import urlopen\n",
      "    import re\n",
      "    \n",
      "    ######################\n",
      "    # Scrape wikipedia list of us cities\n",
      "    \n",
      "    # TODO save local\n",
      "    webpage = urlopen('http://en.wikipedia.org/wiki/List_of_United_States_cities_by_population')\n",
      "    soup=BeautifulSoup(webpage, \"html.parser\")\n",
      "    table = soup.find('table', {'class' : 'wikitable sortable'})\n",
      "    \n",
      "    us_cities = []\n",
      "\n",
      "    rows = table.findAll('tr')\n",
      "    for row in rows[1:200]:\n",
      "        cells = row.findAll('td')\n",
      "\n",
      "        output = []\n",
      "\n",
      "        for i, cell in enumerate(cells):\n",
      "            if i < 4:\n",
      "                text = cell.text.strip().lower()\n",
      "                if i == 0:\n",
      "                    text = int(text)\n",
      "                if i == 1 or i == 2:\n",
      "                    text = re.sub(r\"\\[.*\\]|'\",'',text)\n",
      "                if i == 3:\n",
      "                    text = int(re.sub(r',','',text))\n",
      "                output.append(text)\n",
      "        us_cities.append(output)\n",
      "\n",
      "    us_cities = pd.DataFrame(np.array(us_cities),columns=['rank','city','state','pop'])\n",
      "    \n",
      "    ###########################\n",
      "    # tuple list of state abbreviations\n",
      "    \n",
      "    state_abr_raw = [(\"Alabama\",\"AL\"),(\"Alaska\",\"AK\"),(\"Arizona\",\"AZ\"),\n",
      "                     (\"Arkansas\",\"AR\"),(\"California\",\"CA\"),(\"Colorado\",\"CO\"),\n",
      "                     (\"Connecticut\",\"CT\"),(\"Delaware\",\"DE\"),(\"District of Columbia\",\"DC\"),\n",
      "                     (\"Florida\",\"FL\"),(\"Georgia\",\"GA\"),(\"Hawaii\",\"HI\"),\n",
      "                     (\"Idaho\",\"ID\"),(\"Illinois\",\"IL\"),(\"Indiana\",\"IN\"),\n",
      "                     (\"Iowa\",\"IA\"),(\"Kansas\",\"KS\"),(\"Kentucky\",\"KY\"),\n",
      "                     (\"Louisiana\",\"LA\"),(\"Maine\",\"ME\"),(\"Montana\",\"MT\"),\n",
      "                     (\"Nebraska\",\"NE\"),(\"Nevada\",\"NV\"),(\"New Hampshire\",\"NH\"),\n",
      "                     (\"New Jersey\",\"NJ\"),(\"New Mexico\",\"NM\"),(\"New York\",\"NY\"),\n",
      "                     (\"North Carolina\",\"NC\"),(\"North Dakota\",\"ND\"),(\"Ohio\",\"OH\"),\n",
      "                     (\"Oklahoma\",\"OK\"),(\"Oregon\",\"OR\"),(\"Maryland\",\"MD\"),\n",
      "                     (\"Massachusetts\",\"MA\"),(\"Michigan\",\"MI\"),(\"Minnesota\",\"MN\"),\n",
      "                     (\"Mississippi\",\"MS\"),(\"Missouri\",\"MO\"),(\"Pennsylvania\",\"PA\"),\n",
      "                     (\"Rhode Island\",\"RI\"),(\"South Carolina\",\"SC\"),(\"South Dakota\",\"SD\"),\n",
      "                     (\"Tennessee\",\"TN\"),(\"Texas\",\"TX\"),(\"Utah\",\"UT\"),\n",
      "                     (\"Vermont\",\"VT\"),(\"Virginia\",\"VA\"),(\"Washington\",\"WA\"),\n",
      "                     (\"West Virginia\",\"WV\"),(\"Wisconsin\",\"WI\"),(\"Wyoming\",\"WY\")]\n",
      "    \n",
      "    ############################\n",
      "    # manupulate state abreviations\n",
      "    state_abr = []\n",
      "    for st, abr in state_abr_raw:\n",
      "        state_abr.append([st.lower(), abr.lower()])\n",
      "    state_abr = pd.DataFrame(np.array(state_abr), columns = ['state','abr'])\n",
      "    \n",
      "    #############################\n",
      "    # make US Geos\n",
      "    us_city_state = pd.merge(us_cities,state_abr)\n",
      "    \n",
      "    # US geos\n",
      "    usgeo = us_city_state.loc[:,['city','abr']]\n",
      "    usgeo.columns = ['loc','g1']\n",
      "    usgeo = pd.concat([usgeo,pd.DataFrame({'loc':state_abr.abr,'g1':state_abr.abr})])\n",
      "    usgeo = pd.concat([usgeo,pd.DataFrame({'loc':state_abr.state,'g1':state_abr.abr})])\n",
      "    usgeo['g2'] = 'us'\n",
      "    \n",
      "    \n",
      "    geo = pd.concat([usgeo, pd.DataFrame(other_geos)])\n",
      "    \n",
      "    # get rid of auto generated locations with confusiong names\n",
      "    #geo = geo[[not x in filter_loc for x in geo['loc']]]\n",
      "    \n",
      "    return geo\n",
      "\n",
      "\n",
      "\n",
      "geo = make_geo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'module' object is not callable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-41-27ddae0f4767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mgeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_geo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-41-27ddae0f4767>\u001b[0m in \u001b[0;36mmake_geo\u001b[0;34m(other_geos, filter_loc)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# TODO save local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mwebpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://en.wikipedia.org/wiki/List_of_United_States_cities_by_population'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msoup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'wikitable sortable'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part9\"></a>\n",
      "## 9. Parts of Speech"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part10\"></a>\n",
      "## 10. Final, Composite Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Results Table += Activity Features\n",
      "\n",
      "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>Method</th>\n",
      "<th>Mean ROC-AUC</th>\n",
      "<th>Median ROC-AUC</th>\n",
      "<th>Standard Deviation</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Activity Features with Reweighted Classes</td>\n",
      "<td>0.5456</td>\n",
      "<td>0.5465</td>\n",
      "<td>0.0164</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "<a href=\"#top\">Return to Table of Contents</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"part11\"></a>\n",
      "## 11. Appendix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}