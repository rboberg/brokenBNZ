{
 "metadata": {
  "name": "",
  "signature": "sha256:9ef80ddfa18664fcde2f7609b01301357bd86d8c841f073bb67846cecdd1035b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import pandas as pd\n",
      "import scipy as scipy\n",
      "import datetime as dt\n",
      "import time\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import make_scorer\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "\n",
      "import gensim\n",
      "\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "\n",
      "def balance_samples(y, method='oversample'):\n",
      "    class_counts = np.bincount(y)\n",
      "    \n",
      "    maxi = np.argmax(class_counts)\n",
      "    new_idx = np.argwhere(y==maxi)\n",
      "    \n",
      "    for i in range(len(class_counts)):\n",
      "        if i != maxi:\n",
      "            mult = class_counts[maxi]/class_counts[i]\n",
      "            rem = class_counts[maxi] - class_counts[i]*mult\n",
      "            idxi = np.argwhere(y==i)\n",
      "            np.random.shuffle(idxi)\n",
      "            for j in range(mult):\n",
      "                new_idx = np.vstack((new_idx,idxi))\n",
      "            new_idx = np.vstack((new_idx,idxi[:rem]))\n",
      "        \n",
      "    np.random.shuffle(new_idx)\n",
      "\n",
      "    return np.reshape(new_idx, (new_idx.shape[0],))\n",
      "\n",
      "def oversample_kfold(kf, y):\n",
      "    kf_over = []\n",
      "    for ti, di in kf:\n",
      "        yt = y[ti]\n",
      "        ti_over = ti[balance_samples(yt)]\n",
      "        kf_over.append((ti_over, di))\n",
      "    return kf_over\n",
      "\n",
      "def name2index(df, names):\n",
      "    return_single = False\n",
      "    \n",
      "    if type(names) == type([]):\n",
      "       names = np.array(names)\n",
      "    elif type(names) != type(np.array([])):\n",
      "        names = np.array([names])\n",
      "        return_single = True \n",
      "    \n",
      "    inds = np.where(np.in1d(df.columns, np.array(names)))[0]\n",
      "    \n",
      "    return inds[0] if return_single else inds\n",
      "\n",
      "def print_scores(scores):\n",
      "    print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(scores), np.mean(scores), np.median(scores), np.std(scores))\n",
      "            \n",
      "def test_kfolds(X, y, kf, model, verbose=1, balance=False):\n",
      "    roc_auc_list = []\n",
      "    \n",
      "    for train_i, dev_i in kf:\n",
      "        if balance:\n",
      "            train_i_orig = train_i\n",
      "            y_train = y[train_i_orig]\n",
      "            train_i = train_i_orig[balance_samples(y_train)]\n",
      "        \n",
      "        \n",
      "        X_train = X[train_i]\n",
      "        X_dev = X[dev_i]\n",
      "\n",
      "        model.fit(X_train, y[train_i])\n",
      "\n",
      "        dev_pred = model.predict(X_dev)\n",
      "        \n",
      "        roc_auc_i = roc_auc_score(y[dev_i], dev_pred)\n",
      "        roc_auc_list.append(roc_auc_i)\n",
      "        if verbose > 1:\n",
      "            print('ROC AUC:',roc_auc_i)\n",
      "            \n",
      "    if verbose > 0:\n",
      "        print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(kf), np.mean(roc_auc_list), np.median(roc_auc_list), np.std(roc_auc_list))\n",
      "        \n",
      "    return roc_auc_list\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#useful for text processing\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class SnowballStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = SnowballStemmer('english')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "class PorterStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data from JSON file as list of dicts\n",
      "all_train_dict_list = load_json_file('../../data/train.json')\n",
      "submit_dict_list =  load_json_file('../../data/test.json')\n",
      "\n",
      "n_all = len(all_train_dict_list)\n",
      "n_submit = len(submit_dict_list)\n",
      "\n",
      "# shuffle data to avoid biased split of train / dev data\n",
      "rand.shuffle(all_train_dict_list)\n",
      "\n",
      "# set up kFolds\n",
      "kf = KFold(n_all, n_folds = 5)\n",
      "\n",
      "# process labels\n",
      "all_train_labels = np.array([x['requester_received_pizza'] for x in all_train_dict_list])\n",
      "\n",
      "# pandas is useful for turning dicts in to matrix-like objects\n",
      "# where each column is an numpy array\n",
      "submit_df = pd.DataFrame(submit_dict_list)\n",
      "all_train_df = pd.DataFrame(all_train_dict_list)\n",
      "\n",
      "# limit train to columns available in submit_df\n",
      "submit_cols = submit_df.columns\n",
      "all_train_df = all_train_df[submit_cols]\n",
      "\n",
      "# useful for sklearn scoring\n",
      "roc_scorer = make_scorer(roc_auc_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = all_train_labels\n",
      "\n",
      "kf_over = []\n",
      "for ti, di in kf:\n",
      "    yt = y[ti]\n",
      "    ti_over = ti[balance_samples(yt)]\n",
      "    kf_over.append((ti_over, di))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Set up numeric Activity Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ACTIVITY_VARS = ['requester_account_age_in_days_at_request',\n",
      "                'requester_days_since_first_post_on_raop_at_request',\n",
      "                'requester_number_of_comments_at_request',\n",
      "                'requester_number_of_comments_in_raop_at_request',\n",
      "                'requester_number_of_posts_at_request',\n",
      "                'requester_number_of_posts_on_raop_at_request',\n",
      "                'requester_number_of_subreddits_at_request',\n",
      "                'requester_upvotes_minus_downvotes_at_request',\n",
      "                'requester_upvotes_plus_downvotes_at_request'\n",
      "                ]\n",
      "ACTIVITY_COLUMNS = name2index(all_train_df, ACTIVITY_VARS)\n",
      "\n",
      "class ExtractColumnsTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, cols=[0]):\n",
      "        self.cols = cols\n",
      "        \n",
      "    def fit(self, *args, **kwargs):\n",
      "        return self\n",
      "        \n",
      "    def transform(self, X):\n",
      "        cols = self.cols\n",
      "        return X[:,cols]\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "class ExtractActivities(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, ACTIVITY_COLUMNS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Importance of Weighting Classes Appropriately"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example classification\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print 'Equal Class Weights'\n",
      "pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC())])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nReweighted Classes'\n",
      "wt_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC(class_weight='auto'))])\n",
      "print_scores(cross_val_score(wt_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nRebalanced Sample'\n",
      "rebal_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc',SVC())])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(rebal_pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Equal Class Weights\n",
        "N: 5, Mean: 0.512159, Median: 0.512660, SD: 0.004850"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Reweighted Classes\n",
        "N: 5, Mean: 0.550830, Median: 0.549258, SD: 0.019615"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Rebalanced Sample\n",
        "N: 5, Mean: 0.548333, Median: 0.548132, SD: 0.011720"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Simple Text Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = name2index(all_train_df, 'request_title')\n",
      "BODY_COLUMN = name2index(all_train_df, 'request_text_edit_aware')\n",
      "\n",
      "class ExtractBody(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, BODY_COLUMN)\n",
      "        \n",
      "\n",
      "class ExtractTitle(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, TITLE_COLUMN)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def simple_text(do_all=True, do_count=False,do_tfidf=False, do_titles=False, do_bodies=False, do_both=False, lowercase=False, tokenizer=None, stop_words=None):\n",
      "\n",
      "    # Notes\n",
      "    # results slightly better w/ lowercase = False (when unigrams only)\n",
      "    # bigrams added no value on unigrams\n",
      "\n",
      "    tv = TfidfVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    cv = CountVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "    \n",
      "    body_cv = Pipeline([('body',ExtractBody()),('cv', cv)])\n",
      "    body_tv = Pipeline([('body',ExtractBody()),('tv', tv)])\n",
      "    \n",
      "    title_cv = Pipeline([('title',ExtractTitle()),('cv', cv)])\n",
      "    title_tv = Pipeline([('title',ExtractTitle()),('tv', tv)])\n",
      "\n",
      "    if do_titles or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Titles\n",
      "            print '\\nCount Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer TItles\n",
      "            print '\\nTFIDF Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "    if do_bodies or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Bodies\n",
      "            print '\\nCount Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Bodies\n",
      "            print '\\nTFIDF Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        \n",
      "    if do_both or do_all:\n",
      "        if do_count or do_all:\n",
      "\n",
      "            # Count Vectorizer Titles and Bodies\n",
      "            print '\\nCount Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_cv),\n",
      "                    ('tranform_body',body_cv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "            \n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Titles and Bodies\n",
      "            print '\\nTFIDF Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_tv),\n",
      "                    ('tranform_body',body_tv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_text(do_all = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Count Vectorizer on Titles\n",
        "N: 5, Mean: 0.522600, Median: 0.519174, SD: 0.015706"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles\n",
        "N: 5, Mean: 0.526755, Median: 0.532251, SD: 0.017752"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Bodies\n",
        "N: 5, Mean: 0.526150, Median: 0.533957, SD: 0.020856"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Bodies\n",
        "N: 5, Mean: 0.540730, Median: 0.553200, SD: 0.025648"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.538302, Median: 0.548932, SD: 0.020129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.543395, Median: 0.537019, SD: 0.021522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lowercase seems to be a bit worse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '\\nNot Lower Case'\n",
      "simple_text(do_all=False, do_both=True, lowercase=False, do_tfidf=True)\n",
      "print '\\nLower Case'\n",
      "simple_text(do_all=False, do_both=True, lowercase=True, do_tfidf=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Not Lower Case\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.543395, Median: 0.537019, SD: 0.021522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Lower Case\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.540842, Median: 0.535160, SD: 0.017312"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try more complex tokenizers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=None)\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=SnowballStemTokenizer())\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.543395, Median: 0.537019, SD: 0.021522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.558928, Median: 0.558172, SD: 0.018467"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.554855, Median: 0.538551, SD: 0.022932"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=None, stop_words='english', lowercase=True)\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer(), stop_words='english', lowercase=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.534842, Median: 0.529285, SD: 0.015442"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.556065, Median: 0.555128, SD: 0.008675"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Text Feature Selection and Dimensionality Reduciton"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LinearWeightFeatureThreshold(TransformerMixin):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model = LinearSVC(class_weight='auto', loss='squared_hinge', penalty='l1', dual=False),\n",
      "        return_dense = True,\n",
      "        C = 0.15,\n",
      "        threshold = 0.01,\n",
      "        verbose = 1\n",
      "        ):\n",
      "        self.model = model\n",
      "        self.return_dense = return_dense\n",
      "        self.C = C\n",
      "        self.threshold = threshold\n",
      "        self.verbose = verbose\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        model = self.model\n",
      "        threshold = self.threshold\n",
      "        verbose = self.verbose\n",
      "        C = self.C\n",
      "        \n",
      "        model.set_params(C=C)\n",
      "        \n",
      "        model.fit(X, y)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > threshold)[0]\n",
      "        if verbose > 0:\n",
      "            print 'kept %d/%d features' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "        self.sig_coef_  = sig_coef\n",
      "        return self\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        sig_coef = self.sig_coef_\n",
      "        return_dense = self.return_dense\n",
      "        \n",
      "        X_new = X[:,sig_coef]\n",
      "        \n",
      "        if return_dense and (type(X_new) != type(np.array(1))):\n",
      "            X_new = X_new.toarray()\n",
      "            \n",
      "        return X_new\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {'C':self.C, 'threshold':self.threshold}\n",
      "    \n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            setattr(self, parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=False, tokenizer=SnowballStemTokenizer())\n",
      "l1 = LinearWeightFeatureThreshold()\n",
      "lsvc = LinearSVC(class_weight='auto')\n",
      "\n",
      "pipe_lsvc = Pipeline([('tv',tv), ('features',l1), ('lsvc',lsvc)])\n",
      "\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv), ('features',l1)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv), ('features',l1)])\n",
      "\n",
      "print '\\nL1 Feature Reduction on Titles w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([('title', title_pipe), ('lsvc', lsvc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([('body', body_pipe), ('lsvc', lsvc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "print '\\nL1 Feature Reduction on Both w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Union w/ LinearSVC'\n",
      "\n",
      "l1 = LinearWeightFeatureThreshold(C=0.15)\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv)])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=0.15)),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Both Individually and on Union w/ LinearSVC'\n",
      "\n",
      "l1 = LinearWeightFeatureThreshold(C=0.25)\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv), ('l1',l1)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv), ('l1',l1)])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=.25)),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "L1 Feature Reduction on Titles w/ LinearSVC\n",
        "kept 840/3666 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 842/3664 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 805/3650 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 821/3651 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 810/3643 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.534637, Median: 0.531678, SD: 0.011831"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Bodies w/ LinearSVC\n",
        "kept 731/9159 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 720/9309 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 731/9334 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 703/9339 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 713/9195 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.554203, Median: 0.549947, SD: 0.017372"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Both w/ LinearSVC\n",
        "kept 834/3666 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 730/9159 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 847/3664 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 720/9309 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 801/3650 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 728/9334 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 819/3651 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 702/9339 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 816/3643 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 713/9195 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.558621, Median: 0.554429, SD: 0.019961"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Union w/ LinearSVC\n",
        "kept 60/12825 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/12973 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 66/12984 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/12990 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/12838 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.585933, Median: 0.590966, SD: 0.019714"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Both Individually and on Union w/ LinearSVC\n",
        "kept 103/3666 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 97/9159 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 165/200 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 122/3664 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 87/9309 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 161/209 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 114/3650 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 101/9334 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 164/215 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 110/3651 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 85/9339 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 164/195 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 114/3643 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 92/9195 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 174/206 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.567876, Median: 0.572896, SD: 0.020524"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l1 = LinearWeightFeatureThreshold()\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "pipe_etc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',etc)])\n",
      "pipe_gbc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',gbc)])\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ ETC'\n",
      "print_scores(cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ GBC'\n",
      "#need to oversample GBC b/c no class_weight\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "print_scores(cross_val_score(pipe_gbc, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "L1 Feature Reduction on Bodies w/ ETC\n",
        "kept 732/9159 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 719/9309 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 731/9334 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 701/9339 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 714/9195 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.594775, Median: 0.597506, SD: 0.022445"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Bodies w/ GBC\n",
        "kept 1236/9159 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1237/9309 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1271/9334 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1220/9339 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 1252/9195 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.549622, Median: 0.546092, SD: 0.009645"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Latent Symantic Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import text\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name PunktWordTokenizer",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-22-93f4d34762a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunkt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPunktWordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mImportError\u001b[0m: cannot import name PunktWordTokenizer"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer()\n",
      "docs = ExtractBody().transform(all_train_df.values)\n",
      "stop_words = text.ENGLISH_STOP_WORDS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "? word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PunktWordTokenizer(docs[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'PunktWordTokenizer' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-37-67e9739345f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPunktWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'PunktWordTokenizer' is not defined"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mytokenizer(text, lowercase=True, stops=True):\n",
      "    if lowercase:\n",
      "        text = text.lower()\n",
      "    tokenizer = RegexpTokenizer(r'\\w+')\n",
      "    tokens = tokenizer.tokenize(text)\n",
      "    if stops:\n",
      "        filtered = [w for w in tokens if not w in stop_words]\n",
      "    else:\n",
      "        filtered = tokens\n",
      "    return filtered\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mytokenizer(docs[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "[u'don',\n",
        " u't',\n",
        " u'mean',\n",
        " u'beg',\n",
        " u'left',\n",
        " u'care',\n",
        " u'parents',\n",
        " u'dogs',\n",
        " u'week',\n",
        " u'didn',\n",
        " u't',\n",
        " u'leave',\n",
        " u'money',\n",
        " u'week',\n",
        " u'buy',\n",
        " u'groceries',\n",
        " u'left',\n",
        " u'saturday',\n",
        " u'haven',\n",
        " u't',\n",
        " u'really',\n",
        " u'eat',\n",
        " u'd',\n",
        " u'grab',\n",
        " u'groceries',\n",
        " u'just',\n",
        " u'huge',\n",
        " u'school',\n",
        " u'payment',\n",
        " u'doing',\n",
        " u'pretty',\n",
        " u'lousy',\n",
        " u'producer',\n",
        " u'musician',\n",
        " u'd',\n",
        " u'love',\n",
        " u'discuss',\n",
        " u'works',\n",
        " u'check',\n",
        " u'tracks',\n",
        " u'll',\n",
        " u'try',\n",
        " u'advertise',\n",
        " u'work',\n",
        " u'gratitude',\n",
        " u's',\n",
        " u'stuff',\n",
        " u'https',\n",
        " u'soundcloud',\n",
        " u'com',\n",
        " u'cjrsongs',\n",
        " u'12pm',\n",
        " u'est']"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Time Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATE_TIME_COLUMN_DEFAULT = np.where(all_train_df.columns == 'unix_timestamp_of_request')[0][0]\n",
      "\n",
      "class TimeTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, date_time_column=DATE_TIME_COLUMN_DEFAULT, do_hour=True, do_dow=True, do_month=True):\n",
      "        self.date_time_column = date_time_column\n",
      "        self.do_hour = do_hour\n",
      "        self.do_dow = do_dow\n",
      "        self.do_month = do_month\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        return self\n",
      "    \n",
      "    def extract_from_date_time_(self, dt, do_hour, do_dow, do_month):\n",
      "        extract = []\n",
      "        if do_hour:\n",
      "            extract.append(dt.hour)\n",
      "            \n",
      "        if do_dow:\n",
      "            extract.append(dt.weekday())\n",
      "            \n",
      "        if do_month:\n",
      "            extract.append(dt.month)\n",
      "            \n",
      "        return extract\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        date_time_column = self.date_time_column\n",
      "        do_hour = self.do_hour\n",
      "        do_dow = self.do_dow\n",
      "        do_month = self.do_month\n",
      "        extract_from_date_time = self.extract_from_date_time_\n",
      "        \n",
      "        features = np.array([\n",
      "            extract_from_date_time(dt.datetime.fromtimestamp(timei),\n",
      "                                   do_hour=do_hour,\n",
      "                                   do_dow=do_dow,\n",
      "                                   do_month=do_month) for timei in X[:,date_time_column]\n",
      "        ])\n",
      "        \n",
      "        return features\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exploring time features, it looks like requests are not as succesful at late nights / early mornings or on Mondays / Fridays - though that could be because there's more requests on those days."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# look at hourly success\n",
      "hour = TimeTransformer(do_dow=False, do_month=False).transform(all_train_df.values).flatten()\n",
      "hour_pos = hour[all_train_labels]\n",
      "hour_neg = hour[np.logical_not(all_train_labels)]\n",
      "pd.Series(hour_pos).hist(bins=24, alpha=0.2, normed=True)\n",
      "pd.Series(hour_neg).hist(bins=24, alpha=0.2, normed=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x10e0bfa10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJEmSJEmSJEmSJC3u/wO+1xxjrBNuZAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10e0bf910>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# look at day of week success\n",
      "dow = TimeTransformer(do_hour=False, do_month=False).transform(all_train_df.values).flatten()\n",
      "dow_pos = dow[all_train_labels]\n",
      "dow_neg = dow[np.logical_not(all_train_labels)]\n",
      "pd.Series(dow_pos).hist(bins=7, alpha=0.2, normed=True)\n",
      "pd.Series(dow_neg).hist(bins=7, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x10e0bf790>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1120f8750>"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# look at month success\n",
      "month = TimeTransformer(do_hour=False, do_dow=False).transform(all_train_df.values).flatten()\n",
      "month_pos = month[all_train_labels]\n",
      "month_neg = month[np.logical_not(all_train_labels)]\n",
      "pd.Series(month_pos).hist(bins=12, alpha=0.2, normed=True)\n",
      "pd.Series(month_neg).hist(bins=12, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x1120f6210>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREREREGs3/Bx0X+CHxQya1AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11203e150>"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Times not a very useful classifier by themselves, but maybe in conjuctions with others"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# classifier based on times\n",
      "\n",
      "svc = Pipeline([('scaler',StandardScaler()), ('svc',SVC(class_weight='auto'))])\n",
      "\n",
      "print '\\nSVC'\n",
      "svc_pipe = Pipeline([\n",
      "    ('time',TimeTransformer()),\n",
      "    ('model',svc)\n",
      "    ])\n",
      "\n",
      "print_scores(cross_val_score(svc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nExtra Tree Ensemble'\n",
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_pipe = Pipeline([\n",
      "    ('time',TimeTransformer()),\n",
      "    ('model',etc)\n",
      "    ])\n",
      "\n",
      "print_scores(cross_val_score(etc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC\n",
        "N: 5, Mean: 0.520903, Median: 0.522940, SD: 0.009087"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Tree Ensemble\n",
        "N: 5, Mean: 0.523238, Median: 0.519846, SD: 0.019957"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Text Summary Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "class TextSummaryTransformer(TransformerMixin):\n",
      "    def __init__(self, title_col=TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True):\n",
      "        self.do_title = do_title\n",
      "        self.do_body = do_body\n",
      "        self.title_col = title_col\n",
      "        self.body_col = body_col\n",
      "\n",
      "    def transform(self, X, **transform_params):\n",
      "        do_title = self.do_title\n",
      "        do_body = self.do_body\n",
      "        title_col = self.title_col\n",
      "        body_col = self.body_col\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        if do_title:\n",
      "            title_unicode = X[:, title_col]\n",
      "            title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "            features.append(title_len)\n",
      "            \n",
      "        if do_body:\n",
      "            body_unicode = X[:, body_col]\n",
      "            body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "            #body_para_count = np.array([[len(filter(None, x.splitlines())) for x in body_unicode]]).T\n",
      "            #body_para_density = np.divide(body_len,body_para_count)\n",
      "            features.append(body_len)\n",
      "            #features.append(body_para_count)\n",
      "            #features.append(body_para_density)\n",
      "        \n",
      "        return np.hstack(tuple(features))\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self \n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Title and Body character length\n",
      "Not at all useful based on linear classifiers... but lots of value added with tree ensembles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create some text derivative features\n",
      "# title character length\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "\n",
      "# body character length\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "\n",
      "\n",
      "X_text_summary = np.hstack((title_len, body_len))\n",
      "\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print '\\nSVC on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('svc', svc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nExtra Trees Classifier on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('etc', etc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nGradient Boosting on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('gbc', gbc)])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC on title and body length\n",
        "N: 5, Mean: 0.491194, Median: 0.490985, SD: 0.004870"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees Classifier on title and body length\n",
        "N: 5, Mean: 0.578049, Median: 0.578846, SD: 0.011138"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting on title and body length\n",
        "N: 5, Mean: 0.565212, Median: 0.563792, SD: 0.013208"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Word Case Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DEFAULT_TOKENIZER = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "# To calc the percent of words that are all upper, all lower, or mixed case\n",
      "class CaseAnalysisTransformer(TransformerMixin):\n",
      "    def __init__(self, upper=True, lower=True, mixed=True, tokenizer=DEFAULT_TOKENIZER):\n",
      "        self.upper = upper\n",
      "        self.lower = lower\n",
      "        self.mixed = mixed\n",
      "        self.tokenizer = tokenizer\n",
      "        \n",
      "    def count_case(self, words):\n",
      "        upper = 0.\n",
      "        lower = 0.\n",
      "        mixed = 0.\n",
      "        total = 1.\n",
      "        \n",
      "        for word in words:\n",
      "            isupper = word.isupper() \n",
      "            islower = word.islower() \n",
      "            \n",
      "            upper += isupper\n",
      "            lower += islower\n",
      "            mixed += not(isupper or islower)\n",
      "            total += 1\n",
      "            \n",
      "        return (upper, lower, mixed, total)\n",
      "    \n",
      "    \n",
      "    def tokenize_and_count(self, text, tokenizer):\n",
      "        return self.count_case(tokenizer.tokenize(text))\n",
      "    \n",
      "    def process_vector(self, texts, tokenizer):\n",
      "        ulist = []\n",
      "        llist = []\n",
      "        mlist = []\n",
      "        tlist = []\n",
      "        \n",
      "        for text in texts:\n",
      "            u, l, m, t = self.tokenize_and_count(text, tokenizer)\n",
      "            ulist.append(u)\n",
      "            llist.append(l)\n",
      "            mlist.append(m)\n",
      "            tlist.append(t)\n",
      "        \n",
      "        tar = np.array(tlist)\n",
      "        uar = np.array(ulist)/tar\n",
      "        lar = np.array(llist)/tar\n",
      "        mar = np.array(mlist)/tar\n",
      "        \n",
      "        return np.vstack((uar, lar, mar)).T\n",
      "        \n",
      "    def transform(self, X, **transform_params):\n",
      "        upper = self.upper\n",
      "        lower = self.lower\n",
      "        mixed = self.mixed\n",
      "        tokenizer = self.tokenizer\n",
      "        process_vector = self.process_vector\n",
      "        \n",
      "        return process_vector(X, tokenizer)\n",
      "        \n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc = LinearSVC(class_weight='auto')\n",
      "etc = ExtraTreesClassifier(n_estimators = 100,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "\n",
      "title_case = Pipeline([('col', ExtractTitle()), ('case', CaseAnalysisTransformer())])\n",
      "body_case = Pipeline([('col', ExtractBody()), ('case', CaseAnalysisTransformer())])\n",
      "both_case = FeatureUnion([('title', title_case), ('body', body_case)])\n",
      "\n",
      "features = both_case\n",
      "\n",
      "pipe_lsvc = Pipeline([('features', features), ('model', lsvc)])\n",
      "pipe_etc = Pipeline([('features', features), ('model', etc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "\n",
      "scores = cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)\n",
      "\n",
      "scores = cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.3s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.5s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.4s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.9s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.520258, Median: 0.520815, SD: 0.015244\n",
        "N: 5, Mean: 0.537489, Median: 0.536839, SD: 0.018353"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Paragraph Count and Density"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "class BodyParagraphSummaryTransformer(TransformerMixin):\n",
      "    def __init__(self, body_col=BODY_COLUMN):\n",
      "        self.body_col = body_col\n",
      "\n",
      "    def transform(self, X, **transform_params):\n",
      "        body_col = self.body_col\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        body_unicode = X[:, body_col]\n",
      "        body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "        body_para_count = np.array([[len(filter(None, x.splitlines())) for x in body_unicode]]).T\n",
      "        body_para_density = np.divide(body_len,body_para_count)\n",
      "        features.append(body_para_count)\n",
      "        features.append(body_para_density)\n",
      "        \n",
      "        return np.hstack(tuple(features))\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self \n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print '\\nSVC on paragraph features'\n",
      "pipe = Pipeline([('paragraph_summary', BodyParagraphSummaryTransformer()), ('svc', svc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nExtra Trees Classifier on paragraph features'\n",
      "pipe = Pipeline([('paragraph_summary', BodyParagraphSummaryTransformer()), ('etc', etc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nGradient Boosting on paragraph features'\n",
      "pipe = Pipeline([('paragraph_summary', BodyParagraphSummaryTransformer()), ('gbc', gbc)])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC on paragraph features\n",
        "N: 5, Mean: 0.529070, Median: 0.530994, SD: 0.015165"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees Classifier on paragraph features\n",
        "N: 5, Mean: 0.565071, Median: 0.561775, SD: 0.022444"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting on paragraph features\n",
        "N: 5, Mean: 0.551794, Median: 0.549071, SD: 0.021209"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Words in Brown Corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "from nltk.tokenize.regexp import RegexpTokenizer\n",
      "from sklearn.feature_extraction import text as sklearn_text\n",
      "brown_words = np.unique(np.array(brown.words()))\n",
      "brown_words = np.unique(np.array([x.lower() for x in brown_words]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "DEFAULT_WORD_SET = set(brown_words)\n",
      "DEFAULT_STOP_WORDS = set(['a','request','the','and','for'])\n",
      "DEFAULT_TOKENIZER = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
      "\n",
      "# To calc the percent of words that are all upper, all lower, or mixed case\n",
      "class InCorpusTransformer(TransformerMixin):\n",
      "    def __init__(self, word_set=DEFAULT_WORD_SET, tokenizer=DEFAULT_TOKENIZER, stop_words=DEFAULT_STOP_WORDS):\n",
      "        self.word_set = word_set\n",
      "        self.tokenizer = tokenizer\n",
      "        self.stop_words = stop_words\n",
      "        self.tokenizer = tokenizer\n",
      "    \n",
      "    def count_tokens(self, tokens):\n",
      "        if len(tokens) == 0:\n",
      "            return 2\n",
      "        else:\n",
      "            return sum(np.array([token.lower() in self.word_set for token in tokens]))/float(len(tokens))\n",
      "    \n",
      "    def tokenize_and_count(self, text):\n",
      "        tokens = [x.lower() for x in self.tokenizer.tokenize(text) if x.lower() not in self.stop_words]\n",
      "        return self.count_tokens(tokens)\n",
      "    \n",
      "    def process_vector(self, texts):\n",
      "        return np.array([[self.tokenize_and_count(text) for text in texts]]).T\n",
      "        \n",
      "    def transform(self, X, **transform_params):\n",
      "        if len(X.shape) == 1:\n",
      "            return self.process_vector(X)\n",
      "        else:\n",
      "            features = []\n",
      "            for col in range(X.shape[1]):\n",
      "                features.append(self.process_vector(X[:,col]))\n",
      "            return np.hstack(tuple(features))\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 543
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc = LinearSVC(class_weight='auto')\n",
      "etc = ExtraTreesClassifier(n_estimators = 100,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "\n",
      "title = Pipeline([('col', ExtractTitle()), ('incorpus', InCorpusTransformer())])\n",
      "body = Pipeline([('col', ExtractBody()), ('incorpus', InCorpusTransformer())])\n",
      "both = FeatureUnion([('title', title), ('body', body)])\n",
      "\n",
      "features = both\n",
      "\n",
      "pipe_lsvc = Pipeline([('features', features), ('model', lsvc)])\n",
      "pipe_etc = Pipeline([('features', features), ('model', etc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "\n",
      "scores = cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)\n",
      "\n",
      "scores = cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    1.4s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.8s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    1.5s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.3s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.516077, Median: 0.508250, SD: 0.011299\n",
        "N: 5, Mean: 0.523190, Median: 0.525438, SD: 0.015155"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Interesting Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "class InterestingWordsTransformer(TransformerMixin):\n",
      "    def __init__(self, title_col = TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True, do_tags=True, do_words=True):\n",
      "        self.do_title = do_title\n",
      "        self.do_body = do_body\n",
      "        self.do_tags = do_tags\n",
      "        self.do_words = do_words\n",
      "        self.title_col = title_col\n",
      "        self.body_col = body_col\n",
      "        \n",
      "        self.keywords = {\n",
      "            'sad_food': ['hungry', 'starving', 'no food', 'grocer', 'eaten', 'hunger', 'ramen', 'empty', 'fridge', 'refrig'],\n",
      "            'money': ['broke', 'paid', 'money', 'unemployed', 'lost', 'job', 'bill', 'wage', 'work', 'payday', 'paycheck', 'funds', 'cash', 'bank', 'laid off', 'poor', 'payroll'],\n",
      "            'sad': ['worst', 'awful', 'sick', 'problem', 'catch a break', 'cheer', 'hospital', 'bad', 'shitty', 'stress', 'luck', ':(', 'rough', 'tough'],\n",
      "            'military': ['military', 'veteran', 'soldier', 'army', 'navy', 'marine', 'air force', 'iraq', 'afghanis'],\n",
      "            'happy': ['celebrate', 'birthday', 'party', 'new year', 'bday', 'engage', 'annivers'],\n",
      "            'nice': ['please', 'help', 'thank you', ':)'],\n",
      "            'honest': ['sob story', 'honest', 'just want', 'just because'],\n",
      "            'parent': ['family', 'kids', 'parent', 'mom', 'mommy', 'mother', 'dad', 'father', 'baby', 'boy', 'girl'],\n",
      "            'relationship': ['husband', 'wife', 'girlfriend', 'boyfriend', 'fianc'],\n",
      "            'test': ['study', 'test', 'final', 'midterm'],\n",
      "            'payback': ['pay it forward', 'pay the pizza back', 'pay it back/forward', 'pay it back']\n",
      "        }\n",
      "        #SAMADD\n",
      "    \n",
      "    def find_tag_words(self, keywords, text):\n",
      "        word_dict = {}\n",
      "        tag_dict = {}\n",
      "\n",
      "        for tag, words in keywords.iteritems():\n",
      "\n",
      "            tag_count = None\n",
      "\n",
      "            for word in words:\n",
      "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
      "                word_dict[word] = has_word\n",
      "\n",
      "                if tag_count is None:\n",
      "                    tag_count = has_word\n",
      "                else:\n",
      "                    tag_count = tag_count +  has_word\n",
      "\n",
      "            tag_dict[tag] = tag_count\n",
      "\n",
      "        return (tag_dict, word_dict)\n",
      "    \n",
      "    # manually create keywords with categories\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        do_title = self.do_title\n",
      "        do_tags = self.do_tags\n",
      "        do_words = self.do_words\n",
      "        do_body = self.do_body\n",
      "        keywords = self.keywords\n",
      "        find_tag_words = self.find_tag_words\n",
      "        body_col = self.body_col\n",
      "        title_col = self.title_col\n",
      "        \n",
      "        features = []\n",
      "        feature_names = []\n",
      "\n",
      "        # find keywords and tags\n",
      "        if do_title:\n",
      "            title_unicode = np.array([x.lower() for x in X[:,title_col]])\n",
      "            title_tag_dict, title_word_dict = find_tag_words(keywords, title_unicode)\n",
      "\n",
      "            if do_tags:\n",
      "                features.append(pd.DataFrame(title_tag_dict).values)\n",
      "                feature_names.append('title_tags')\n",
      "            if do_words:\n",
      "                features.append(pd.DataFrame(title_word_dict).values)\n",
      "                feature_names.append('title_words')\n",
      "\n",
      "        if do_body:\n",
      "            body_unicode = np.array([x.lower() for x in X[:,body_col]])\n",
      "            body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
      "\n",
      "            if do_tags:\n",
      "                features.append(pd.DataFrame(body_tag_dict).values)\n",
      "                feature_names.append('body_tags')\n",
      "\n",
      "            if do_words:\n",
      "                features.append(pd.DataFrame(body_word_dict).values)\n",
      "                feature_names.append('body_words')\n",
      "\n",
      "        return np.hstack(tuple(features))\n",
      "    \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {'do_words': self.do_words, 'do_tags':self.do_tags, 'do_body':self.do_body, 'do_title':self.do_title}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class CheckWordsTransformer(TransformerMixin):\n",
      "    def __init__(self, words=[]):\n",
      "        self.words = words\n",
      "        \n",
      "    def find_words(self, words, text, lower=True):\n",
      "        word_dict = {}\n",
      "\n",
      "        for word in words:\n",
      "            if lower:\n",
      "                has_word = np.array([(1 if word in t.lower() else 0) for t in text])\n",
      "            else:\n",
      "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
      "            word_dict[word] = has_word\n",
      "\n",
      "        return word_dict\n",
      "    \n",
      "    # manually create keywords with categories\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        words = self.words\n",
      "        find_words = self.find_words\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        if len(X.shape) > 1:\n",
      "            cols = X.shape[1]\n",
      "            for i in cols:\n",
      "                features.append(pd.DataFrame(find_words(words, X[:,i])).values)\n",
      "        else:\n",
      "            features.append(pd.DataFrame(find_words(words, X)).values)\n",
      "            \n",
      "        return np.hstack(tuple(features))\n",
      "    \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {'words': self.words}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc = LinearSVC(class_weight='auto')\n",
      "etc = ExtraTreesClassifier(class_weight='auto', n_estimators=200, max_depth=3, min_samples_split=15)\n",
      "\n",
      "pipe_body = Pipeline([('body',ExtractBody()), ('words',CheckWordsTransformer(words=['kid', 'sob story','?','generous', 'reddit', 'random acts of pizza','bumm','you','fulfill','forward','trade','http','thank','raop','edit','rough','crav','apprec',':-)',':)',':D','verif','comfort']))])\n",
      "pipe_title = Pipeline([('title',ExtractTitle()), ('words',CheckWordsTransformer(words=['kid', 'sob story','?','generous', 'reddit', 'random acts of pizza','bumm','high','you''fulfill','forward','trade','thank','raop','rough','crav','apprec',':-)',':)',':D','verif','comfort']))])\n",
      "union = FeatureUnion([('body', pipe_body), ('title', pipe_title)])\n",
      "\n",
      "pipe_etc = Pipeline([('features',union), ('model',etc)])\n",
      "pipe_lsvc = Pipeline([('features',union), ('model',lsvc)])\n",
      "pipe_etc_l1 = Pipeline([('features',union), ('l1', LinearWeightFeatureThreshold(C=0.5)), ('model',etc)])\n",
      "pipe_lsvc_l1 = Pipeline([('features',union), ('l1', LinearWeightFeatureThreshold(C=0.5)), ('model',lsvc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_scores(cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "print_scores(cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.558648, Median: 0.564289, SD: 0.024292\n",
        "N: 5, Mean: 0.558004, Median: 0.553721, SD: 0.013048"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_scores(cross_val_score(pipe_etc_l1, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "print_scores(cross_val_score(pipe_lsvc_l1, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 32/44 features\n",
        "kept 34/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 34/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 35/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.554519, Median: 0.560772, SD: 0.029890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 32/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 34/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 34/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 35/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.557645, Median: 0.550030, SD: 0.013845"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.523190, Median: 0.525438, SD: 0.015155\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train a model based on keywords and tags\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_oob = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           oob_score = True,\n",
      "                           bootstrap=True,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "\n",
      "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_pca = Pipeline([('scale', StandardScaler()),('pca', RandomizedPCA(n_components=3)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_l1 = Pipeline([('scale', StandardScaler()),('l1', LinearWeightFeatureThreshold(C=0.05)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "\n",
      "\n",
      "models = {'Extra Trees':etc, 'ET Out Of Bag':etc_oob, 'Gradient Boosting':gbc, 'Linear SVC':lsvc, 'Linear SVC PCA 3':lsvc_pca}\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Tags'\n",
      "trans = InterestingWordsTransformer(do_words=False)\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "    \n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n",
      "\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Words'\n",
      "trans = InterestingWordsTransformer(do_tags=False)\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "\n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n",
      "\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Words & Tags'\n",
      "trans = InterestingWordsTransformer()\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "\n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "##############\n",
        "Body & Title Tags\n",
        "\n",
        "Gradient Boosting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.545817, Median: 0.549386, SD: 0.020188"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.541472, Median: 0.539082, SD: 0.013534"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC PCA 3\n",
        "N: 5, Mean: 0.553995, Median: 0.545856, SD: 0.017529"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.553015, Median: 0.541681, SD: 0.019736"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 5, Mean: 0.558610, Median: 0.562401, SD: 0.025546"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.558412, Median: 0.558060, SD: 0.026881"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "ET Out Of Bag\n",
        "N: 5, Mean: 0.546011, Median: 0.546757, SD: 0.024192"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.549779, Median: 0.551015, SD: 0.023471"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 5, Mean: 0.555373, Median: 0.551999, SD: 0.021820"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.557573, Median: 0.556174, SD: 0.021681"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "##############\n",
        "Body & Title Words\n",
        "\n",
        "Gradient Boosting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.518795, Median: 0.511314, SD: 0.019704"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.514420, Median: 0.517272, SD: 0.014839"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC PCA 3\n",
        "N: 5, Mean: 0.558086, Median: 0.553755, SD: 0.011904"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.564616, Median: 0.566086, SD: 0.011672"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 5, Mean: 0.543072, Median: 0.541400, SD: 0.013500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.541276, Median: 0.540994, SD: 0.014420"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "ET Out Of Bag\n",
        "N: 5, Mean: 0.541840, Median: 0.546474, SD: 0.019696"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.543336, Median: 0.543974, SD: 0.013890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 5, Mean: 0.536566, Median: 0.537468, SD: 0.014383"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.533987, Median: 0.529519, SD: 0.011162"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "##############\n",
        "Body & Title Words & Tags\n",
        "\n",
        "Gradient Boosting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.527478, Median: 0.530245, SD: 0.019165"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.522444, Median: 0.523009, SD: 0.012306"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC PCA 3\n",
        "N: 5, Mean: 0.550870, Median: 0.541432, SD: 0.019852"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.553183, Median: 0.552173, SD: 0.013461"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 5, Mean: 0.536388, Median: 0.532212, SD: 0.015831"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.534997, Median: 0.536474, SD: 0.018380"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "ET Out Of Bag\n",
        "N: 5, Mean: 0.544133, Median: 0.540641, SD: 0.017981"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.539445, Median: 0.535000, SD: 0.013983"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 5, Mean: 0.545253, Median: 0.551154, SD: 0.018637"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.542608, Median: 0.547821, SD: 0.018111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 2,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_oob = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 2,\n",
      "                           min_samples_split=10,\n",
      "                           oob_score = True,\n",
      "                           bootstrap=True,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "\n",
      "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_pca = Pipeline([('scale', StandardScaler()),('pca', RandomizedPCA(n_components=3)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_l1 = Pipeline([('scale', StandardScaler()),('l1', LinearWeightFeatureThreshold(C=0.05)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "\n",
      "megapipe = Pipeline([\n",
      "    ('subsets', FeatureUnion([\n",
      "        ('title_tags', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_body=False,do_words=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca),\n",
      "            ]))\n",
      "        ])),\n",
      "        ('body_tags', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_title=False,do_words=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca)\n",
      "            ]))\n",
      "        ])),\n",
      "        ('title_words', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_body=False,do_tags=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca),\n",
      "                ('lsvc_l1', lsvc_l1)\n",
      "            ]))\n",
      "        ])),\n",
      "        ('title_tags', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_title=False,do_tags=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca),\n",
      "                ('lsvc_l1', lsvc_l1)\n",
      "            ]))\n",
      "        ])),\n",
      "    ])),\n",
      "    ('ensemble', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "scorer = make_scorer(roc_auc_score)\n",
      "scores = cross_val_score(megapipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer)\n",
      "print_scores(scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 43/89 features\n",
        "kept 61/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 53/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 53/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 54/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 55/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 53/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 55/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 55/89 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.533167, Median: 0.549947, SD: 0.024570"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('tsum', TextSummaryTransformer()),\n",
      "        ('tags', InterestingWordsTransformer(do_words=False))\n",
      "        ])\n",
      "    ),\n",
      "    ('etc', etc)\n",
      "])\n",
      "\n",
      "\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Combine Feature Sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=True, tokenizer=SnowballStemTokenizer())\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "kf = KFold(n_all, n_folds = 5)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.1))\n",
      "        ])),\n",
      "        ('title_l1', Pipeline([\n",
      "            ('title',ExtractTitle()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.1))\n",
      "        ])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    ('model', etc)\n",
      "])\n",
      "\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer)\n",
      "\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 21/9175 features\n",
        "kept 12/3671 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 15/3670 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/3655 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 7/3656 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 12/3648 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.593664, Median: 0.589323, SD: 0.021410"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 10, Mean: 0.604379, Median: 0.600186, SD: 0.022009\n"
       ]
      }
     ],
     "prompt_number": 373
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    ('scaler', StandardScaler()),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=0.025)),\n",
      "    ('model', etc)\n",
      "])\n",
      "\n",
      "#scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer, verbose=1)\n",
      "\n",
      "#print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 410
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### somewhat shallow trees with heavy leaves and lots of estimators works for tree ensembles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.1))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    ('scaler', StandardScaler()),\n",
      "    #('l1', LinearWeightFeatureThreshold(C=0.025)),\n",
      "    ('model', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(pipe, {'model__max_depth':[2,5,10], 'model__min_samples_split':[5,15,30], 'model__n_estimators':[10,100,1000]}, cv=kf, scoring=scorer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out = gs.fit(all_train_df.values, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 21/9175 features\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 17/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-95-28d2b637bb72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgs_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \"\"\"\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 for train, test in cv)\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    447\u001b[0m             delayed(_fit_transform_one)(trans, name, X, y,\n\u001b[1;32m    448\u001b[0m                                         self.transformer_weights, **fit_params)\n\u001b[0;32m--> 449\u001b[0;31m             for name, trans in self.transformer_list)\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, name, X, y, transformer_weights, **fit_params)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX_transformed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtransformer_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-51-f942562d5bfe>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, **transform_params)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_title\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtitle_unicode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mtitle_tag_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_word_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_tag_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_unicode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-51-f942562d5bfe>\u001b[0m in \u001b[0;36mfind_tag_words\u001b[0;34m(self, keywords, text)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mhas_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'gs_out' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-96-68e0066a25fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgs_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'gs_out' is not defined"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### With gradient boosting, it looks likes slow learning and lots of estimators is the way to go"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.1))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    #('scaler', StandardScaler()),\n",
      "    #('l1', LinearWeightFeatureThreshold(C=0.025)),\n",
      "    ('model', gbc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kf_over = oversample_kfold(kf, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(pipe, {'model__learning_rate':[.01,.1,1,5], 'model__n_estimators':[20,100,200]}, cv=kf_over, scoring=scorer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out = gs.fit(all_train_df.values, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 69/9175 features\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/9175 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9323 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9350 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9351 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9216 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 26/10502 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "[mean: 0.58290, std: 0.01252, params: {'model__learning_rate': 0.01, 'model__n_estimators': 20},\n",
        " mean: 0.61019, std: 0.00702, params: {'model__learning_rate': 0.01, 'model__n_estimators': 100},\n",
        " mean: 0.60860, std: 0.00743, params: {'model__learning_rate': 0.01, 'model__n_estimators': 200},\n",
        " mean: 0.60113, std: 0.00638, params: {'model__learning_rate': 0.1, 'model__n_estimators': 20},\n",
        " mean: 0.60531, std: 0.00531, params: {'model__learning_rate': 0.1, 'model__n_estimators': 100},\n",
        " mean: 0.59304, std: 0.00721, params: {'model__learning_rate': 0.1, 'model__n_estimators': 200},\n",
        " mean: 0.57097, std: 0.00846, params: {'model__learning_rate': 1, 'model__n_estimators': 20},\n",
        " mean: 0.56265, std: 0.01604, params: {'model__learning_rate': 1, 'model__n_estimators': 100},\n",
        " mean: 0.55685, std: 0.01797, params: {'model__learning_rate': 1, 'model__n_estimators': 200},\n",
        " mean: 0.50647, std: 0.03845, params: {'model__learning_rate': 5, 'model__n_estimators': 20},\n",
        " mean: 0.51855, std: 0.04582, params: {'model__learning_rate': 5, 'model__n_estimators': 100},\n",
        " mean: 0.51855, std: 0.04582, params: {'model__learning_rate': 5, 'model__n_estimators': 200}]"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### what combinations of features work best?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=True, tokenizer=LemmaTokenizer())\n",
      "gbc = GradientBoostingClassifier(n_estimators = 200,\n",
      "                                 learning_rate=0.01,\n",
      "                                 max_depth = 4,\n",
      "                                 min_samples_split=10)\n",
      "etc = ExtraTreesClassifier(n_estimators = 100,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "preprocess = FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.1))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('edit', Pipeline([('body',ExtractBody()), ('words',CheckWordsTransformer(words=['edit']))])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_body=False, do_words=False)),\n",
      "        ('body_case', Pipeline([('text', ExtractBody()), ('case', CaseAnalysisTransformer())]))\n",
      "    ])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', preprocess),\n",
      "    #('scaler', StandardScaler()),\n",
      "    #('l1', LinearWeightFeatureThreshold(C=1)),\n",
      "    ('model', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=KFold(n_all, 10), scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 19/11889 features\n",
        "kept 19/11777 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 20/11911 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/11928 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 18/11906 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 16/11974 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 14/11947 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 16/11887 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/11858 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 19/11844 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    5.7s\n",
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   56.5s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.606062, Median: 0.612244, SD: 0.021914"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Start Sam's Error Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = name2index(all_train_df, 'request_title')\n",
      "BODY_COLUMN = name2index(all_train_df, 'request_text_edit_aware')\n",
      "\n",
      "# Number of errors to examine per fold. Keep the number negative to find the biggest error cases.\n",
      "# Keep in mind that this will print 10x this many error cases.\n",
      "num_errors_per_fold = -3\n",
      "\n",
      "echeck_kf = KFold(n_all, 10)\n",
      "\n",
      "for train_index, test_index in echeck_kf:\n",
      "    X_train, X_test = all_train_df.values[train_index], all_train_df.values[test_index]\n",
      "    y_train, y_test = all_train_labels[train_index], all_train_labels[test_index]\n",
      "    \n",
      "    pipe.fit(X_train, y_train)\n",
      "    cl_probs = pipe.predict_proba(X_test)\n",
      "\n",
      "    # Loop through this fold of test data and determine the R ratio for each one:\n",
      "    ratios = []\n",
      "    for i in range(0, cl_probs.shape[0]):\n",
      "        ratios.append(cl_probs[i].max() / cl_probs[i][y_test[i]])\n",
      "\n",
      "    # Find the 3 largest ratios and print them as error cases to examine:\n",
      "    ratios = np.asarray(ratios)\n",
      "    heavy_ratios = ratios.argsort()[num_errors_per_fold:][::-1]\n",
      "    for r in heavy_ratios:\n",
      "        print \"== We guessed %s for this, but it was actually %s. ==\" % (np.argmax(cl_probs[r]),\n",
      "                                                                         y_test[r])\n",
      "        print \"\\n\", X_test[r, TITLE_COLUMN]\n",
      "        print \"\\n\", X_test[r, BODY_COLUMN]\n",
      "        print \"\\n==========\\n\"\n",
      "\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 19/11889 features\n",
        "== We guessed 0 for this, but it was actually True. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[Request] [Manteca, CA] 17 days sober and kinda having a rough time. Would greatly appreciate a random act of pizza.\n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[REQUEST] Will you be my angel tonight? [seattle]\n",
        "\n",
        "Dear /r/Random_Acts_Of_Pizza\n",
        "\n",
        "I come before you now as a humble man. I would like to think I am an honest man so I shall tell you an honest tale. \n",
        "\n",
        "My name is Dave and I currently am working at a Homeless shelter for homeless youth in Seattle called ROOTS. I love working at ROOTS, I can sleep easily in the mornings (since I work throughout the nights) knowing that I am helping keeping 30 homeless youth off the streets and away from SNOWzilla's wrath as he stomps through Seattle. \n",
        "\n",
        "SNOWzilla has ruined things here in Seattle. Because of the nature of the place I work at, someone MUST be on staff every night, and I have to work throughout the weekend and since the buses are closed? I must walk 5 miles in ice and snow to get to work. \n",
        "\n",
        "Sadly though, I don't get paid very much. [(Bank Statement)](http://imgur.com/DvEeh) But that is the way of the world working for a homeless shelter. I haven't eaten in the last day and a half because of there being nothing in my kitchen [My fridge](http://imgur.com/05ffR) and stores being closed.\n",
        "\n",
        "So I am asking you all, if you are willing, would be able to show a snowed-in man with no money some kindness? Even under these difficult times, and even if whether or not my ask for some kindness has no results, know that I have fun even when times are grim [I did it all for you, Reddit!](http://imgur.com/bhnxu)\n",
        "\n",
        "I hope you can all find it in your hearts to give some kindness my way. I hope you all have a SAFE time as snowzilla causes havoc in everyone's regions and I live you with one last picture.\n",
        "\n",
        "[Will you be my angel tonight??](http://imgur.com/vBBNm)\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request] Girlfriend and I have been struggling recently since I've lost my job and would love to have some good food. Also she doesn't really believe in the generosity of internet strangers but I DO and would like to prove her wrong.\n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "kept 19/11777 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 0 for this, but it was actually True. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[Request] After 17 days in NICU, the baby is finally home! Dad has to work and I don't feel like cooking. Help? (edited just because)\n",
        "\n",
        "&lt;a href=\"http://imgur.com/CPw0Q\" title=\"Hosted by imgur.com\"&gt;http://imgur.com/CPw0Q.jpg&lt;/a&gt;\n",
        "\n",
        " pic of what a hungry baby may look like\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[Request] Slow Canada Post leaving me broke and hungry\n",
        "\n",
        "hello fellow redditors,\n",
        "\n",
        "I am a contract programmer, and have been waiting over a week and a half for Canada Post to deliver at least one of my recent pay cheques.\n",
        "\n",
        "So far I've managed to get by on a bit of groceries I had in my cupboard and from the food bank... but these supplies have unfortunately run dry.\n",
        "\n",
        "Every day, I check the mail like an eager kid waiting for that parcel he ordered from the back of a comic book... but so far, nothing.\n",
        "\n",
        "I tend to be really generous when I do have cash and have bought many meals for hungry folks... and will certainly be putting up an [Offer] post in here when my cheques finally do arrive.\n",
        "\n",
        "If anyone out there sympathizes with the daily sinking heart feeling of waiting for that *special and important piece of mail*... please help a brother out?\n",
        "\n",
        "Thanks guys!\n",
        "\n",
        "**EDIT:** I live in Victoria, BC, Canada ... Postal Code: V8T 4A6 ... there is a [Pizza Zone](http://www.pizzazone.ca/) and a [Palagio Pizza](http://www.palagio.ca/) very close by to me!\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request] 2 college students who won't have time to get lunch because of work could use some help-PHILLY Metro\n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "kept 20/11911 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 1 for this, but it was actually False. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[Request] Papa Johns has buffalo sauce as a topping again! (OH, USA)\n",
        "\n",
        "This was my favorite short term promotion and its back! I don't have the money otherwise I would buy it myself. Thanks for reading my post and have an awesome day!\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[request] Portland OR, USA. Licking my chops for a pizza.\n",
        "\n",
        "Random poem for a random pizza and a random day....\n",
        " \n",
        " \n",
        "\n",
        "* At work so busy all day, No time to eat gotta run can't stay! \n",
        "\n",
        "* On my way home tummy starts to grumble, as my engine starts to putter and rumble.\n",
        "\n",
        "* Oh no I need gas I say to myself. Have to reach for the pizza money up on the top shelf. \n",
        "\n",
        "* Not rich, nor poor. I walked into the house like every man through the front door. \n",
        "\n",
        "* I sit and look at whats on my desk, load up a bowl and take a deep breath. \n",
        "\n",
        "* After a hit I see in the corner a fresh new wrapped game.. Maybe a gift for a foreigner!!!!\n",
        "\n",
        "* I log onto reddit and draw up a post, I decided a contest is what I should host.\n",
        "\n",
        "* RandomActsOfGaming is where it has been layed. For 36 hours is where it will have stayed.\n",
        "\n",
        "* Post completed happy with myself, A pizza is what I want I look to my shelf.\n",
        "\n",
        "* Oh that's right I spent it on gas, and the rest of my money has to last.\n",
        "\n",
        "* Oh well no worries what can you do, should not have got the extra super jumbo sized corn dog thing while high at the zoo. So random acts of pizza I turn to you\n",
        "\n",
        "* I am not down on my luck or feeling blue. Others deserve it more then me this is true. \n",
        "\n",
        "* But if you find it in your heart to warm mine up tonight, I will surely do the same on first paychecks light.\n",
        "\n",
        "*\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[REQUEST] Nashville would like some pizza to go with AP exam review\n",
        "\n",
        "For those who don't know, AP classes are high school classes that allow the opportunity for college credit at the end.\n",
        "\n",
        "Tomorrow I have AP Calculus AB, Friday I have AP US History and AP European History, and next Wednesday I have AP English Language and Composition. \n",
        "\n",
        "I've been studying my ass off and craving some good (or crappy, I'm not really that picky) pizza, and it occurred to me that there's a subreddit just for this! \n",
        "\n",
        "==========\n",
        "\n",
        "kept 21/11928 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 0 for this, but it was actually True. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[REQUEST] Just want to smile. I'm too reserved to spill my sob story. Kindness by pizza giving = Smiles.\n",
        "\n",
        "OP WILL deliver with pics v\u00eda PM. \n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request] Ireland, will do anything for pizza, I haven't eaten in days!\n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request](North Carolina) Hungover and studying all day for some upcoming exams. Some pizza would go a long way\n",
        "\n",
        "Title pretty much says it all. No sob story, would simply love a pizza to help me through today!\n",
        "\n",
        "==========\n",
        "\n",
        "kept 18/11906 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 1 for this, but it was actually False. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[REQ] Just found out about this place, Figured I might as well ask.\n",
        "\n",
        "Well, I do tech support here in Fayetteville, NC. I got sick a couple of weeks ago and had to go to the doc. I don't have insurance yet, so that tapped out my savings, to start. It also got slow at work, which meant I had to leave early several days in a row.\n",
        "\n",
        "I just barely made rent. I had 5 bucks left to go grocery shopping on. I bought a 10lb bag of rice and some flavorings (pizza sauce, soy sauce, salsa that was on sale) and have been eating that for the last week. At this point, I just want something with some flavor. I get paid on the 20th, and would love to buy a pizza or two then for someone.\n",
        "\n",
        "Just figured I would ask. It may work, it may not. Never know.\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[Request] 19 and struggling, been off my feet for a few months.\n",
        "\n",
        "Hey my name is Andrew. Currently i'm unemployed and stream video games via twitch.tv full time to try and make some money here and there. I live in the Denver, Colorado area roughly.\n",
        "\n",
        "I live at home with my mom after a few heavy life incidents led me to leave my job and move back home.\n",
        "\n",
        "Streaming hasn't been making me much lately and i've been pretty broke, had to quit smoking cold turkey and can't exactly afford food at the moment either.\n",
        "\n",
        "The last few days i've been really achy and sick and my mom didn't get child support for whatever  reason so we are scraping by until we get food stamps in a few days.\n",
        "\n",
        "I've been looking for a job and working on getting my GED to help out on that front but no luck so far seeing as how I can't commute downtown every day.\n",
        "\n",
        "If anyone would be cool and get me a pizza or something, that would be pretty legit.\n",
        "\n",
        "Not really sure what else to put here, so sorry if anyone is looking for more info i'd be happy to edit my post for whatever reason.\n",
        "\n",
        "\n",
        "Edit: I saw a contest for some bad MS paint art, i like to draw champions from League of Legends in paint for people who donate to my stream, so i'd be happy to draw one or more for someone who donates a pizza. (Doesn't have to be league related, but i'm terrible.)\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[REQUEST] I just got fired and I don't have any money - Oklahoma City\n",
        "\n",
        "I just got fired from my job and I'm broke. I don't have any money or food in my house, I've been eating bread and rice all week, and on top of that I signed a NC/NDA where I was fired from and I can't even get another job working on the same type of stuff that I know how to work on (fixing cell phones, computers, and mostly reballing PS3 and XBOX 360s) due to the NC/NDA agreement they had me sign. I'm not sure what to do and I'm really hungry, if someone could please send me something like pizza or foods coupon I would really appreciate it. Thanks\n",
        "\n",
        "==========\n",
        "\n",
        "kept 16/11974 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 0 for this, but it was actually True. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[REQUEST] UK. Student wanting a pizza to hold me over until loans \n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request] 26 , male , UK. Get paid in 7 days , have a little food for next week, hoping for a square meal that is not rice cakes this evening.\n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request] Pizza for finals in Northern Iowa\n",
        "\n",
        "My friend and I will be studying for finals for many hours this weekend. We are poor, but not destitute. Receiving a pizza for studying would lift our spirits and give us a reason to study very hard and do well on our finals. Any pizza gift card would be met with great excitement!\n",
        "\n",
        "Thank you for your consideration.\n",
        "\n",
        "==========\n",
        "\n",
        "kept 14/11947 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 0 for this, but it was actually True. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[Request] Nothing in the fridge, no money, no pay until Friday morning. If some good person can help me out I'll pay it forward on Friday times 2!\n",
        "\n",
        "\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[request] Pennsylvnia USA Would love to have pizza night for me and my daughter\n",
        "\n",
        "its just me and my 9 year old daughter tonight.we plan on watching christmas movies.she got real good grades on her report card and would like to suprise her with a pizza.thanks\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[REQUEST] There's a 3700+ acre fire going on in my town. Would any of you be willing to help me send pizza to the firefighters?\n",
        "\n",
        "Hi, information about the fire [here.](http://www.nbcbayarea.com/news/local/Morgan-Fire-Near-Mt-Diablo-Chars-1500-Acres-222950261.html)\n",
        "\n",
        "I'll be stopping by the fire station myself later tonight and will be bringing homemade dishes... I'm thinking probably a few large casseroles or something easy to make in bulk. The thought crossed my mind that this seems like the sort of thing Reddit would love to help with. \n",
        "\n",
        "Pizza donations can be sent to the ConFIRE Station #11 at 6500 Center St. in Clayton, CA. The firefighters have been working all day and night since the fire broke out yesterday, and I'm sure they'd appreciate some hot, cheesy slices of pizza! \n",
        "\n",
        "To help you out, [here](http://www.yelp.com/search?find_desc=pizza&amp;find_loc=6500+Center+St.%2C+clayton%2C+ca&amp;ns=1&amp;ls=883fdc931eaafb15) is the Yelp page for pizza places near the address. \n",
        "\n",
        "Hoping to be able to send at least 4-5 pizzas over there for dinner tonight. Let's make this happen! :)\n",
        "\n",
        "==========\n",
        "\n",
        "kept 16/11887 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 1 for this, but it was actually False. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[Request] I just renounced my Mormon beliefs after 28 years, and I'm hungry for pizza here in Atlanta.\n",
        "\n",
        "*\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[Request] Hungry Family of 9 in NC (Triad area)\n",
        "\n",
        "Apparently there has been a huge delay on food stamps in the state of NC due to some software changes. And we have been delayed for about a month. We just got hit with it.  Large Family 2 adults 4 teens 3 little ones. We are in the process of contacting local churches and food pantries etc. Let me know. Papa Johns is the only things that delivers to our house (out in the boonies).\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[Request] When the moon hits your eye..\n",
        "\n",
        "Like a big pizza pie, that's amore!\n",
        "\n",
        "Mmm Pizza Mia?\n",
        "\n",
        "==========\n",
        "\n",
        "kept 21/11858 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 1 for this, but it was actually False. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[Request] No Tear-Jerking Story, Just Broke, Cold, And Would Love A Pizza!\n",
        "\n",
        "The title says it all - We don't really have a sob story, other than to say that we're poor and neglected to take something out of the freezer to thaw for dinner tonight.  It's cold outside, and we'd be grateful for some cheesy goodness.  Thanks!\n",
        "\n",
        "edit: We can pay it forward Friday. Could probably even come with a contest to do so...\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "[REQUEST] Ithaca, NY - Midterms Coming Up, Would Appreciate Study Pizza!\n",
        "\n",
        "Hey, doods! I'm currently a rising sophomore studying for midterms coming up, hopefully some pizza could relieve some stress..., and some hunger. I would appreciate the help and return the offer someday when I get my paycheck.\n",
        "\n",
        "Thank you!\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 0 for this, but it was actually True. ==\n",
        "\n",
        "(REQUEST) - Had Food Poisoning all day, finally getting appetite back but no food at home.\n",
        "\n",
        "Woke up 6 this morning puking. Haven't had any food all day, but now my appetite is coming back but I have no food. Anything would help and be greatly appreciated. \n",
        "\n",
        "==========\n",
        "\n",
        "kept 19/11844 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "== We guessed 0 for this, but it was actually True. =="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[REQUEST]  flat broke and no food\n",
        "\n",
        "no money no food and hungry  a pizza would e nice =[\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[REQUEST] Due to poor decision making I had to move back in with my parents at 24. I want a damn pizza.\n",
        "\n",
        "I quit my job to go abroad for 30 days.  Not the best decision of my life, but I would never get the opportunity to take a trip like I did again.  While I was gone, due to circumstances outside my control, I lost my apartment.  So, I come back after 30 days to no job and no place to live.  My roommate was kind enough to start packing for me after he had packed his own things.\n",
        "\n",
        "Now I'm here, living with my parents who can't afford to take me in.  I've just got a job (I made seven dollars yesterday! Yay!) but, can't really afford to spend anything on anything extra.  I'd really just like a pizza.\n",
        "\n",
        "I live in Alaska.  If anyone is willing to hook me up let me know.\n",
        "\n",
        "==========\n",
        "\n",
        "== We guessed 1 for this, but it was actually False. ==\n",
        "\n",
        "[Request] Air Force finance fucked my paycheck, I crashed my car, my phone and computer are broken, and I'm flat broke. Help an airman out? [Wright-Patterson AFB, OH]\n",
        "\n",
        "I'm stationed at WPAFB. My pay got all jacked up and they took everything back at once. That's a lot of money. Anyone in the air force understands how fucked finance can be. I crashed my car last week, my computer screen broke, my phone isn't working. I'm currently typing this in the tiny area of my screen that still works. I hate asking for charity since I've been poor all my life. But I've had such a bad run the past few weeks and I'm burnt out and flat broke. And since it's my food allowance, (BAS) that is fucked up, I can't eat at the dining facility on base using my military ID. I'd really appreciate some help, guys. If not, that's cool. I have oatmeal. I'm just so tired of oatmeal..\n",
        "\n",
        "edit: When I get back on my feet, I intend to pay it forward. I believe wholeheartedly in that philosophy.\n",
        "\n",
        "==========\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "================"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Find the right C in the l1 feature reduction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=True, tokenizer=SnowballStemTokenizer())\n",
      "etc = ExtraTreesClassifier(n_estimators = 100,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "\n",
      "preprocess = FeatureUnion([\n",
      "        ('bodyl1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('body_words', Pipeline([('body',ExtractBody()), ('words',CheckWordsTransformer(words=['thank']))])),\n",
      "        ('title_words', Pipeline([('body',ExtractTitle()), ('words',CheckWordsTransformer(words=['thank']))])),\n",
      "        ('body_spell', Pipeline([('col', ExtractTitle()), ('incorpus', InCorpusTransformer())])),\n",
      "        ('title_spell', Pipeline([('col', ExtractBody()), ('incorpus', InCorpusTransformer())])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_body=False, do_words=False)),\n",
      "        ('body_case', Pipeline([('text', ExtractBody()), ('case', CaseAnalysisTransformer())])),\n",
      "        ('title_case', Pipeline([('text', ExtractBody()), ('case', CaseAnalysisTransformer())]))\n",
      "    ])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', preprocess),\n",
      "    ('scaler', StandardScaler()),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=1)),\n",
      "    ('model', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 556
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=KFold(n_all, 10), scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 63/9901 features\n",
        "kept 84/97 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9890 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 93/98 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9868 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 89/96 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9965 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 96/101 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9924 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 81/94 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 74/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 98/108 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9907 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 89/97 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 72/9905 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 98/106 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 90/101 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9888 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 98/105 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    9.5s\n",
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.7min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.601502, Median: 0.602955, SD: 0.028001"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 557
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(pipe, {\n",
      "    'preprocess__bodyl1__l1__C':[.0025,.005],\n",
      "    #'preprocess__bodyl1__tv__tokenizer':[None, LemmaTokenizer(), SnowballStemTokenizer()],\n",
      "    'preprocess__bodyl1__tv__ngram_range':[(1,1), (1,2)],\n",
      "    }, cv=KFold(n_all,10), scoring=roc_scorer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out = gs.fit(all_train_df.values, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 63/9901 features\n",
        "kept 64/9890 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9868 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9965 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9924 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 74/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9907 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 72/9905 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9888 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/95369 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/95248 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 23/94799 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 23/95520 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/94837 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 23/95409 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 20/94802 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 20/94852 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/94694 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 22/94418 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 188/9901 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 196/9890 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 180/9868 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 203/9965 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 189/9924 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 180/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 188/9907 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 191/9905 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 178/9895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9888 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 101/95369 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 115/95248 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 103/94799 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 117/95520 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 111/94837 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 126/95409 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 107/94802 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 111/94852 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 118/94694 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 112/94418 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 208/10502 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.array(gs_out.grid_scores_[:6])\n",
      "print '---'\n",
      "print np.array(gs_out.grid_scores_[6:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ {'preprocess__bodyl1__l1__C': 0.0025, 'preprocess__bodyl1__tv__ngram_range': (1, 1)}\n",
        "  0.59576193245957354\n",
        "  array([ 0.61850466,  0.59076525,  0.5859745 ,  0.57758387,  0.60118421,\n",
        "        0.60231082,  0.61336399,  0.63673372,  0.54167366,  0.58952464])]\n",
        " [ {'preprocess__bodyl1__l1__C': 0.0025, 'preprocess__bodyl1__tv__ngram_range': (1, 2)}\n",
        "  0.59266843495683863\n",
        "  array([ 0.597039  ,  0.60112198,  0.56466302,  0.59388528,  0.57960526,\n",
        "        0.6063817 ,  0.6279961 ,  0.62871169,  0.54192552,  0.5853548 ])]\n",
        " [ {'preprocess__bodyl1__l1__C': 0.005, 'preprocess__bodyl1__tv__ngram_range': (1, 1)}\n",
        "  0.60804264267509989\n",
        "  array([ 0.63498694,  0.61982163,  0.58912072,  0.58570076,  0.61756579,\n",
        "        0.61871408,  0.59127648,  0.66792385,  0.56365224,  0.59166395])]\n",
        " [ {'preprocess__bodyl1__l1__C': 0.005, 'preprocess__bodyl1__tv__ngram_range': (1, 2)}\n",
        "  0.60518230911457982\n",
        "  array([ 0.61377931,  0.61284522,  0.60236794,  0.6072105 ,  0.58434211,\n",
        "        0.58758381,  0.63928372,  0.64188218,  0.57233285,  0.59019544])]]\n",
        "---\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}