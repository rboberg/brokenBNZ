{
 "metadata": {
  "name": "",
  "signature": "sha256:f5bfef8775c20670b59aafde55286201ac425515add4d3b15fac3f4aebd65a52"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import pandas as pd\n",
      "import scipy as scipy\n",
      "import datetime as dt\n",
      "import time\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import make_scorer\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "\n",
      "import gensim\n",
      "\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "\n",
      "def balance_samples(y, method='oversample'):\n",
      "    class_counts = np.bincount(y)\n",
      "    \n",
      "    maxi = np.argmax(class_counts)\n",
      "    new_idx = np.argwhere(y==maxi)\n",
      "    \n",
      "    for i in range(len(class_counts)):\n",
      "        if i != maxi:\n",
      "            mult = class_counts[maxi]/class_counts[i]\n",
      "            rem = class_counts[maxi] - class_counts[i]*mult\n",
      "            idxi = np.argwhere(y==i)\n",
      "            np.random.shuffle(idxi)\n",
      "            for j in range(mult):\n",
      "                new_idx = np.vstack((new_idx,idxi))\n",
      "            new_idx = np.vstack((new_idx,idxi[:rem]))\n",
      "        \n",
      "    np.random.shuffle(new_idx)\n",
      "\n",
      "    return np.reshape(new_idx, (new_idx.shape[0],))\n",
      "\n",
      "def oversample_kfold(kf, y):\n",
      "    kf_over = []\n",
      "    for ti, di in kf:\n",
      "        yt = y[ti]\n",
      "        ti_over = ti[balance_samples(yt)]\n",
      "        kf_over.append((ti_over, di))\n",
      "    return kf_over\n",
      "\n",
      "def name2index(df, names):\n",
      "    return_single = False\n",
      "    \n",
      "    if type(names) == type([]):\n",
      "       names = np.array(names)\n",
      "    elif type(names) != type(np.array([])):\n",
      "        names = np.array([names])\n",
      "        return_single = True \n",
      "    \n",
      "    inds = np.where(np.in1d(df.columns, np.array(names)))[0]\n",
      "    \n",
      "    return inds[0] if return_single else inds\n",
      "\n",
      "def print_scores(scores):\n",
      "    print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(scores), np.mean(scores), np.median(scores), np.std(scores))\n",
      "            \n",
      "def test_kfolds(X, y, kf, model, verbose=1, balance=False):\n",
      "    roc_auc_list = []\n",
      "    \n",
      "    for train_i, dev_i in kf:\n",
      "        if balance:\n",
      "            train_i_orig = train_i\n",
      "            y_train = y[train_i_orig]\n",
      "            train_i = train_i_orig[balance_samples(y_train)]\n",
      "        \n",
      "        \n",
      "        X_train = X[train_i]\n",
      "        X_dev = X[dev_i]\n",
      "\n",
      "        model.fit(X_train, y[train_i])\n",
      "\n",
      "        dev_pred = model.predict(X_dev)\n",
      "        \n",
      "        roc_auc_i = roc_auc_score(y[dev_i], dev_pred)\n",
      "        roc_auc_list.append(roc_auc_i)\n",
      "        if verbose > 1:\n",
      "            print('ROC AUC:',roc_auc_i)\n",
      "            \n",
      "    if verbose > 0:\n",
      "        print 'N: %d, Mean: %f, Median: %f, SD: %f' %(len(kf), np.mean(roc_auc_list), np.median(roc_auc_list), np.std(roc_auc_list))\n",
      "        \n",
      "    return roc_auc_list\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#useful for text processing\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class SnowballStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = SnowballStemmer('english')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "class PorterStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class PuncTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.reg = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
      "    def __call__(self, doc):\n",
      "        return self.reg.tokenize(doc)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 584
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data from JSON file as list of dicts\n",
      "all_train_dict_list = load_json_file('../../data/train.json')\n",
      "submit_dict_list =  load_json_file('../../data/test.json')\n",
      "\n",
      "n_all = len(all_train_dict_list)\n",
      "n_submit = len(submit_dict_list)\n",
      "\n",
      "# shuffle data to avoid biased split of train / dev data\n",
      "rand.shuffle(all_train_dict_list)\n",
      "\n",
      "# set up kFolds\n",
      "kf = KFold(n_all, n_folds = 5)\n",
      "\n",
      "# process labels\n",
      "all_train_labels = np.array([x['requester_received_pizza'] for x in all_train_dict_list])\n",
      "\n",
      "# pandas is useful for turning dicts in to matrix-like objects\n",
      "# where each column is an numpy array\n",
      "submit_df = pd.DataFrame(submit_dict_list)\n",
      "all_train_df = pd.DataFrame(all_train_dict_list)\n",
      "\n",
      "# limit train to columns available in submit_df\n",
      "submit_cols = submit_df.columns\n",
      "all_train_df = all_train_df[submit_cols]\n",
      "\n",
      "# useful for sklearn scoring\n",
      "roc_scorer = make_scorer(roc_auc_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = all_train_labels\n",
      "\n",
      "kf_over = []\n",
      "for ti, di in kf:\n",
      "    yt = y[ti]\n",
      "    ti_over = ti[balance_samples(yt)]\n",
      "    kf_over.append((ti_over, di))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Set up numeric Activity Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ACTIVITY_VARS = ['requester_account_age_in_days_at_request',\n",
      "                'requester_days_since_first_post_on_raop_at_request',\n",
      "                'requester_number_of_comments_at_request',\n",
      "                'requester_number_of_comments_in_raop_at_request',\n",
      "                'requester_number_of_posts_at_request',\n",
      "                'requester_number_of_posts_on_raop_at_request',\n",
      "                'requester_number_of_subreddits_at_request',\n",
      "                'requester_upvotes_minus_downvotes_at_request',\n",
      "                'requester_upvotes_plus_downvotes_at_request'\n",
      "                ]\n",
      "ACTIVITY_COLUMNS = name2index(all_train_df, ACTIVITY_VARS)\n",
      "\n",
      "class ExtractColumnsTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, cols=[0]):\n",
      "        self.cols = cols\n",
      "        \n",
      "    def fit(self, *args, **kwargs):\n",
      "        return self\n",
      "        \n",
      "    def transform(self, X):\n",
      "        cols = self.cols\n",
      "        return X[:,cols]\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "class ExtractActivities(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, ACTIVITY_COLUMNS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Importance of Weighting Classes Appropriately"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example classification\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print 'Equal Class Weights'\n",
      "pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC())])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nReweighted Classes'\n",
      "wt_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC(class_weight='auto'))])\n",
      "print_scores(cross_val_score(wt_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nRebalanced Sample'\n",
      "rebal_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc',SVC())])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(rebal_pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Equal Class Weights\n",
        "N: 10, Mean: 0.509970, Median: 0.506022, SD: 0.007418"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Reweighted Classes\n",
        "N: 10, Mean: 0.550708, Median: 0.548081, SD: 0.028351"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Rebalanced Sample\n",
        "N: 10, Mean: 0.546803, Median: 0.546348, SD: 0.026495"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Simple Text Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = name2index(all_train_df, 'request_title')\n",
      "BODY_COLUMN = name2index(all_train_df, 'request_text_edit_aware')\n",
      "\n",
      "class ExtractBody(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, BODY_COLUMN)\n",
      "        \n",
      "\n",
      "class ExtractTitle(ExtractColumnsTransformer):\n",
      "    def __init__(self):\n",
      "        ExtractColumnsTransformer.__init__(self, TITLE_COLUMN)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def simple_text(do_all=True, do_count=False,do_tfidf=False, do_titles=False, do_bodies=False, do_both=False, lowercase=False, tokenizer=None, stop_words=None):\n",
      "\n",
      "    # Notes\n",
      "    # results slightly better w/ lowercase = False (when unigrams only)\n",
      "    # bigrams added no value on unigrams\n",
      "\n",
      "    tv = TfidfVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    cv = CountVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
      "    lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "    \n",
      "    body_cv = Pipeline([('body',ExtractBody()),('cv', cv)])\n",
      "    body_tv = Pipeline([('body',ExtractBody()),('tv', tv)])\n",
      "    \n",
      "    title_cv = Pipeline([('title',ExtractTitle()),('cv', cv)])\n",
      "    title_tv = Pipeline([('title',ExtractTitle()),('tv', tv)])\n",
      "\n",
      "    if do_titles or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Titles\n",
      "            print '\\nCount Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer TItles\n",
      "            print '\\nTFIDF Vectorizer on Titles'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',title_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "    if do_bodies or do_all:\n",
      "        if do_count or do_all:\n",
      "            # Count Vectorizer Bodies\n",
      "            print '\\nCount Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_cv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Bodies\n",
      "            print '\\nTFIDF Vectorizer on Bodies'\n",
      "            \n",
      "            pipe = Pipeline([('tranform',body_tv),('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "        \n",
      "    if do_both or do_all:\n",
      "        if do_count or do_all:\n",
      "\n",
      "            # Count Vectorizer Titles and Bodies\n",
      "            print '\\nCount Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_cv),\n",
      "                    ('tranform_body',body_cv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "            \n",
      "        if do_tfidf or do_all:\n",
      "            # TFIDF Vectorizer Titles and Bodies\n",
      "            print '\\nTFIDF Vectorizer on Titles and Bodies'\n",
      "            \n",
      "            pipe = Pipeline([\n",
      "                ('features',FeatureUnion([\n",
      "                    ('tranform_title',title_tv),\n",
      "                    ('tranform_body',body_tv)\n",
      "                ])),\n",
      "                ('model',lsvc)])\n",
      "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_text(do_all = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Count Vectorizer on Titles\n",
        "N: 10, Mean: 0.517904, Median: 0.530613, SD: 0.026283"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles\n",
        "N: 10, Mean: 0.520280, Median: 0.533602, SD: 0.034452"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Bodies\n",
        "N: 10, Mean: 0.539667, Median: 0.541570, SD: 0.022771"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Bodies\n",
        "N: 10, Mean: 0.552067, Median: 0.550295, SD: 0.018721"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Count Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.540530, Median: 0.539181, SD: 0.024330"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.545234, Median: 0.548648, SD: 0.028926"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 225
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lowercase seems to be a bit worse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '\\nNot Lower Case'\n",
      "simple_text(do_all=False, do_both=True, lowercase=False, do_tfidf=True)\n",
      "print '\\nLower Case'\n",
      "simple_text(do_all=False, do_both=True, lowercase=True, do_tfidf=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Not Lower Case\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.545234, Median: 0.548648, SD: 0.028926"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Lower Case\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 10, Mean: 0.544905, Median: 0.539174, SD: 0.025612"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try more complex tokenizers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=None)\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=SnowballStemTokenizer())\n",
      "simpble_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.550088, Median: 0.547935, SD: 0.013379"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.548292, Median: 0.559778, SD: 0.020452"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.550497, Median: 0.552507, SD: 0.008981"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "simple_text() got an unexpected keyword argument 'stop_words'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-36-44da5debf756>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msimple_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_tfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSnowballStemTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msimple_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_tfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLemmaTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msimple_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_tfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLemmaTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: simple_text() got an unexpected keyword argument 'stop_words'"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=None, stop_words='english', lowercase=True)\n",
      "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer(), stop_words='english', lowercase=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.531939, Median: 0.537776, SD: 0.016612"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "TFIDF Vectorizer on Titles and Bodies\n",
        "N: 5, Mean: 0.548210, Median: 0.551549, SD: 0.014315"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Text Feature Selection and Dimensionality Reduciton"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LinearWeightFeatureThreshold(TransformerMixin):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model = LinearSVC(class_weight='auto', loss='hinge'),\n",
      "        return_dense = True,\n",
      "        C = 0.0025,\n",
      "        threshold = 0.01,\n",
      "        verbose = 1\n",
      "        ):\n",
      "        self.model = model\n",
      "        self.return_dense = return_dense\n",
      "        self.C = C\n",
      "        self.threshold = threshold\n",
      "        self.verbose = verbose\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        model = self.model\n",
      "        threshold = self.threshold\n",
      "        verbose = self.verbose\n",
      "        C = self.C\n",
      "        \n",
      "        model.set_params(C=C)\n",
      "        \n",
      "        model.fit(X, y)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > threshold)[0]\n",
      "        if verbose > 0:\n",
      "            print 'kept %d/%d features' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "        self.sig_coef_  = sig_coef\n",
      "        return self\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        sig_coef = self.sig_coef_\n",
      "        return_dense = self.return_dense\n",
      "        \n",
      "        X_new = X[:,sig_coef]\n",
      "        \n",
      "        if return_dense and (type(X_new) != type(np.array(1))):\n",
      "            X_new = X_new.toarray()\n",
      "            \n",
      "        return X_new\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {'C':self.C, 'threshold':self.threshold}\n",
      "    \n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            setattr(self, parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=False, tokenizer=SnowballStemTokenizer())\n",
      "l1 = LinearWeightFeatureThreshold()\n",
      "lsvc = LinearSVC(class_weight='auto')\n",
      "\n",
      "pipe_lsvc = Pipeline([('tv',tv), ('features',l1), ('lsvc',lsvc)])\n",
      "\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv), ('features',l1)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv), ('features',l1)])\n",
      "\n",
      "print '\\nL1 Feature Reduction on Titles w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([('title', title_pipe), ('lsvc', lsvc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([('body', body_pipe), ('lsvc', lsvc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "print '\\nL1 Feature Reduction on Both w/ LinearSVC'\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Union w/ LinearSVC'\n",
      "\n",
      "l1 = LinearWeightFeatureThreshold(C=0.0025)\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv)])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=.0025)),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nL1 Feature Reduction on Both Individually and on Union w/ LinearSVC'\n",
      "\n",
      "l1 = LinearWeightFeatureThreshold(C=0.0025)\n",
      "title_pipe = Pipeline([('extract', ExtractTitle()),('tv',tv), ('l1',l1)])\n",
      "body_pipe = Pipeline([('extract', ExtractBody()),('tv',tv), ('l1',l1)])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('features',FeatureUnion([\n",
      "        ('title', title_pipe),\n",
      "        ('body', body_pipe)\n",
      "    ])),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=.002)),\n",
      "    ('model',lsvc)\n",
      "])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "L1 Feature Reduction on Titles w/ LinearSVC\n",
        "kept 29/3880 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/3894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 29/3882 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 35/3904 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 29/3912 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 27/3879 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/3877 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/3920 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/3877 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/3895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.531144, Median: 0.531727, SD: 0.025873"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Bodies w/ LinearSVC\n",
        "kept 64/9808 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 70/9849 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9978 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 68/9850 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/9816 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9874 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/9991 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 61/9941 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9885 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.586179, Median: 0.589549, SD: 0.013592"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Both w/ LinearSVC\n",
        "kept 29/3880 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9808 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/3894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 70/9849 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 29/3882 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9978 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 35/3904 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 29/3912 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 68/9850 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 27/3879 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/9816 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/3877 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9874 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/3920 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/9991 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/3877 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 61/9941 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/3895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9885 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.588234, Median: 0.578475, SD: 0.028515"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Union w/ LinearSVC\n",
        "kept 93/13688 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 101/13743 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 89/13860 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 99/13804 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 97/13762 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 92/13695 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 101/13751 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 90/13911 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 92/13818 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 93/13780 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.588234, Median: 0.578475, SD: 0.028515"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "L1 Feature Reduction on Both Individually and on Union w/ LinearSVC\n",
        "kept 29/3880 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9808 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/93 features\n",
        "kept 31/3894 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 70/9849 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 61/101 features\n",
        "kept 29/3882 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9978 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 61/89 features\n",
        "kept 35/3904 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9900 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 66/99 features\n",
        "kept 29/3912 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 68/9850 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 58/97 features\n",
        "kept 27/3879 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/9816 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 61/92 features\n",
        "kept 30/3877 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9874 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/101 features\n",
        "kept 25/3920 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/9991 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/90 features\n",
        "kept 31/3877 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 61/9941 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 53/92 features\n",
        "kept 30/3895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9885 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 57/93 features\n",
        "N: 10, Mean: 0.582351, Median: 0.573796, SD: 0.032144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 284
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l1 = LinearWeightFeatureThreshold()\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "pipe_etc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',etc)])\n",
      "pipe_gbc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',gbc)])\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ ETC'\n",
      "print_scores(cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "\n",
      "\n",
      "print '\\nL1 Feature Reduction on Bodies w/ GBC'\n",
      "#need to oversample GBC b/c no class_weight\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "print_scores(cross_val_score(pipe_gbc, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "L1 Feature Reduction on Bodies w/ ETC\n",
        "kept 64/9808 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 70/9849 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9978 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-347-3cb6ba5f78c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\nL1 Feature Reduction on Bodies w/ ETC'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe_etc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_train_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroc_scorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m                                               fit_params)\n\u001b[1;32m-> 1361\u001b[1;33m                       for train, test in cv)\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1457\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1459\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \"\"\"\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 804\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 236\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-3-35a6c3e81642>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstmr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstmr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPorterStemTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Ross\\Anaconda\\lib\\site-packages\\nltk\\stem\\snowball.pyc\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# STEP 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__step2_suffixes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tional\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 347
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Latent Symantic Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import text\n",
      "from nltk.tokenize.punkt import PunktWordTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer()\n",
      "docs = ExtractBody().transform(all_train_df.values)\n",
      "stop_words = text.ENGLISH_STOP_WORDS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "? word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PunktWordTokenizer(docs[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "<nltk.tokenize.punkt.PunktWordTokenizer at 0x1b4e9898>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mytokenizer(text, lowercase=True, stops=True):\n",
      "    if lowercase:\n",
      "        text = text.lower()\n",
      "    tokenizer = RegexpTokenizer(r'\\w+')\n",
      "    tokens = tokenizer.tokenize(text)\n",
      "    if stops:\n",
      "        filtered = [w for w in tokens if not w in stop_words]\n",
      "    else:\n",
      "        filtered = tokens\n",
      "    return filtered\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mytokenizer(docs[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "[u'today',\n",
        " u'soo',\n",
        " u'll',\n",
        " u'stuck',\n",
        " u'house',\n",
        " u'day',\n",
        " u'cleaning',\n",
        " u'doing',\n",
        " u'homework',\n",
        " u've',\n",
        " u'week',\n",
        " u'just',\n",
        " u'looking',\n",
        " u'pizza',\n",
        " u'pie',\n",
        " u'lunch',\n",
        " u'edit',\n",
        " u'welp',\n",
        " u'got',\n",
        " u'hungry',\n",
        " u'finished',\n",
        " u'homework',\n",
        " u'ordered',\n",
        " u'jimmy',\n",
        " u'johns',\n",
        " u'instead',\n",
        " u'maybe',\n",
        " u'time']"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Time Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATE_TIME_COLUMN_DEFAULT = np.where(all_train_df.columns == 'unix_timestamp_of_request')[0][0]\n",
      "\n",
      "class TimeTransformer(TransformerMixin):\n",
      "    \n",
      "    def __init__(self, date_time_column=DATE_TIME_COLUMN_DEFAULT, do_hour=True, do_dow=True, do_month=True):\n",
      "        self.date_time_column = date_time_column\n",
      "        self.do_hour = do_hour\n",
      "        self.do_dow = do_dow\n",
      "        self.do_month = do_month\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        return self\n",
      "    \n",
      "    def extract_from_date_time_(self, dt, do_hour, do_dow, do_month):\n",
      "        extract = []\n",
      "        if do_hour:\n",
      "            extract.append(dt.hour)\n",
      "            \n",
      "        if do_dow:\n",
      "            extract.append(dt.weekday())\n",
      "            \n",
      "        if do_month:\n",
      "            extract.append(dt.month)\n",
      "            \n",
      "        return extract\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        date_time_column = self.date_time_column\n",
      "        do_hour = self.do_hour\n",
      "        do_dow = self.do_dow\n",
      "        do_month = self.do_month\n",
      "        extract_from_date_time = self.extract_from_date_time_\n",
      "        \n",
      "        features = np.array([\n",
      "            extract_from_date_time(dt.datetime.fromtimestamp(timei),\n",
      "                                   do_hour=do_hour,\n",
      "                                   do_dow=do_dow,\n",
      "                                   do_month=do_month) for timei in X[:,date_time_column]\n",
      "        ])\n",
      "        \n",
      "        return features\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exploring time features, it looks like requests are not as succesful at late nights / early mornings or on Mondays / Fridays - though that could be because there's more requests on those days."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# look at hourly success\n",
      "hour = TimeTransformer(do_dow=False, do_month=False).transform(all_train_df.values).flatten()\n",
      "hour_pos = hour[all_train_labels]\n",
      "hour_neg = hour[np.logical_not(all_train_labels)]\n",
      "pd.Series(hour_pos).hist(bins=24, alpha=0.2, normed=True)\n",
      "pd.Series(hour_neg).hist(bins=24, alpha=0.2, normed=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 349,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x189bd278>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG8FJREFUeJzt3X9wXfV55/H3g4khipOoHTUmAa+ViT3EdMqP4LIuXQUz\nYWvF24V0mi3rSRvMbArMxEma2e4QNjNr/unskp1sWMoUvIu7Jl2mnoVOG+8Wo9106sY7OBA5/gmS\nbQVkbAMSWuJSLCQk/Owf98rP1UW695wj3XuPdD6vGU84555z71ef3j766rnnfK+5OyIisrhd1OoB\niIhI46nYi4gUgIq9iEgBqNiLiBSAir2ISAGo2IuIFEDdYm9m3WbWb2YnzOzeGR7/tJntM7MxM/vX\nac4VEZHmsFrX2ZvZEuAYcAtwBvgJsMnd+yqO+SVgJfAF4Ofu/t2k54qISHPUm9nfAAy4+6C7TwA7\ngdsqD3D3N9y9F5hIe66IiDRHvWJ/OXCqYvt0eV8SczlXRETmUb1iP5e1FLQOg4hITlxc5/EzwIqK\n7RWUZuhJJDrXzPRLQUQkA3e3pMfWm9n3AqvNrNPMlgK3A7tmObb6RROf6+76587WrVtbPoa8/FMW\nykJZ1P6XVs2ZvbtPmtkWoAdYAmx39z4zu7v8+DYzu4zSlTYfAc6b2TeAq9z97ZnOTT3CAhkcHGz1\nEHJDWQRlEZRFdvXaOLj7bmB31b5tFf/9OtPbNTXPFRGR5tMdtDmyefPmVg8hN5RFUBZBWWRX86aq\npgzAzFs9BhFZWPbu3c/oaLpz2tqgq+v6xgyoBcwMT/EBbd02jjTPnj17WL9+fauHkQvKIiiLMJXF\n6Ch0dKQr3CMj+xs0qoVBbRwRkQJQG0dE5s3eH+9l9N10/ZW2pW10retKdU5Pz/5MM/sNG9TGERGZ\ns9F3R+lY1ZHqnJGBkQaNRiqpjZMje/bsafUQckNZBGURlEV2KvYiIgWgYp8juuIiKIugLIKyyE7F\nXkSkAFTsc0T9yKAsgrIIyiI7FXsRkQJQsc8R9SODsgjKIiiL7FTsRUQKQMU+R9SPDMoiKIugLLJT\nsRcRKQAV+xxRPzIoi6AsgrLITmvjiEhLHTl6At5Jt57OkSPHufnmxbOoWTOo2OeI1i0PyiIs9izG\nx5OvTd/bu4e1a9czPn68waNafFTsRWRGWZYrPtJ/hJtX3dygEclcqNjnyGKevaWlLEKrssiyXPH4\nkfEGjaZk7dr1DX3+xUwf0IqIFICKfY7oGuKgLIKyCL29e1o9hAVLxV5EpABU7HNEfeqgLIKyCOrZ\nZ6diLyJSACr2OaLebFAWQVkE9eyzU7EXESkAFfscUW82KIugLIJ69tnppioRmTeDg2fYt68v3Tkv\nn4bPNmhAcoGKfY4s9jVQ0lAWYSFlMTEJ7e1r0p0zcTTxsVNr40h6auOIiBSAin2OLJTZWzMoi6As\ngmb12anYi4gUgIp9juh66qAsgrIIus4+O31AK1IQaden19r0i0vdYm9m3cCDwBLgMXd/YIZjHgI+\nD4wCm939QHn/fcDvAueBI8Cd7t7YBa8XMPVmg7II85VF2vXpG702fRbq2WdXs41jZkuAh4Fu4Cpg\nk5mtqTpmI7DK3VcDdwGPlPd3Ar8PfMbdf4XSL4t/Oc/jFxGRBOrN7G8ABtx9EMDMdgK3AZV3TdwK\nPA7g7s+ZWbuZLQfeAiaANjN7D2gDzszv8BeXhXQ9daMpi9CqLLLcIDX0+kiDRlOi6+yzq1fsLwdO\nVWyfBv5xgmMud/efmtl3gVeAd4Aed//hHMcrIk2S5QapycldDRrN3B3tPwAfTPfLqG1pG13ruho0\nouaqV+w94fPY+3aYfQr4A6AT+HvgSTP7krs/UX3s5s2b6ezsBKC9vZ1rr732wkxm6kqEImyvX78+\nV+PRdn62p8z1+Xqf7QVg7Y1rE233HSxtr7k22fabw8P0HexNfHzfwV5eP/PKhZ9v6mqbqdl79Xbl\nviTHV26PnR9jcHgw1c//N0/+De+Nvdfy//tP1YYdO3YAXKiXaZj77PXczNYB97t7d3n7PuB85Ye0\nZvYosMfdd5a3+4GbgPXAP3X3r5T3/x6wzt2/WvUaXmsMIjI/en7Uk+oD2j/+j9u55Tf/VarXePzR\nB7jjnntTnfPDp57ka1/5D6nOefSxrVyz9sZU5xx6YTf3/OHvpTpnZGCEDZ/dkOqcZjEz3P19E+3Z\n1LvOvhdYbWadZrYUuB2o/jttF/Dl8ouvA866+xBwDFhnZh80MwNuAV5MOrAiqp7FFZmyCMoiTM3U\nJxinvbMj1b8Jf7e1g2+xmm0cd580sy1AD6Wraba7e5+Z3V1+fJu7P21mG81sADgH3Fl+7KCZfZ/S\nL4zzwE+B/9LAn0VERGZR9zp7d98N7K7at61qe8ss534H+M5cBlgkU306URaVlEXQlTjZabkEEZEC\nULHPEfVmg7IIyiJobZzstDaOiLTU0Mgp9h3sSXTsiYFDTFw8ztDIqfoHyzQq9jmi3mwoUhZ79+5n\ntOb6ZB+mp2f/tD1tbdDVdX1Dx9Usk0zQ3pnsktBf7fzchXMkHRV7kRYbHYWOjnSFe2Rkf/2DRCqo\n2OfIHq0Hc4GyCPO1HsyRoye49I03Eh/f6HVusqi8O1fSUbEXKYjxcbgsxVo3eV7nRtJTsc8RzWSD\nsggzzeqPHu1P/TyDL59m5dXzMKAW0qw+OxV7kQWof+A4l7QnX+cG4PRruoKlyFTsc0R96qAswkw9\n+6m1YdJYDFewqGefnW6qEhEpABX7HNFMNiiLoPVggmb12anYi4gUgIp9jmgNlKAsgtaDCVPfdCXp\nqdiLiBSAin2OqE8dlEVQzz6oZ5+dir2ISAGo2OeI+tRBWQT17IN69tmp2IuIFICKfY6oTx2URVDP\nPqhnn52KvYhIAajY54j61EFZBPXsg3r22anYi4gUgIp9jqhPHZRFUM8+qGefnYq9iEgBqNjniPrU\nQVkE9eyDevbZqdiLiBSAin2OqE8dlEVQzz6oZ5+dir2ISAHoO2hzRN+7GoqUxdH+A1yybGTWx0/0\nH2L1p6+Ztm9opJhfHq7voM1OxV6kxcbOj7G8xpeHLzv70fd9ufhi+PJwaS61cXKkKDPZJJRF0Ew2\nKIvsVOxFRApAbZwcKVKfuh5lEdSnDnPJYuj1N9i3ry/VOWNDZ9nw2Q2ZXi9vVOxFpBAmJ4329jWp\nzjn5yr4Gjab56rZxzKzbzPrN7ISZ3TvLMQ+VHz9kZtdV7G83s6fMrM/MXjSzdfM5+MVGM9mgLIJm\n9UFZZFez2JvZEuBhoBu4CthkZmuqjtkIrHL31cBdwCMVD/9n4Gl3XwNcDaT7G0pEROZFvZn9DcCA\nuw+6+wSwE7it6phbgccB3P05oN3MlpvZR4Eud//T8mOT7v738zv8xUXrwQRlEbQeTFAW2dUr9pcD\nlXdvnC7vq3fMFcAngTfM7L+Z2U/N7L+aWdtcBywiIunVK/ae8HlshvMuBj4D/Im7fwY4B3wr3fCK\nRX3qoCyC+tRBWWRX72qcM8CKiu0VlGbutY65orzPgNPu/pPy/qeYpdhv3ryZzs5OANrb27n22msv\n/D/71J/z2tb2Yt1+aeAEK6/+NSDaFFNFbbbtKUmPb9b2m8PD0y6PTHL+m8PDqX+eZv38Lw2cmHYZ\ncCvfL3v27GHHjh0AF+plGuY+++TdzC4GjgGfA14Fngc2uXtfxTEbgS3uvrF8tc2D7r6u/NiPgK+4\n+3Ezux/4oLvfW/UaXmsMRaJry0ORsvij7z18odjPZKZryx9/9AHuuGfGi+NmlfacZrxG2nOmsmjW\n2E4e3se3v7kl1TnNYma4e3VXZVY1Z/buPmlmW4AeYAmw3d37zOzu8uPb3P1pM9toZgOUWjV3VjzF\n14AnzGwp8LOqx0REpEnq3lTl7ruB3VX7tlVtz/irz90PAb86lwEWSVFmskkoi6A+dVAW2WltHBGR\nAtByCTlSpD51PQs1i7179zM6mu6cwZdPs/Lq2R/X2jhBWWSnYi8yj0ZHoaPj+lTnTEw82aDRiAQV\n+xxZiDPZRslDFllm6T94ehdXXT37t07NpN63TmkmG5RFdir2IrPIMksffffJ932rVD361ilpBn1A\nmyNaDyYoi6D1YIKyyE7FXkSkAFTscyQPfeq8UBZBfeqgLLJTz15EZBaDg6/Q07M/8fFtbdDVle5z\nnmZRsc+RhXpteSMoi6Bry0Ozs5h496JUH9KPjCT/xdBsKvYiIrMYGjnFvoM9iY8ff/sUGzZoZi91\naCYblEXQrD40O4tJJlJdSnvy8IkGjmZu9AGtiEgBqNjniK4tD8oi6NryoCyyU7EXESkAFfscUZ86\nKIugnn1QFtmp2IuIFICKfY6oTx2URVCfOiiL7FTsRUQKQMU+R9SnDsoiqE8dlEV2KvYiIgWgYp8j\n6lMHZRHUpw7KIjsVexGRAlCxzxH1qYOyCOpTB2WRnYq9iEgBqNjniPrUQVkE9amDsshOxV5EpABU\n7HNEfeqgLIL61EFZZJeLLy85/rPjqY5v/3A7H/vYxxo0GhGRxScXxf7UO6cSHzv2zhifOv+pRVns\n9b2rYb6z2Lt3P6Oj6c45cuQ4N9/c+q+Y03fQBmWRXS6Kffsvtic+9q2zbzVwJLJYjY6S6oujAcbH\n0/3FKZJn6tnniGb1QVkEzWSDsshOxV5EpABU7HNE15YHZRF0bXlQFtmp2IuIFICKfY6oTx2URVCf\nOiiL7OoWezPrNrN+MzthZvfOcsxD5ccPmdl1VY8tMbMDZvY/52vQIiKSTs1ib2ZLgIeBbuAqYJOZ\nrak6ZiOwyt1XA3cBj1Q9zTeAFwGfr0EvVupTB2UR1KcOyiK7ejP7G4ABdx909wlgJ3Bb1TG3Ao8D\nuPtzQLuZLQcwsyuAjcBjgM3nwEVEJLl6xf5yoPL21tPlfUmP+R7wb4DzcxhjYahPHZRFUJ86KIvs\n6t1Bm7T1Uj1rNzP7TWDY3Q+Y2frUIxNpscHT/ew72JPqnKGR5Et/iDRTvWJ/BlhRsb2C0sy91jFX\nlPf9NnBruad/KfARM/u+u3+5+kW2/sFWPrHiEwAs+8gyrvzlK1l7Y+k3eO+zpR7d1PbB5w9y6uJT\nfHr1p4Ho7U7NBBfydmWfOg/jaeX21L6ZHj98+BhXXll6Pxw6VHp/XHNN7e2LLvoIN998Pb29pedb\nu7b0fLW2JxjntbODQMwop3rGs20PD5+Ztn5LveP7Dvby5vDwhZ95psdPDhyj+4tfmvZ4reNbuf3m\n8PC8//yV28889QQrV13ZtJ8/7c/z0sCJaes6zXd92LFjBwCdnZ2kZe6zT97N7GLgGPA54FXgeWCT\nu/dVHLMR2OLuG81sHfCgu6+rep6bgD90938+w2t475nkH7q8dfYtPv6Bj18o9ouJFkILtbLo6dmf\nep2bZ575c7q7N6U6548f+xa3fPFfpDrn8Ucf4I57ZrxoLfM5My3+1YjXmevxzThnKos8jg3g5OF9\nfPubW1K9RlZmhrsn/iy05sze3SfNbAvQAywBtrt7n5ndXX58m7s/bWYbzWwAOAfcOdvTJR1UUanQ\nB2UR1KcOyiK7uqteuvtuYHfVvm1V2zV/lbn73wF/l2WAIiIyd7lY4lhK1MYJ853FQv6wVWu4B2WR\nnYq9FMIE47R3dqQ6Z5KJBo1GpPm0Nk6OaFYflEXQTDYoi+xU7EVECkDFPke0HkxQFkHrwQRlkZ2K\nvYhIAeTiA9paN3bN5diFRn3qoCyC+tRBWWSXi2L/4x8fS3zs6NvnuPry84vyDloRkUbJRbFvb09e\nuN8dP8PhF45gS99L9RptS9voWteVdmhNpevsg7IIurY8KIvsclHs0xo/P0bHqnTXTI8MjDRoNCIi\n+acPaHNEM9mgLIJmskFZZKdiLyJSACr2OaJry4OyCLq2PCiL7FTsRUQKQMU+R9SnDsoiqE8dlEV2\nKvYiIgWgYp8j6lMHZRHUpw7KIjsVexGRAlCxzxH1qYOyCOpTB2WRnYq9iEgBqNjniPrUQVkE9amD\nsshOxV5EpABU7HNEfeqgLIL61EFZZKdiLyJSACr2OaI+dVAWQX3qoCyyU7EXESkAFfscUZ86KIug\nPnVQFtmp2IuIFICKfY6oTx2URVCfOiiL7FTsRUQKQMU+R9SnDsoiqE8dlEV2KvYiIgVwcasH0CxH\nXzya+py2pW10retqwGhmtmfPHs1oy5RF6DvYqxltmbLIrjDFfuy9MTpWdaQ6Z2RgpEGjERFprsIU\n+4VAM9lQK4uj/Qe4ZFm6X8RDI6fmOKLW0Uw2KIvsVOxlwRk7P8byznR/pU0y0aDRiCwMiT6gNbNu\nM+s3sxNmdu8sxzxUfvyQmV1X3rfCzP7WzF4ws6Nm9vX5HPxio2vLg7IIurY8KIvs6hZ7M1sCPAx0\nA1cBm8xsTdUxG4FV7r4auAt4pPzQBPBNd/9lYB3w1epzRUSk8ZLM7G8ABtx90N0ngJ3AbVXH3Ao8\nDuDuzwHtZrbc3V9394Pl/W8DfcAn5m30i4x69kFZBPWpg7LILkmxvxyo/HTrdHlfvWOuqDzAzDqB\n64Dn0g5SRETmJskHtJ7wuWy288xsGfAU8I3yDH+abQ9s5ZcuK0342z60jJWrrrzwG3yqRze1PfDi\nEcZ+Pnjh3N5nS4+vvXFtze20x09tT/WOp2aajdyu7FM34/Uasf3QQ9sYG4Nrrinld+hQKc9a25de\nCl//+t3Tnq86k8rXe2ngBCuv/jXg/e+P2banJD0+6/abw8PTrgVPcv6bw8M1x3dy4BjdX/xSS36e\nPPz8ldvPPPUEK1dd2bSfP+3P89LAiWn3iMx3fdixYwcAnZ2dpGXutWu5ma0D7nf37vL2fcB5d3+g\n4phHgT3uvrO83Q/c5O5DZvYB4H8Bu939wRme3//sh8k/dBkeOsO5oRfYePtvJD4H4Jm/fIbu3+pO\ndc7IwAgbPrsh1TlzsRhuJOrp2U9Hx/WpzhkZ2c+GDdPPqZXFH33v4QvFPqnHH32AO+6Z8dqC3J8z\n041EzRhbXn7+SlNZ5HFsACcP7+Pb39yS6jWyMjPcvXqSPaskbZxeYLWZdZrZUuB2YFfVMbuAL5cH\nsA44Wy70BmwHXpyp0Mt0C73QzydlEdSnDsoiu7ptHHefNLMtQA+wBNju7n1mdnf58W3u/rSZbTSz\nAeAccGf59F8Hfhc4bGYHyvvuc/dn5jLolwfPsG9fX6pzBgfPzOUlRUQWtEQ3Vbn7bmB31b5tVdvv\n+9vF3f8vDVhsbWLCaG9PdwXnxOSz8z2MebcY2jjzRVkErQcTlEV2hbmDduj1N1L/NTA2dLapPXsR\nkUYpTLGfnEz/18DJV/Y1aDQzK+pM9mj/Afhg1Vo3F0HPj3pmPH7w1EDqD2gXMs1kg7LIrjDFXvKr\n/6XjXHLFpYmPP/3aqw0cjcjipC8vyZGirgcz8e5FtLevmfbvtcFz79s39W9ystUjbi6tBxOURXYq\n9iIiBaBinyNF7dnPRL3ZoCyCsshOxV5EpABU7HOkqD37mag3G5RFUBbZqdiLiBSAin2OqGcf1JsN\nyiIoi+xU7EVECkDFPkfUsw/qzQZlEZRFdrqDtobBV3426y37s2lb2kbXuq4GjSj/jvYf4JJlI/UP\nrDA0cqr+QSIyJyr2NUwwTseqjlTnjAykK3SVFkPPfuz8GMs702U2ycT79qk3G5RFUBbZqY0jIlIA\nKvY5op59UG82KIugLLJTG2ee/eCvn6F3/4nEx3+07VK23P2VBo5IRETFft6Nvjueaq31k4djzfzF\n0LOfL+rNBmURlEV2KvYtpit+RKQZVOxryPJVhkOvp7sap/KKn95ne1l7Y/2Zy1yu+Fko9F2jQVkE\nZZGdin0NWb7KcHJyV4NGIyKSnYp9jiSZ1QMcffFo6udeaK0fzd6CsgjKIjsV+wVo7L2xpt7sJSIL\nn4p9jiTt2TfL3r37GR1Nd87gy6dZefXcX1u92aAsgrLITsVeZjU6Ch0d16c6Z2LiyQaNRkTmQsU+\nR/I0q4fWLmqm2VtQFkFZZKdiXxBZPtTtP3mUm/7ZdanOmWlRMxFpPRX7HGlkzz7Lh7oT/m5DxpKE\nerNBWQRlkZ0WQhMRKQAV+xzJW8++lTR7C8oiKIvs1MYpiMHBMw1f+kFE8kvFPkca2bOfmGRBLf2g\n3mxQFkFZZKc2johIAWhm32LTVta0DyVqtex77iAfvWxlytdZWC0Zzd6CsgjKIjsV+xbLsrLm+Du7\nFlRLRkRar24bx8y6zazfzE6Y2b2zHPNQ+fFDZnZdmnMl6Ps1g7IIyiIoi+xqFnszWwI8DHQDVwGb\nzGxN1TEbgVXuvhq4C3gk6bky3cmBY60eQm4oi6AsgrLIrt7M/gZgwN0H3X0C2AncVnXMrcDjAO7+\nHNBuZpclPFcqjJ57u9VDyA1lEZRFUBbZ1Sv2lwOVK1udLu9LcswnEpwrIiJNUO8DWk/4PDaXQZw8\nfjjxseNjY1xkc3q53Hrj9VdbPYTcUBZBWQRlkZ25z17PzWwdcL+7d5e37wPOu/sDFcc8Cuxx953l\n7X7gJuCT9c4t70/6C0VERCq4e+KZb72ZfS+w2sw6gVeB24FNVcfsArYAO8u/HM66+5CZ/b8E56Ya\nrIiIZFOz2Lv7pJltAXqAJcB2d+8zs7vLj29z96fNbKOZDQDngDtrndvIH0ZERGZWs40jIiKLQ0vX\nxtFNV8HMBs3ssJkdMLPnWz2eZjKzPzWzITM7UrHvF83s/5jZcTP732bW3soxNsssWdxvZqfL740D\nZtbdyjE2i5mtMLO/NbMXzOyomX29vL9w740aWSR+b7RsZl++6eoYcAtwBvgJsKmorR4zexm43t3f\nbPVYms3MuoC3ge+7+6+U930HGHH375QnAr/g7t9q5TibYZYstgL/4O7/qaWDa7Ly/TqXuftBM1sG\n7Ae+QKlVXKj3Ro0sfoeE741Wzux109X7FfLDanffC/y8aveFm/XK//uFpg6qRWbJAgr43nD31939\nYPm/3wb6KN2rU7j3Ro0sIOF7o5XFPskNW0XiwA/NrNfMfr/Vg8mB5e4+VP7vIWB5KweTA18rrz21\nvQhti2rlq/quA56j4O+Niix+XN6V6L3RymKvT4an+3V3vw74PPDV8p/zAnip11jk98sjlO5buRZ4\nDfhua4fTXOW2xV8A33D3f6h8rGjvjXIWT1HK4m1SvDdaWezPACsqtldQmt0Xkru/Vv7fN4C/pNTm\nKrKhcp8SM/s4MNzi8bSMuw97GfAYBXpvmNkHKBX6P3P3vyrvLuR7oyKL/z6VRZr3RiuL/YUbtsxs\nKaWbrgq56LqZtZnZh8v//SHgN4Ajtc9a9HYBd5T/+w7gr2ocu6iVC9qU36Ig7w0zM2A78KK7P1jx\nUOHeG7Nlkea90dLr7M3s88CDxE1X/75lg2khM/skpdk8lG50e6JIWZjZn1NaYqODUg/23wE/AP4H\n8I+AQeB33P1sq8bYLDNksRVYT+nPdAdeBu6u6FkvWmb2T4AfAYeJVs19wPMU7L0xSxb/ltKqBIne\nG7qpSkSkAPSF4yIiBaBiLyJSACr2IiIFoGIvIlIAKvYiIgWgYi8iUgAq9iIiBaBiLyJSAP8f9mEK\n1ikCIjsAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x25451fd0>"
       ]
      }
     ],
     "prompt_number": 349
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# look at day of week success\n",
      "dow = TimeTransformer(do_hour=False, do_month=False).transform(all_train_df.values).flatten()\n",
      "dow_pos = dow[all_train_labels]\n",
      "dow_neg = dow[np.logical_not(all_train_labels)]\n",
      "pd.Series(dow_pos).hist(bins=7, alpha=0.2, normed=True)\n",
      "pd.Series(dow_neg).hist(bins=7, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 350,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x199a9ac8>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x20bdce80>"
       ]
      }
     ],
     "prompt_number": 350
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# look at month success\n",
      "month = TimeTransformer(do_hour=False, do_dow=False).transform(all_train_df.values).flatten()\n",
      "month_pos = month[all_train_labels]\n",
      "month_neg = month[np.logical_not(all_train_labels)]\n",
      "pd.Series(month_pos).hist(bins=12, alpha=0.2, normed=True)\n",
      "pd.Series(month_neg).hist(bins=12, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 351,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x17d667b8>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDTjZm5m1ASd7M7M24GRvZtYG/j/DOa7JZOCSOAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x197bc2e8>"
       ]
      }
     ],
     "prompt_number": 351
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Times not a very useful classifier by themselves, but maybe in conjuctions with others"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# classifier based on times\n",
      "\n",
      "svc = Pipeline([('scaler',StandardScaler()), ('svc',SVC(class_weight='auto'))])\n",
      "\n",
      "print '\\nSVC'\n",
      "svc_pipe = Pipeline([\n",
      "    ('time',TimeTransformer()),\n",
      "    ('model',svc)\n",
      "    ])\n",
      "\n",
      "print_scores(cross_val_score(svc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nExtra Tree Ensemble'\n",
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_pipe = Pipeline([\n",
      "    ('time',TimeTransformer()),\n",
      "    ('model',etc)\n",
      "    ])\n",
      "\n",
      "print_scores(cross_val_score(etc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC\n",
        "N: 10, Mean: 0.534091, Median: 0.540619, SD: 0.024005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Tree Ensemble\n",
        "N: 10, Mean: 0.531512, Median: 0.529352, SD: 0.022831"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 352
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Text Summary Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "class TextSummaryTransformer(TransformerMixin):\n",
      "    def __init__(self, title_col=TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True):\n",
      "        self.do_title = do_title\n",
      "        self.do_body = do_body\n",
      "        self.title_col = title_col\n",
      "        self.body_col = body_col\n",
      "\n",
      "    def transform(self, X, **transform_params):\n",
      "        do_title = self.do_title\n",
      "        do_body = self.do_body\n",
      "        title_col = self.title_col\n",
      "        body_col = self.body_col\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        if do_title:\n",
      "            title_unicode = X[:, title_col]\n",
      "            title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "            features.append(title_len)\n",
      "            \n",
      "        if do_body:\n",
      "            body_unicode = X[:, body_col]\n",
      "            body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "            features.append(body_len)\n",
      "        \n",
      "        return np.hstack(tuple(features))\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self \n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Title and Body character length\n",
      "Not at all useful based on linear classifiers... but lots of value added with tree ensembles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create some text derivative features\n",
      "# title character length\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "\n",
      "# body character length\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "\n",
      "\n",
      "X_text_summary = np.hstack((title_len, body_len))\n",
      "\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "print '\\nSVC on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('svc', svc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nExtra Trees Classifier on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('etc', etc)])\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "\n",
      "print '\\nGradient Boosting on title and body length'\n",
      "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('gbc', gbc)])\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC on title and body length\n",
        "N: 10, Mean: 0.491877, Median: 0.490472, SD: 0.013954"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees Classifier on title and body length\n",
        "N: 10, Mean: 0.568696, Median: 0.562971, SD: 0.025445"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Gradient Boosting on title and body length\n",
        "N: 10, Mean: 0.561088, Median: 0.555999, SD: 0.020587"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 365
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Word Case Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DEFAULT_TOKENIZER = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "# To calc the percent of words that are all upper, all lower, or mixed case\n",
      "class CaseAnalysisTransformer(TransformerMixin):\n",
      "    def __init__(self, upper=True, lower=True, mixed=True, tokenizer=DEFAULT_TOKENIZER):\n",
      "        self.upper = upper\n",
      "        self.lower = lower\n",
      "        self.mixed = mixed\n",
      "        self.tokenizer = tokenizer\n",
      "        \n",
      "    def count_case(self, words):\n",
      "        upper = 0.\n",
      "        lower = 0.\n",
      "        mixed = 0.\n",
      "        total = 1.\n",
      "        \n",
      "        for word in words:\n",
      "            isupper = word.isupper() \n",
      "            islower = word.islower() \n",
      "            \n",
      "            upper += isupper\n",
      "            lower += islower\n",
      "            mixed += not(isupper or islower)\n",
      "            total += 1\n",
      "            \n",
      "        return (upper, lower, mixed, total)\n",
      "    \n",
      "    \n",
      "    def tokenize_and_count(self, text, tokenizer):\n",
      "        return self.count_case(tokenizer.tokenize(text))\n",
      "    \n",
      "    def process_vector(self, texts, tokenizer):\n",
      "        ulist = []\n",
      "        llist = []\n",
      "        mlist = []\n",
      "        tlist = []\n",
      "        \n",
      "        for text in texts:\n",
      "            u, l, m, t = self.tokenize_and_count(text, tokenizer)\n",
      "            ulist.append(u)\n",
      "            llist.append(l)\n",
      "            mlist.append(m)\n",
      "            tlist.append(t)\n",
      "        \n",
      "        tar = np.array(tlist)\n",
      "        uar = np.array(ulist)/tar\n",
      "        lar = np.array(llist)/tar\n",
      "        mar = np.array(mlist)/tar\n",
      "        \n",
      "        return np.vstack((uar, lar, mar)).T\n",
      "        \n",
      "    def transform(self, X, **transform_params):\n",
      "        upper = self.upper\n",
      "        lower = self.lower\n",
      "        mixed = self.mixed\n",
      "        tokenizer = self.tokenizer\n",
      "        process_vector = self.process_vector\n",
      "        \n",
      "        return process_vector(X, tokenizer)\n",
      "        \n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc = LinearSVC(class_weight='auto')\n",
      "etc = ExtraTreesClassifier(n_estimators = 100,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "\n",
      "title_case = Pipeline([('col', ExtractTitle()), ('case', CaseAnalysisTransformer())])\n",
      "body_case = Pipeline([('col', ExtractBody()), ('case', CaseAnalysisTransformer())])\n",
      "both_case = FeatureUnion([('title', title_case), ('body', body_case)])\n",
      "\n",
      "features = both_case\n",
      "\n",
      "pipe_lsvc = Pipeline([('features', features), ('model', lsvc)])\n",
      "pipe_etc = Pipeline([('features', features), ('model', etc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "\n",
      "scores = cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)\n",
      "\n",
      "scores = cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.2s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.3s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.3s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.7s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.525390, Median: 0.527014, SD: 0.017622\n",
        "N: 5, Mean: 0.539171, Median: 0.546703, SD: 0.020333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Words in Brown Corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "from nltk.tokenize.regexp import RegexpTokenizer\n",
      "from sklearn.feature_extraction import text as sklearn_text\n",
      "brown_words = np.unique(np.array(brown.words()))\n",
      "brown_words = np.unique(np.array([x.lower() for x in brown_words]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 561
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "DEFAULT_WORD_SET = set(brown_words)\n",
      "DEFAULT_STOP_WORDS = set(['a','request','the','and','for'])\n",
      "DEFAULT_TOKENIZER = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
      "\n",
      "# To calc the percent of words that are all upper, all lower, or mixed case\n",
      "class InCorpusTransformer(TransformerMixin):\n",
      "    def __init__(self, word_set=DEFAULT_WORD_SET, tokenizer=DEFAULT_TOKENIZER, stop_words=DEFAULT_STOP_WORDS):\n",
      "        self.word_set = word_set\n",
      "        self.tokenizer = tokenizer\n",
      "        self.stop_words = stop_words\n",
      "        self.tokenizer = tokenizer\n",
      "    \n",
      "    def count_tokens(self, tokens):\n",
      "        if len(tokens) == 0:\n",
      "            return 2\n",
      "        else:\n",
      "            return sum(np.array([token.lower() in self.word_set for token in tokens]))/float(len(tokens))\n",
      "    \n",
      "    def tokenize_and_count(self, text):\n",
      "        tokens = [x.lower() for x in self.tokenizer.tokenize(text) if x.lower() not in self.stop_words]\n",
      "        return self.count_tokens(tokens)\n",
      "    \n",
      "    def process_vector(self, texts):\n",
      "        return np.array([[self.tokenize_and_count(text) for text in texts]]).T\n",
      "        \n",
      "    def transform(self, X, **transform_params):\n",
      "        if len(X.shape) == 1:\n",
      "            return self.process_vector(X)\n",
      "        else:\n",
      "            features = []\n",
      "            for col in range(X.shape[1]):\n",
      "                features.append(self.process_vector(X[:,col]))\n",
      "            return np.hstack(tuple(features))\n",
      "        \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 562
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 562
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc = LinearSVC(class_weight='auto')\n",
      "etc = ExtraTreesClassifier(n_estimators = 100,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "\n",
      "title = Pipeline([('col', ExtractTitle()), ('incorpus', InCorpusTransformer())])\n",
      "body = Pipeline([('col', ExtractBody()), ('incorpus', InCorpusTransformer())])\n",
      "both = FeatureUnion([('title', title), ('body', body)])\n",
      "\n",
      "features = both\n",
      "\n",
      "pipe_lsvc = Pipeline([('features', features), ('model', lsvc)])\n",
      "pipe_etc = Pipeline([('features', features), ('model', etc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 563
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)\n",
      "\n",
      "scores = cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.9s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.9s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    1.0s\n",
        "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.3s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.519416, Median: 0.519370, SD: 0.007789\n",
        "N: 5, Mean: 0.517488, Median: 0.527981, SD: 0.020337"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 564
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Interesting Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
      "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
      "\n",
      "class InterestingWordsTransformer(TransformerMixin):\n",
      "    def __init__(self, title_col = TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True, do_tags=True, do_words=True):\n",
      "        self.do_title = do_title\n",
      "        self.do_body = do_body\n",
      "        self.do_tags = do_tags\n",
      "        self.do_words = do_words\n",
      "        self.title_col = title_col\n",
      "        self.body_col = body_col\n",
      "        \n",
      "        self.keywords = {\n",
      "            'sad_food': ['hungry', 'starving', 'no food', 'grocer', 'eaten', 'hunger', 'ramen', 'empty', 'fridge', 'refrig'],\n",
      "            'money': ['broke', 'paid', 'money', 'unemployed', 'lost', 'job', 'bill', 'wage', 'work', 'payday', 'paycheck', 'funds', 'cash', 'bank', 'laid off', 'poor', 'payroll'],\n",
      "            'sad': ['worst', 'awful', 'sick', 'problem', 'catch a break', 'cheer', 'hospital', 'bad', 'shitty', 'stress', 'luck', ':(', 'rough', 'tough'],\n",
      "            'military': ['military', 'veteran', 'soldier', 'army', 'navy', 'marine', 'air force', 'iraq', 'afghanis'],\n",
      "            'happy': ['celebrate', 'birthday', 'party', 'new year', 'bday', 'engage', 'annivers'],\n",
      "            'nice': ['please', 'help', 'thank you', ':)'],\n",
      "            'honest': ['sob story', 'honest', 'just want', 'just because'],\n",
      "            'parent': ['family', 'kids', 'parent', 'mom', 'mommy', 'mother', 'dad', 'father', 'baby', 'boy', 'girl'],\n",
      "            'relationship': ['husband', 'wife', 'girlfriend', 'boyfriend', 'fianc'],\n",
      "            'test': ['study', 'test', 'final', 'midterm']\n",
      "        }\n",
      "    \n",
      "    def find_tag_words(self, keywords, text):\n",
      "        word_dict = {}\n",
      "        tag_dict = {}\n",
      "\n",
      "        for tag, words in keywords.iteritems():\n",
      "\n",
      "            tag_count = None\n",
      "\n",
      "            for word in words:\n",
      "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
      "                word_dict[word] = has_word\n",
      "\n",
      "                if tag_count is None:\n",
      "                    tag_count = has_word\n",
      "                else:\n",
      "                    tag_count = tag_count +  has_word\n",
      "\n",
      "            tag_dict[tag] = tag_count\n",
      "\n",
      "        return (tag_dict, word_dict)\n",
      "    \n",
      "    # manually create keywords with categories\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        do_title = self.do_title\n",
      "        do_tags = self.do_tags\n",
      "        do_words = self.do_words\n",
      "        do_body = self.do_body\n",
      "        keywords = self.keywords\n",
      "        find_tag_words = self.find_tag_words\n",
      "        body_col = self.body_col\n",
      "        title_col = self.title_col\n",
      "        \n",
      "        features = []\n",
      "        feature_names = []\n",
      "\n",
      "        # find keywords and tags\n",
      "        if do_title:\n",
      "            title_unicode = np.array([x.lower() for x in X[:,title_col]])\n",
      "            title_tag_dict, title_word_dict = find_tag_words(keywords, title_unicode)\n",
      "\n",
      "            if do_tags:\n",
      "                features.append(pd.DataFrame(title_tag_dict).values)\n",
      "                feature_names.append('title_tags')\n",
      "            if do_words:\n",
      "                features.append(pd.DataFrame(title_word_dict).values)\n",
      "                feature_names.append('title_words')\n",
      "\n",
      "        if do_body:\n",
      "            body_unicode = np.array([x.lower() for x in X[:,body_col]])\n",
      "            body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
      "\n",
      "            if do_tags:\n",
      "                features.append(pd.DataFrame(body_tag_dict).values)\n",
      "                feature_names.append('body_tags')\n",
      "\n",
      "            if do_words:\n",
      "                features.append(pd.DataFrame(body_word_dict).values)\n",
      "                feature_names.append('body_words')\n",
      "\n",
      "        return np.hstack(tuple(features))\n",
      "    \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        return {'do_words': self.do_words, 'do_tags':self.do_tags, 'do_body':self.do_body, 'do_title':self.do_title}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CheckWordsTransformer(TransformerMixin):\n",
      "    def __init__(self, words=[]):\n",
      "        self.words = words\n",
      "        \n",
      "    def find_words(self, words, text, lower=True):\n",
      "        word_dict = {}\n",
      "\n",
      "        for word in words:\n",
      "            if lower:\n",
      "                has_word = np.array([(1 if word in t.lower() else 0) for t in text])\n",
      "            else:\n",
      "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
      "            word_dict[word] = has_word\n",
      "\n",
      "        return word_dict\n",
      "    \n",
      "    # manually create keywords with categories\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        words = self.words\n",
      "        find_words = self.find_words\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        if len(X.shape) > 1:\n",
      "            cols = X.shape[1]\n",
      "            for i in cols:\n",
      "                features.append(pd.DataFrame(find_words(words, X[:,i])).values)\n",
      "        else:\n",
      "            features.append(pd.DataFrame(find_words(words, X)).values)\n",
      "            \n",
      "        return np.hstack(tuple(features))\n",
      "    \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {'words': self.words}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 558
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsvc = LinearSVC(class_weight='auto')\n",
      "etc = ExtraTreesClassifier(class_weight='auto', n_estimators=200, max_depth=3, min_samples_split=15)\n",
      "\n",
      "pipe_body = Pipeline([('body',ExtractBody()), ('words',CheckWordsTransformer(words=['kid', 'sob story','?','generous', 'reddit', 'random acts of pizza','bumm','you','fulfill','forward','trade','http','thank','raop','edit','rough','crav','apprec',':-)',':)',':D','verif','comfort']))])\n",
      "pipe_title = Pipeline([('title',ExtractTitle()), ('words',CheckWordsTransformer(words=['kid', 'sob story','?','generous', 'reddit', 'random acts of pizza','bumm','high','you''fulfill','forward','trade','thank','raop','rough','crav','apprec',':-)',':)',':D','verif','comfort']))])\n",
      "union = FeatureUnion([('body', pipe_body), ('title', pipe_title)])\n",
      "\n",
      "pipe_etc = Pipeline([('features',union), ('model',etc)])\n",
      "pipe_lsvc = Pipeline([('features',union), ('model',lsvc)])\n",
      "pipe_etc_l1 = Pipeline([('features',union), ('l1', LinearWeightFeatureThreshold(C=0.5)), ('model',etc)])\n",
      "pipe_lsvc_l1 = Pipeline([('features',union), ('l1', LinearWeightFeatureThreshold(C=0.5)), ('model',lsvc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 559
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_scores(cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "print_scores(cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 5, Mean: 0.543740, Median: 0.539840, SD: 0.017327\n",
        "N: 5, Mean: 0.553995, Median: 0.555618, SD: 0.010388"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 560
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_scores(cross_val_score(pipe_etc_l1, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
      "print_scores(cross_val_score(pipe_lsvc_l1, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 30/44 features\n",
        "kept 33/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.546746, Median: 0.552490, SD: 0.021307"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 33/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 30/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 31/44 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.548057, Median: 0.550613, SD: 0.016725"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 334
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 10, Mean: 0.605632, Median: 0.606877, SD: 0.033131\n"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train a model based on keywords and tags\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_oob = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           oob_score = True,\n",
      "                           bootstrap=True,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "\n",
      "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_pca = Pipeline([('scale', StandardScaler()),('pca', RandomizedPCA(n_components=3)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_l1 = Pipeline([('scale', StandardScaler()),('l1', LinearWeightFeatureThreshold(C=0.05)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "\n",
      "\n",
      "models = {'Extra Trees':etc, 'ET Out Of Bag':etc_oob, 'Gradient Boosting':gbc, 'Linear SVC':lsvc, 'Linear SVC PCA 3':lsvc_pca}\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Tags'\n",
      "trans = InterestingWordsTransformer(do_words=False)\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "    \n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n",
      "\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Words'\n",
      "trans = InterestingWordsTransformer(do_tags=False)\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "\n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n",
      "\n",
      "\n",
      "print '\\n##############'\n",
      "print 'Body & Title Words & Tags'\n",
      "trans = InterestingWordsTransformer()\n",
      "X_tag = trans.transform(all_train_df.values)\n",
      "\n",
      "for name, model in models.iteritems():\n",
      "    print '\\n%s' % name\n",
      "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
      "    pipe = Pipeline([('trans',trans),('model',model)])\n",
      "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
      "\n",
      "    test_kfolds(X_tag, all_train_labels, kf, model, balance = (name=='Gradient Boosting'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "##############\n",
        "Body & Title Tags\n",
        "\n",
        "Gradient Boosting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.541294, Median: 0.544889, SD: 0.023428"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.544456, Median: 0.541295, SD: 0.018260"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC PCA 3\n",
        "N: 10, Mean: 0.552116, Median: 0.554540, SD: 0.018983"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.549922, Median: 0.556199, SD: 0.022521"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 10, Mean: 0.562793, Median: 0.577642, SD: 0.037403"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.564433, Median: 0.577177, SD: 0.038860"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "ET Out Of Bag\n",
        "N: 10, Mean: 0.553270, Median: 0.550050, SD: 0.029467"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.555012, Median: 0.560387, SD: 0.021861"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 10, Mean: 0.560665, Median: 0.568786, SD: 0.026703"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.558828, Median: 0.566584, SD: 0.023481"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "##############\n",
        "Body & Title Words\n",
        "\n",
        "Gradient Boosting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.535372, Median: 0.531543, SD: 0.015413"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.534783, Median: 0.529863, SD: 0.015799"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC PCA 3\n",
        "N: 10, Mean: 0.552632, Median: 0.539333, SD: 0.029402"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.559155, Median: 0.554687, SD: 0.037255"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 10, Mean: 0.550509, Median: 0.547695, SD: 0.025243"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.554421, Median: 0.561411, SD: 0.020314"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "ET Out Of Bag\n",
        "N: 10, Mean: 0.553206, Median: 0.548741, SD: 0.022753"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.560378, Median: 0.563916, SD: 0.023590"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 10, Mean: 0.560430, Median: 0.558719, SD: 0.016200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.556140, Median: 0.560807, SD: 0.015813"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "##############\n",
        "Body & Title Words & Tags\n",
        "\n",
        "Gradient Boosting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.539874, Median: 0.532418, SD: 0.020234"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.539794, Median: 0.541293, SD: 0.017479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC PCA 3\n",
        "N: 10, Mean: 0.551116, Median: 0.548721, SD: 0.021255"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.549982, Median: 0.555799, SD: 0.022020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Linear SVC\n",
        "N: 10, Mean: 0.546079, Median: 0.535257, SD: 0.027261"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.540863, Median: 0.542467, SD: 0.023064"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "ET Out Of Bag\n",
        "N: 10, Mean: 0.564023, Median: 0.559451, SD: 0.024543"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.560831, Median: 0.560426, SD: 0.027316"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Extra Trees\n",
        "N: 10, Mean: 0.565079, Median: 0.569591, SD: 0.020935"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.563260, Median: 0.566523, SD: 0.021464"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 367
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 2,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "etc_oob = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 2,\n",
      "                           min_samples_split=10,\n",
      "                           oob_score = True,\n",
      "                           bootstrap=True,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "\n",
      "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_pca = Pipeline([('scale', StandardScaler()),('pca', RandomizedPCA(n_components=3)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "lsvc_l1 = Pipeline([('scale', StandardScaler()),('l1', LinearWeightFeatureThreshold(C=0.05)), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "\n",
      "megapipe = Pipeline([\n",
      "    ('subsets', FeatureUnion([\n",
      "        ('title_tags', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_body=False,do_words=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca),\n",
      "            ]))\n",
      "        ])),\n",
      "        ('body_tags', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_title=False,do_words=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca)\n",
      "            ]))\n",
      "        ])),\n",
      "        ('title_words', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_body=False,do_tags=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca),\n",
      "                ('lsvc_l1', lsvc_l1)\n",
      "            ]))\n",
      "        ])),\n",
      "        ('title_tags', Pipeline([\n",
      "            ('interesting',InterestingWordsTransformer(do_title=False,do_tags=False)),\n",
      "            ('models', FeatureUnion([\n",
      "                ('etc', etc),\n",
      "                ('etc_oob', etc_oob),\n",
      "                ('lsvc', lsvc),\n",
      "                ('lsvc_pca', lsvc_pca),\n",
      "                ('lsvc_l1', lsvc_l1)\n",
      "            ]))\n",
      "        ])),\n",
      "    ])),\n",
      "    ('ensemble', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 357
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "scorer = make_scorer(roc_auc_score)\n",
      "scores = cross_val_score(megapipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 64/85 features\n",
        "kept 72/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 40/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 75/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 37/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 68/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 46/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 72/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 35/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 59/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 65/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 70/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 69/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 73/85 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 358
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('tsum', TextSummaryTransformer()),\n",
      "        ('tags', InterestingWordsTransformer(do_words=False))\n",
      "        ])\n",
      "    ),\n",
      "    ('etc', etc)\n",
      "])\n",
      "\n",
      "\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 354
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Geographic Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MANUAL_GEOS = [\n",
      "{'loc':'nyc', 'g1':'ny', 'g2':'us'}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "def make_geo(other_geos=MANUAL_GEOS, filter_loc=[]):\n",
      "\n",
      "    from bs4 import BeautifulSoup\n",
      "    from urllib import urlopen\n",
      "    import re\n",
      "    \n",
      "    ######################\n",
      "    # Scrape wikipedia list of us cities\n",
      "    \n",
      "    # TODO save local\n",
      "    webpage = urlopen('http://en.wikipedia.org/wiki/List_of_United_States_cities_by_population')\n",
      "    soup=BeautifulSoup(webpage, \"html.parser\")\n",
      "    table = soup.find('table', {'class' : 'wikitable sortable'})\n",
      "    \n",
      "    us_cities = []\n",
      "\n",
      "    rows = table.findAll('tr')\n",
      "    for row in rows[1:200]:\n",
      "        cells = row.findAll('td')\n",
      "\n",
      "        output = []\n",
      "\n",
      "        for i, cell in enumerate(cells):\n",
      "            if i < 4:\n",
      "                text = cell.text.strip().lower()\n",
      "                if i == 0:\n",
      "                    text = int(text)\n",
      "                if i == 1 or i == 2:\n",
      "                    text = re.sub(r\"\\[.*\\]|'\",'',text)\n",
      "                if i == 3:\n",
      "                    text = int(re.sub(r',','',text))\n",
      "                output.append(text)\n",
      "        us_cities.append(output)\n",
      "\n",
      "    us_cities = pd.DataFrame(np.array(us_cities),columns=['rank','city','state','pop'])\n",
      "    \n",
      "    ###########################\n",
      "    # tuple list of state abbreviations\n",
      "    \n",
      "    state_abr_raw = [(\"Alabama\",\"AL\"),(\"Alaska\",\"AK\"),(\"Arizona\",\"AZ\"),\n",
      "                     (\"Arkansas\",\"AR\"),(\"California\",\"CA\"),(\"Colorado\",\"CO\"),\n",
      "                     (\"Connecticut\",\"CT\"),(\"Delaware\",\"DE\"),(\"District of Columbia\",\"DC\"),\n",
      "                     (\"Florida\",\"FL\"),(\"Georgia\",\"GA\"),(\"Hawaii\",\"HI\"),\n",
      "                     (\"Idaho\",\"ID\"),(\"Illinois\",\"IL\"),(\"Indiana\",\"IN\"),\n",
      "                     (\"Iowa\",\"IA\"),(\"Kansas\",\"KS\"),(\"Kentucky\",\"KY\"),\n",
      "                     (\"Louisiana\",\"LA\"),(\"Maine\",\"ME\"),(\"Montana\",\"MT\"),\n",
      "                     (\"Nebraska\",\"NE\"),(\"Nevada\",\"NV\"),(\"New Hampshire\",\"NH\"),\n",
      "                     (\"New Jersey\",\"NJ\"),(\"New Mexico\",\"NM\"),(\"New York\",\"NY\"),\n",
      "                     (\"North Carolina\",\"NC\"),(\"North Dakota\",\"ND\"),(\"Ohio\",\"OH\"),\n",
      "                     (\"Oklahoma\",\"OK\"),(\"Oregon\",\"OR\"),(\"Maryland\",\"MD\"),\n",
      "                     (\"Massachusetts\",\"MA\"),(\"Michigan\",\"MI\"),(\"Minnesota\",\"MN\"),\n",
      "                     (\"Mississippi\",\"MS\"),(\"Missouri\",\"MO\"),(\"Pennsylvania\",\"PA\"),\n",
      "                     (\"Rhode Island\",\"RI\"),(\"South Carolina\",\"SC\"),(\"South Dakota\",\"SD\"),\n",
      "                     (\"Tennessee\",\"TN\"),(\"Texas\",\"TX\"),(\"Utah\",\"UT\"),\n",
      "                     (\"Vermont\",\"VT\"),(\"Virginia\",\"VA\"),(\"Washington\",\"WA\"),\n",
      "                     (\"West Virginia\",\"WV\"),(\"Wisconsin\",\"WI\"),(\"Wyoming\",\"WY\")]\n",
      "    \n",
      "    ############################\n",
      "    # manupulate state abreviations\n",
      "    state_abr = []\n",
      "    for st, abr in state_abr_raw:\n",
      "        state_abr.append([st.lower(), abr.lower()])\n",
      "    state_abr = pd.DataFrame(np.array(state_abr), columns = ['state','abr'])\n",
      "    \n",
      "    #############################\n",
      "    # make US Geos\n",
      "    us_city_state = pd.merge(us_cities,state_abr)\n",
      "    \n",
      "    # US geos\n",
      "    usgeo = us_city_state.loc[:,['city','abr']]\n",
      "    usgeo.columns = ['loc','g1']\n",
      "    usgeo = pd.concat([usgeo,pd.DataFrame({'loc':state_abr.abr,'g1':state_abr.abr})])\n",
      "    usgeo = pd.concat([usgeo,pd.DataFrame({'loc':state_abr.state,'g1':state_abr.abr})])\n",
      "    usgeo['g2'] = 'us'\n",
      "    \n",
      "    # add other geos\n",
      "    other_geos = [\n",
      "        {'loc':'nyc', 'g1':'ny', 'g2':'us'}\n",
      "    ]\n",
      "    \n",
      "    geo = pd.concat([usgeo, pd.DataFrame(other_geos)])\n",
      "    \n",
      "    # get rid of auto generated locations with confusiong names\n",
      "    #geo = geo[[not x in filter_loc for x in geo['loc']]]\n",
      "    \n",
      "    return geo\n",
      "\n",
      "\n",
      "\n",
      "geo = make_geo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 682
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DEFAULT_TOKENIZER = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
      "FILTER_DEFAULT = ['in', 'hi', 'me', 'ok', 'HI', 'OK', 'or']\n",
      "DEFAULT_GEO = geo\n",
      "\n",
      "class GeoTransformer(TransformerMixin):\n",
      "    def __init__(self, geo=DEFAULT_GEO, tokenizer=DEFAULT_TOKENIZER, stop_words=FILTER_DEFAULT):\n",
      "        self.tokenizer = tokenizer\n",
      "        self.stop_words = stop_words\n",
      "        self.geo = geo\n",
      "        \n",
      "    def find_words(self, words, texts, g1=None, lower=True):\n",
      "        word_dict = {}\n",
      "        do_g1 = not g1 is None\n",
      "        \n",
      "        if do_g1:\n",
      "            g1_dict = {}\n",
      "        \n",
      "        token_list = [[x.lower() for x in self.tokenizer.tokenize(text) if x not in self.stop_words] for text in texts]\n",
      "    \n",
      "        i = 0\n",
      "        for word in words:\n",
      "            has_word = np.array([(1 if word in tokens else 0) for tokens in token_list])\n",
      "            word_dict[word] = has_word\n",
      "            if do_g1:\n",
      "                g1i = g1.iloc[i]\n",
      "                if g1i in g1_dict:\n",
      "                    g1_dict[g1i] += has_word\n",
      "                else:\n",
      "                    g1_dict[g1i] = has_word\n",
      "            i += 1\n",
      "\n",
      "        return (word_dict, g1_dict)\n",
      "    \n",
      "    def transform(self, X, **transform_params):\n",
      "        geo = self.geo\n",
      "        find_words = self.find_words\n",
      "        \n",
      "        words = geo['loc']\n",
      "        g1 = geo['g1']\n",
      "        \n",
      "        features = []\n",
      "        \n",
      "        \n",
      "        if len(X.shape) > 1:\n",
      "            cols = X.shape[1]\n",
      "            for i in cols:\n",
      "                locs, g1s = find_words(words, X[:,i], g1)\n",
      "                df = pd.DataFrame(g1s)\n",
      "                #df = pd.DataFrame(locss)\n",
      "                features.append(df.values)\n",
      "        else:\n",
      "            locs, g1s = find_words(words, X, g1)\n",
      "            df = pd.DataFrame(g1s)\n",
      "            #df = pd.DataFrame(locss)\n",
      "            features.append(df.values)\n",
      "        \n",
      "        self.feature_names_ = df.columns\n",
      "        \n",
      "        return np.hstack(tuple(features))\n",
      "    \n",
      "    def fit(self, X, y, **fit_params):\n",
      "        #do nothing\n",
      "        return self\n",
      "    \n",
      "    def get_params(self, deep=True):\n",
      "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
      "        return {}\n",
      "\n",
      "    def set_params(self, **parameters):\n",
      "        for parameter, value in parameters.items():\n",
      "            self.setattr(parameter, value)\n",
      "        return self\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 741
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "geo_trans = GeoTransformer(geo)\n",
      "geo_title = geo_trans.transform(ExtractTitle().transform(all_train_df.values))\n",
      "geo_body = geo_trans.transform(ExtractBody().transform(all_train_df.values))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 742
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### look at occurences of locations in titles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'total occurences:'\n",
      "print np.sum(geo_title)\n",
      "print 'individual occurences:'\n",
      "print zip(geo_trans.feature_names_, np.sum(geo_title,0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total occurences:\n",
        "2109\n",
        "individual occurences:\n",
        "[('ak', 11), ('al', 15), ('ar', 12), ('az', 78), ('ca', 191), ('co', 55), ('ct', 19), ('dc', 30), ('de', 7), ('fl', 143), ('ga', 90), ('hi', 4), ('ia', 17), ('id', 12), ('il', 83), ('in', 97), ('ks', 32), ('ky', 57), ('la', 14), ('ma', 43), ('md', 24), ('me', 31), ('mi', 54), ('mn', 32), ('mo', 27), ('ms', 12), ('mt', 5), ('nc', 48), ('nd', 1), ('ne', 8), ('nh', 3), ('nj', 23), ('nm', 10), ('nv', 12), ('ny', 75), ('oh', 93), ('ok', 14), ('or', 65), ('pa', 66), ('ri', 10), ('sc', 27), ('sd', 2), ('tn', 51), ('tx', 206), ('ut', 13), ('va', 54), ('vt', 9), ('wa', 86), ('wi', 31), ('wv', 6), ('wy', 1)]\n"
       ]
      }
     ],
     "prompt_number": 743
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### look at occurences of locations in body"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'total occurences:'\n",
      "print np.sum(geo_body)\n",
      "print 'individual occurences:'\n",
      "print zip(geo_trans.feature_names_, np.sum(geo_body,0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total occurences:\n",
        "1527\n",
        "individual occurences:\n",
        "[('ak', 10), ('al', 8), ('ar', 11), ('az', 39), ('ca', 121), ('co', 35), ('ct', 8), ('dc', 29), ('de', 6), ('fl', 63), ('ga', 41), ('hi', 161), ('ia', 6), ('id', 40), ('il', 42), ('in', 148), ('ks', 10), ('ky', 23), ('la', 12), ('ma', 24), ('md', 8), ('me', 58), ('mi', 33), ('mn', 14), ('mo', 14), ('ms', 19), ('mt', 7), ('nc', 20), ('nd', 2), ('ne', 8), ('nh', 5), ('nj', 5), ('nm', 1), ('nv', 5), ('ny', 46), ('oh', 100), ('ok', 11), ('or', 68), ('pa', 31), ('ri', 2), ('sc', 6), ('sd', 3), ('tn', 29), ('tx', 77), ('ut', 6), ('va', 31), ('vt', 6), ('wa', 54), ('wi', 19), ('wv', 2), ('wy', 0)]\n"
       ]
      }
     ],
     "prompt_number": 744
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators = 200,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto'))])\n",
      "\n",
      "title = Pipeline([('text',ExtractTitle()),('geo',GeoTransformer(geo))])\n",
      "body = Pipeline([('text',ExtractBody()),('geo',GeoTransformer(geo))])\n",
      "union = FeatureUnion([('title', title), ('body', body)])\n",
      "\n",
      "pipe = Pipeline([('features',body),('model',etc)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 748
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Body:'\n",
      "pipe = Pipeline([('features',body),('model',etc)])\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Body:\n",
        "N: 5, Mean: 0.499531, Median: 0.496690, SD: 0.005479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 749
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Title:'\n",
      "pipe = Pipeline([('features',title),('model',etc)])\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Title:\n",
        "N: 5, Mean: 0.508351, Median: 0.508290, SD: 0.011660"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 750
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Union:'\n",
      "pipe = Pipeline([('features',union),('model',etc)])\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Union:\n",
        "N: 5, Mean: 0.514810, Median: 0.516635, SD: 0.015900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 751
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Combine Feature Sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=True, tokenizer=SnowballStemTokenizer())\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "kf = KFold(n_all, n_folds = 5)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 376
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        ('title_l1', Pipeline([\n",
      "            ('title',ExtractTitle()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    ('model', etc)\n",
      "])\n",
      "\n",
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer)\n",
      "\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 56/9149 features\n",
        "kept 19/3654 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 28/3664 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 20/3661 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/3678 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/3653 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 5, Mean: 0.603441, Median: 0.599189, SD: 0.014246"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 377
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N: 10, Mean: 0.604379, Median: 0.600186, SD: 0.022009\n"
       ]
      }
     ],
     "prompt_number": 373
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    ('scaler', StandardScaler()),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=0.025)),\n",
      "    ('model', etc)\n",
      "])\n",
      "\n",
      "#scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=scorer, verbose=1)\n",
      "\n",
      "#print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 410
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### somewhat shallow trees with heavy leaves and lots of estimators works for tree ensembles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    ('scaler', StandardScaler()),\n",
      "    #('l1', LinearWeightFeatureThreshold(C=0.025)),\n",
      "    ('model', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 433
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(pipe, {'model__max_depth':[2,5,10], 'model__min_samples_split':[5,15,30], 'model__n_estimators':[10,100,1000]}, cv=kf, scoring=scorer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 434
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out = gs.fit(all_train_df.values, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 56/9149 features\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 56/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 49/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 52/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 78/10502 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 435
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 438,
       "text": [
        "[mean: 0.56231, std: 0.02606, params: {'model__min_samples_split': 5, 'model__max_depth': 2, 'model__n_estimators': 10},\n",
        " mean: 0.59372, std: 0.01838, params: {'model__min_samples_split': 5, 'model__max_depth': 2, 'model__n_estimators': 100},\n",
        " mean: 0.60112, std: 0.01604, params: {'model__min_samples_split': 5, 'model__max_depth': 2, 'model__n_estimators': 1000},\n",
        " mean: 0.57620, std: 0.01930, params: {'model__min_samples_split': 15, 'model__max_depth': 2, 'model__n_estimators': 10},\n",
        " mean: 0.60701, std: 0.00887, params: {'model__min_samples_split': 15, 'model__max_depth': 2, 'model__n_estimators': 100},\n",
        " mean: 0.59932, std: 0.01495, params: {'model__min_samples_split': 15, 'model__max_depth': 2, 'model__n_estimators': 1000},\n",
        " mean: 0.57421, std: 0.01948, params: {'model__min_samples_split': 30, 'model__max_depth': 2, 'model__n_estimators': 10},\n",
        " mean: 0.59124, std: 0.00913, params: {'model__min_samples_split': 30, 'model__max_depth': 2, 'model__n_estimators': 100},\n",
        " mean: 0.60015, std: 0.01492, params: {'model__min_samples_split': 30, 'model__max_depth': 2, 'model__n_estimators': 1000},\n",
        " mean: 0.57286, std: 0.01361, params: {'model__min_samples_split': 5, 'model__max_depth': 5, 'model__n_estimators': 10},\n",
        " mean: 0.60602, std: 0.01030, params: {'model__min_samples_split': 5, 'model__max_depth': 5, 'model__n_estimators': 100},\n",
        " mean: 0.60180, std: 0.01268, params: {'model__min_samples_split': 5, 'model__max_depth': 5, 'model__n_estimators': 1000},\n",
        " mean: 0.59076, std: 0.00856, params: {'model__min_samples_split': 15, 'model__max_depth': 5, 'model__n_estimators': 10},\n",
        " mean: 0.60216, std: 0.01855, params: {'model__min_samples_split': 15, 'model__max_depth': 5, 'model__n_estimators': 100},\n",
        " mean: 0.60238, std: 0.01469, params: {'model__min_samples_split': 15, 'model__max_depth': 5, 'model__n_estimators': 1000},\n",
        " mean: 0.57426, std: 0.01693, params: {'model__min_samples_split': 30, 'model__max_depth': 5, 'model__n_estimators': 10},\n",
        " mean: 0.60426, std: 0.01772, params: {'model__min_samples_split': 30, 'model__max_depth': 5, 'model__n_estimators': 100},\n",
        " mean: 0.60378, std: 0.01437, params: {'model__min_samples_split': 30, 'model__max_depth': 5, 'model__n_estimators': 1000},\n",
        " mean: 0.57119, std: 0.00920, params: {'model__min_samples_split': 5, 'model__max_depth': 10, 'model__n_estimators': 10},\n",
        " mean: 0.58727, std: 0.00875, params: {'model__min_samples_split': 5, 'model__max_depth': 10, 'model__n_estimators': 100},\n",
        " mean: 0.58746, std: 0.01397, params: {'model__min_samples_split': 5, 'model__max_depth': 10, 'model__n_estimators': 1000},\n",
        " mean: 0.57046, std: 0.00745, params: {'model__min_samples_split': 15, 'model__max_depth': 10, 'model__n_estimators': 10},\n",
        " mean: 0.58806, std: 0.01306, params: {'model__min_samples_split': 15, 'model__max_depth': 10, 'model__n_estimators': 100},\n",
        " mean: 0.59405, std: 0.01211, params: {'model__min_samples_split': 15, 'model__max_depth': 10, 'model__n_estimators': 1000},\n",
        " mean: 0.56690, std: 0.01727, params: {'model__min_samples_split': 30, 'model__max_depth': 10, 'model__n_estimators': 10},\n",
        " mean: 0.59691, std: 0.01337, params: {'model__min_samples_split': 30, 'model__max_depth': 10, 'model__n_estimators': 100},\n",
        " mean: 0.59639, std: 0.01500, params: {'model__min_samples_split': 30, 'model__max_depth': 10, 'model__n_estimators': 1000}]"
       ]
      }
     ],
     "prompt_number": 438
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### With gradient boosting, it looks likes slow learning and lots of estimators is the way to go"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 442
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = Pipeline([\n",
      "    ('preprocess', FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_words=True))\n",
      "    ])),\n",
      "    #('scaler', StandardScaler()),\n",
      "    #('l1', LinearWeightFeatureThreshold(C=0.025)),\n",
      "    ('model', gbc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 443
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kf_over = oversample_kfold(kf, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 444
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(pipe, {'model__learning_rate':[.01,.1,1,5], 'model__n_estimators':[20,100,200]}, cv=kf_over, scoring=scorer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 445
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out = gs.fit(all_train_df.values, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 168/9149 features\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 168/9149 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 182/9381 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 166/9167 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9378 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 177/9332 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 78/10502 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 446
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 448,
       "text": [
        "[mean: 0.59138, std: 0.01668, params: {'model__learning_rate': 0.01, 'model__n_estimators': 20},\n",
        " mean: 0.59708, std: 0.01149, params: {'model__learning_rate': 0.01, 'model__n_estimators': 100},\n",
        " mean: 0.60140, std: 0.00658, params: {'model__learning_rate': 0.01, 'model__n_estimators': 200},\n",
        " mean: 0.59951, std: 0.01125, params: {'model__learning_rate': 0.1, 'model__n_estimators': 20},\n",
        " mean: 0.58094, std: 0.01407, params: {'model__learning_rate': 0.1, 'model__n_estimators': 100},\n",
        " mean: 0.57900, std: 0.00585, params: {'model__learning_rate': 0.1, 'model__n_estimators': 200},\n",
        " mean: 0.56158, std: 0.00434, params: {'model__learning_rate': 1, 'model__n_estimators': 20},\n",
        " mean: 0.55415, std: 0.01995, params: {'model__learning_rate': 1, 'model__n_estimators': 100},\n",
        " mean: 0.54720, std: 0.00403, params: {'model__learning_rate': 1, 'model__n_estimators': 200},\n",
        " mean: 0.47438, std: 0.03003, params: {'model__learning_rate': 5, 'model__n_estimators': 20},\n",
        " mean: 0.47924, std: 0.01348, params: {'model__learning_rate': 5, 'model__n_estimators': 100},\n",
        " mean: 0.47316, std: 0.02715, params: {'model__learning_rate': 5, 'model__n_estimators': 200}]"
       ]
      }
     ],
     "prompt_number": 448
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### what combinations of features work best?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=True, tokenizer=LemmaTokenizer())\n",
      "gbc = GradientBoostingClassifier(n_estimators = 200,\n",
      "                                 learning_rate=0.01,\n",
      "                                 max_depth = 4,\n",
      "                                 min_samples_split=10)\n",
      "etc = ExtraTreesClassifier(n_estimators = 200,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=15,\n",
      "                           class_weight='auto')\n",
      "kf_over = oversample_kfold(kf, all_train_labels)\n",
      "\n",
      "preprocess = FeatureUnion([\n",
      "        ('body_l1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('edit', Pipeline([('body',ExtractBody()), ('words',CheckWordsTransformer(words=['edit']))])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_body=False, do_words=False)),\n",
      "        ('body_case', Pipeline([('text', ExtractBody()), ('case', CaseAnalysisTransformer())])),\n",
      "        ('title_geo', Pipeline([('text',ExtractTitle()),('geo',GeoTransformer(geo))])),\n",
      "        ('body_geo', Pipeline([('text',ExtractBody()),('geo',GeoTransformer(geo))]))\n",
      "    ])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', preprocess),\n",
      "    ('scaler', StandardScaler()),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=1)),\n",
      "    ('model', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 755
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=KFold(n_all, 10), scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 59/11901 features\n",
        "kept 167/189 features"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Find the right C in the l1 feature reduction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=True, tokenizer=SnowballStemTokenizer())\n",
      "#tv = TfidfVectorizer(ngram_range=(1,1), lowercase=True, tokenizer=PuncTokenizer())\n",
      "etc = ExtraTreesClassifier(n_estimators = 200,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "preprocess = FeatureUnion([\n",
      "        ('bodyl1', Pipeline([\n",
      "            ('body', ExtractBody()),\n",
      "            ('tv', tv),\n",
      "            ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        ])),\n",
      "        #('title_l1', Pipeline([\n",
      "        #    ('title',ExtractTitle()),\n",
      "        #    ('tv', tv),\n",
      "        #    ('l1', LinearWeightFeatureThreshold(C=0.0025))\n",
      "        #])),\n",
      "        ('body_words', Pipeline([('body',ExtractBody()), ('words',CheckWordsTransformer(words=['thank']))])),\n",
      "        ('title_words', Pipeline([('body',ExtractTitle()), ('words',CheckWordsTransformer(words=['thank']))])),\n",
      "        ('body_spell', Pipeline([('col', ExtractTitle()), ('incorpus', InCorpusTransformer())])),\n",
      "        ('title_spell', Pipeline([('col', ExtractBody()), ('incorpus', InCorpusTransformer())])),\n",
      "        ('activity',ExtractActivities()),\n",
      "        ('time', TimeTransformer()),\n",
      "        ('text_summary', TextSummaryTransformer()),\n",
      "        ('interesting', InterestingWordsTransformer(do_tags=True, do_body=False, do_words=False)),\n",
      "        ('body_case', Pipeline([('text', ExtractBody()), ('case', CaseAnalysisTransformer())])),\n",
      "        ('title_case', Pipeline([('text', ExtractBody()), ('case', CaseAnalysisTransformer())]))\n",
      "    ])\n",
      "\n",
      "pipe = Pipeline([\n",
      "    ('preprocess', preprocess),\n",
      "    ('scaler', StandardScaler()),\n",
      "    ('l1', LinearWeightFeatureThreshold(C=1)),\n",
      "    ('model', etc)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 593
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=KFold(n_all, 10), scoring=roc_scorer, verbose=1)\n",
      "print_scores(scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 63/9901 features\n",
        "kept 86/97 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 64/9890 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 91/98 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9868 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 91/96 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9965 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 95/101 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9924 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 85/94 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 74/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 101/108 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9907 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 88/97 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 72/9905 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 97/106 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 91/101 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9888 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 97/105 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    9.6s\n",
        "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.6min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N: 10, Mean: 0.603630, Median: 0.604366, SD: 0.022445"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 594
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(pipe, {\n",
      "    'preprocess__bodyl1__l1__C':[.0025,.005],\n",
      "    #'preprocess__bodyl1__tv__tokenizer':[None, LemmaTokenizer(), SnowballStemTokenizer()],\n",
      "    'preprocess__bodyl1__tv__ngram_range':[(1,1), (1,2)],\n",
      "    }, cv=KFold(n_all,10), scoring=roc_scorer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 579
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_out = gs.fit(all_train_df.values, all_train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kept 63/9901 features\n",
        "kept 64/9890 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 62/9868 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9965 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 60/9924 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 74/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 63/9907 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 72/9905 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 67/9895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 71/9888 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/95369 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/95248 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 23/94799 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 23/95520 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 21/94837 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 23/95409 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 20/94802 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 20/94852 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 25/94694 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 22/94418 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 188/9901 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 196/9890 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 180/9868 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 203/9965 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 189/9924 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 180/9913 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 188/9907 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 191/9905 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 178/9895 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 190/9888 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 101/95369 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 115/95248 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 103/94799 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 117/95520 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 111/94837 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 126/95409 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 107/94802 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 111/94852 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 118/94694 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 112/94418 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kept 208/10502 features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.array(gs_out.grid_scores_[:6])\n",
      "print '---'\n",
      "print np.array(gs_out.grid_scores_[6:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ {'preprocess__bodyl1__l1__C': 0.0025, 'preprocess__bodyl1__tv__ngram_range': (1, 1)}\n",
        "  0.59576193245957354\n",
        "  array([ 0.61850466,  0.59076525,  0.5859745 ,  0.57758387,  0.60118421,\n",
        "        0.60231082,  0.61336399,  0.63673372,  0.54167366,  0.58952464])]\n",
        " [ {'preprocess__bodyl1__l1__C': 0.0025, 'preprocess__bodyl1__tv__ngram_range': (1, 2)}\n",
        "  0.59266843495683863\n",
        "  array([ 0.597039  ,  0.60112198,  0.56466302,  0.59388528,  0.57960526,\n",
        "        0.6063817 ,  0.6279961 ,  0.62871169,  0.54192552,  0.5853548 ])]\n",
        " [ {'preprocess__bodyl1__l1__C': 0.005, 'preprocess__bodyl1__tv__ngram_range': (1, 1)}\n",
        "  0.60804264267509989\n",
        "  array([ 0.63498694,  0.61982163,  0.58912072,  0.58570076,  0.61756579,\n",
        "        0.61871408,  0.59127648,  0.66792385,  0.56365224,  0.59166395])]\n",
        " [ {'preprocess__bodyl1__l1__C': 0.005, 'preprocess__bodyl1__tv__ngram_range': (1, 2)}\n",
        "  0.60518230911457982\n",
        "  array([ 0.61377931,  0.61284522,  0.60236794,  0.6072105 ,  0.58434211,\n",
        "        0.58758381,  0.63928372,  0.64188218,  0.57233285,  0.59019544])]]\n",
        "---\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}