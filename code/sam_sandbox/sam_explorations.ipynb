{
 "metadata": {
  "name": "",
  "signature": "sha256:3e5e58016f61e8c6fe0595a7633675947a0a1cf16db6bff17ab5c50adab9484c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "# General libraries.\n",
      "import json\n",
      "import csv\n",
      "import re\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# SK-learn libraries for feature extraction from text.\n",
      "from sklearn.feature_extraction.text import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard load and submission generation functions from @Ross\n",
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# === Convert the data to data frames for easier processing, and set up train/dev/test ===\n",
      "\n",
      "# First, bring in the JSON data:\n",
      "full_train_json = load_json_file('../../data/train.json')\n",
      "test_json = load_json_file('../../data/test.json')\n",
      "\n",
      "# Randomize the data (Commented for now to ensure repeatability of results)\n",
      "#rand.shuffle(full_train_json)\n",
      "\n",
      "full_train_df = pd.DataFrame(full_train_json)\n",
      "test_df = pd.DataFrame(test_json)\n",
      "\n",
      "# Generate the training labels:\n",
      "full_train_labels = np.array(full_train_df[\"requester_received_pizza\"])\n",
      "\n",
      "# The test data has fewer columns, so we're going to restrict to\n",
      "# just the columns that train and test share.\n",
      "# The test data seems to be meant to simulate that the data were collected\n",
      "# instantly on posting, so it is missing any \"_at_retrieval\" fields, as well\n",
      "# as anything indicating that it was edited (pre-edit text, whether it was edited, etc).\n",
      "# Also, it's missing flair. Which is mean. What's wrong with flair?\n",
      "full_train_df = full_train_df[test_df.columns]\n",
      "\n",
      "# Check in with the training and test data shapes:\n",
      "print \"Overall training data shape is: \" + str(full_train_df.shape)\n",
      "print \"Test data shape is: \" + str(test_df.shape)\n",
      "\n",
      "n_train_all = len(train_all_json)\n",
      "n_test = len(test_json)\n",
      "\n",
      "\n",
      "# === Randomize training data and split into train and dev sets ===\n",
      "# set size of dev set\n",
      "pct_dev = 0.25\n",
      "n_dev = int(n_train_all * pct_dev)\n",
      "n_train = n_train_all - n_dev\n",
      "\n",
      "# Split the training dataframe into train and dev\n",
      "train_df = full_train_df[n_dev:]\n",
      "dev_df = full_train_df[:n_dev]\n",
      "\n",
      "# Also process, then split, the labels\n",
      "train_labels = full_train_labels[n_dev:]\n",
      "dev_labels = full_train_labels[:n_dev]\n",
      "\n",
      "# Check in with the train and dev data shapes:\n",
      "print \"Training data shape is: \" + str(train_df.shape)\n",
      "print \"Test data shape is: \" + str(dev_df.shape)\n",
      "print \"Training labels shape is: \" + str(train_labels.shape)\n",
      "print \"Test labels shape is: \" + str(dev_labels.shape)\n",
      "\n",
      "print \"The labels we have to work with include:\"\n",
      "print full_train_df.columns.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overall training data shape is: (4040, 17)\n",
        "Test data shape is: (1631, 17)\n",
        "Training data shape is: (3030, 17)\n",
        "Test data shape is: (1010, 17)\n",
        "Training labels shape is: (3030,)\n",
        "Test labels shape is: (1010,)\n",
        "The labels we have to work with include:\n",
        "[u'giver_username_if_known' u'request_id' u'request_text_edit_aware'\n",
        " u'request_title' u'requester_account_age_in_days_at_request'\n",
        " u'requester_days_since_first_post_on_raop_at_request'\n",
        " u'requester_number_of_comments_at_request'\n",
        " u'requester_number_of_comments_in_raop_at_request'\n",
        " u'requester_number_of_posts_at_request'\n",
        " u'requester_number_of_posts_on_raop_at_request'\n",
        " u'requester_number_of_subreddits_at_request'\n",
        " u'requester_subreddits_at_request'\n",
        " u'requester_upvotes_minus_downvotes_at_request'\n",
        " u'requester_upvotes_plus_downvotes_at_request' u'requester_username'\n",
        " u'unix_timestamp_of_request' u'unix_timestamp_of_request_utc']\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# === Exploring the text data - title only - a la Assignment 2 to get a feel for it ===\n",
      "# First, generate the CountVectorizer and train it with the training title contents.\n",
      "train_title_cv = CountVectorizer()\n",
      "train_title_tdm = train_title_cv.fit_transform(train_df[\"request_title\"])\n",
      "print \"The size of the vocabulary is %d.\" % len(train_title_cv.vocabulary_)\n",
      "print \"The average number of non-zero features per example is %.3f.\" % (train_title_tdm.nnz / float(train_title_tdm.shape[0]))\n",
      "print \"The fraction of non-zero items in the matrix is %.4f.\" % (train_title_tdm.nnz / float(train_title_tdm.shape[0] * train_title_tdm.shape[1]))\n",
      "\n",
      "# Generate the term-document matrix for the dev title contents based on the training vocab:\n",
      "dev_title_tdm = train_title_cv.transform(dev_df[\"request_title\"])\n",
      "\n",
      "# We'll use a GridSearch for each parameter optimization.\n",
      "# knn test from Assignment 2\n",
      "ks = {'n_neighbors': [1, 2, 3, 4, 5, 6, 8, 10, 15, 20]}\n",
      "knn_gs = GridSearchCV(KNeighborsClassifier(), ks, scoring='roc_auc')\n",
      "knn_gs.fit(train_title_tdm, train_labels)\n",
      "knn_gs.score(dev_title_tdm, dev_labels)\n",
      "print \"\\n===== k-Nearest Neighbors =====\"\n",
      "print \"The optimal parameter for kNN is %s with roc_auc of %.3f.\" % (str(knn_gs.best_params_), knn_gs.best_score_)\n",
      "\n",
      "# Multinomial NB test from Assignment 2\n",
      "alphas = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 3.0]}\n",
      "mnb_gs = GridSearchCV(MultinomialNB(), alphas, scoring='roc_auc')\n",
      "mnb_gs.fit(train_title_tdm, train_labels)\n",
      "mnb_gs.score(dev_title_tdm, dev_labels)\n",
      "print \"\\n===== Multinomial Naive Bayes =====\"\n",
      "print \"The optimal parameter for MultinomialNB is %s with roc_auc of %.3f.\" % (str(mnb_gs.best_params_), mnb_gs.best_score_)\n",
      "\n",
      "# LogReg test from Assignment 2\n",
      "cs = {'C': [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 5.0]}\n",
      "logr_gs = GridSearchCV(LogisticRegression(), cs, scoring='roc_auc')\n",
      "logr_gs.fit(train_title_tdm, train_labels)\n",
      "logr_gs.score(dev_title_tdm, dev_labels)\n",
      "print \"\\n===== Logistic Regression =====\"\n",
      "print \"The optimal parameter for Logistic Regression is %s with roc_auc of %.3f.\" % (str(logr_gs.best_params_), logr_gs.best_score_)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The size of the vocabulary is 3832.\n",
        "The average number of non-zero features per example is 11.278.\n",
        "The fraction of non-zero items in the matrix is 0.0029.\n",
        "\n",
        "===== k-Nearest Neighbors ====="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The optimal parameter for kNN is {'n_neighbors': 15} with roc_auc of 0.521.\n",
        "\n",
        "===== Multinomial Naive Bayes ====="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The optimal parameter for MultinomialNB is {'alpha': 0.1} with roc_auc of 0.525.\n",
        "\n",
        "===== Logistic Regression ====="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The optimal parameter for Logistic Regression is {'C': 0.5} with roc_auc of 0.536.\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make submission csv\n",
      "#submit_id = [x['request_id'] for x in submit_dict_list]\n",
      "#base_predict = lr_base.predict(np.array([[1]]*n_submit))\n",
      "#make_submission_csv(base_predict, submit_id, 'baseline')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    }
   ],
   "metadata": {}
  }
 ]
}