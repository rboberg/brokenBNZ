{
 "metadata": {
  "name": "",
  "signature": "sha256:c131b8822ffe4c13a657b70e35ad0e2070eb2b039fc51d75566b31f86af03fb9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import pandas as pd\n",
      "import scipy as scipy\n",
      "import datetime as dt\n",
      "import time\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import make_scorer\n",
      "from sklearn.ensemble import GradientBoostingClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "\n",
      "\n",
      "\n",
      "def balance_samples(y, method='oversample'):\n",
      "    class_counts = np.bincount(y)\n",
      "    \n",
      "    maxi = np.argmax(class_counts)\n",
      "    new_idx = np.argwhere(y==maxi)\n",
      "    \n",
      "    for i in range(len(class_counts)):\n",
      "        if i != maxi:\n",
      "            mult = class_counts[maxi]/class_counts[i]\n",
      "            rem = class_counts[maxi] - class_counts[i]*mult\n",
      "            idxi = np.argwhere(y==i)\n",
      "            np.random.shuffle(idxi)\n",
      "            for j in range(mult):\n",
      "                new_idx = np.vstack((new_idx,idxi))\n",
      "            new_idx = np.vstack((new_idx,idxi[:rem]))\n",
      "        \n",
      "    np.random.shuffle(new_idx)\n",
      "\n",
      "    return np.reshape(new_idx, (new_idx.shape[0],))\n",
      "\n",
      "            \n",
      "def test_kfolds(X, y, kf, model, verbose=True, balance=False):\n",
      "    roc_auc_list = []\n",
      "    \n",
      "    for train_i, dev_i in kf:\n",
      "        if balance:\n",
      "            train_i_orig = train_i\n",
      "            y_train = y[train_i_orig]\n",
      "            train_i = train_i_orig[balance_samples(y_train)]\n",
      "        \n",
      "        \n",
      "        X_train = X[train_i]\n",
      "        X_dev = X[dev_i]\n",
      "\n",
      "        model.fit(X_train, y[train_i])\n",
      "\n",
      "        dev_pred = model.predict(X_dev)\n",
      "        \n",
      "        roc_auc_i = roc_auc_score(y[dev_i], dev_pred)\n",
      "        roc_auc_list.append(roc_auc_i)\n",
      "        if verbose:\n",
      "            print('ROC AUC:',roc_auc_i)\n",
      "            \n",
      "    if verbose:\n",
      "        print 'Mean: %f, Median: %f' %(np.mean(roc_auc_list), np.median(roc_auc_list))\n",
      "        \n",
      "    return roc_auc_list\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data from JSON file as list of dicts\n",
      "all_train_dict_list = load_json_file('../../data/train.json')\n",
      "submit_dict_list =  load_json_file('../../data/test.json')\n",
      "\n",
      "n_all = len(all_train_dict_list)\n",
      "n_submit = len(submit_dict_list)\n",
      "\n",
      "# shuffle data to avoid biased split of train / dev data\n",
      "rand.shuffle(all_train_dict_list)\n",
      "\n",
      "# set up kFolds\n",
      "kf = KFold(n_all, n_folds = 5)\n",
      "\n",
      "# process labels\n",
      "all_train_labels = np.array([x['requester_received_pizza'] for x in all_train_dict_list])\n",
      "\n",
      "# pandas is useful for turning dicts in to matrix-like objects\n",
      "# where each column is an numpy array\n",
      "submit_df = pd.DataFrame(submit_dict_list)\n",
      "all_train_df = pd.DataFrame(all_train_dict_list)\n",
      "\n",
      "# limit train to columns available in submit_df\n",
      "submit_cols = submit_df.columns\n",
      "all_train_df = all_train_df[submit_cols]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# which columns are numeric activity variables\n",
      "activity_var = ['requester_account_age_in_days_at_request',\n",
      "                'requester_days_since_first_post_on_raop_at_request',\n",
      "                'requester_number_of_comments_at_request',\n",
      "                'requester_number_of_comments_in_raop_at_request',\n",
      "                'requester_number_of_posts_at_request',\n",
      "                'requester_number_of_posts_on_raop_at_request',\n",
      "                'requester_number_of_subreddits_at_request',\n",
      "                'requester_upvotes_minus_downvotes_at_request',\n",
      "                'requester_upvotes_plus_downvotes_at_request'\n",
      "                ]\n",
      "# look at correlations of requester activity variables:\n",
      "activity_all = all_train_df.loc[:,activity_var]\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_act = scaler.fit_transform(activity_all)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example classification\n",
      "svc = SVC(class_weight='auto')\n",
      "lr = LogisticRegression(class_weight='auto')\n",
      "\n",
      "all_res = test_kfolds(X_act, all_train_labels, kf, svc)\n",
      "\n",
      "all_res = test_kfolds(X_act, all_train_labels, kf, lr)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.53405315614617943)\n",
        "('ROC AUC:', 0.53744047472685996)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.53894038866622396)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58045177221312383)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.54338896557493621)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.546855, Median: 0.538940\n",
        "('ROC AUC:', 0.54762442344289253)\n",
        "('ROC AUC:', 0.54096896079733126)\n",
        "('ROC AUC:', 0.54614079520027548)\n",
        "('ROC AUC:', 0.54261881931657152)\n",
        "('ROC AUC:', 0.52205211862634382)\n",
        "Mean: 0.539881, Median: 0.542619\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Notes\n",
      "# results slightly better w/ lowercase = False (when unigrams only)\n",
      "# bigrams added no value on unigrams\n",
      "\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "\n",
      "cv = CountVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "lsvc_pipe = Pipeline([('cv',cv),('lsvc',lsvc)])\n",
      "\n",
      "test_kfolds(titles, all_train_labels, kf, lsvc_pipe)\n",
      "\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "lsvc_pipe = Pipeline([('tv',tv),('lsvc',lsvc)])\n",
      "\n",
      "test_kfolds(titles, all_train_labels, kf, lsvc_pipe)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.51440183208076629)\n",
        "('ROC AUC:', 0.51164277459490026)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.51912185366413399)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.56461768224901776)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.53045133224578578)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.528047, Median: 0.519122\n",
        "('ROC AUC:', 0.51567590233203242)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.50673322022506906)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.52414615554845223)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.54803686135412866)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.52742293052244116)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.524403, Median: 0.524146\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[0.51567590233203242,\n",
        " 0.50673322022506906,\n",
        " 0.52414615554845223,\n",
        " 0.54803686135412866,\n",
        " 0.52742293052244116]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try to mix text and numeric via output of text only model\n",
      "#NOTES\n",
      "# For text model that is input in to the final model, LinearSVC isn't great\n",
      "# because it only gives binary predictions \n",
      "\n",
      "X1 = all_train_df['request_title'].values\n",
      "X2 = all_train_df.loc[:,activity_var].values\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "cv = CountVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = .1)\n",
      "mnb = MultinomialNB(alpha=50)\n",
      "model1 = Pipeline([('cv',cv),('lsvc',lsvc)])\n",
      "#model1 = Pipeline([('cv',cv),('mnb',mnb)])\n",
      "model2 = SVC(class_weight='auto', C=1)\n",
      "\n",
      "roc_auc_list = []\n",
      "    \n",
      "for train_i, dev_i in kf:\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    X_train1 = X1[train_i]\n",
      "    X_dev1 = X1[dev_i]\n",
      "    \n",
      "    # fit initial text model\n",
      "    model1.fit(X_train1, y[train_i])\n",
      "    \n",
      "    if False:\n",
      "        #if predict has proba\n",
      "        train_pred1 = model1.predict_log_proba(X_train1)[:,0].reshape((train_i.shape[0],1))\n",
      "        dev_pred1 = model1.predict_log_proba(X_dev1)[:,0].reshape((dev_i.shape[0],1))\n",
      "        #print train_pred1\n",
      "    else:\n",
      "        train_pred1 = model1.decision_function(X_train1).reshape((train_i.shape[0],1))\n",
      "        dev_pred1 = model1.decision_function(X_dev1).reshape((dev_i.shape[0],1))\n",
      "    \n",
      "    # add output of initial model in to second X matrix\n",
      "    X_train2 = np.hstack((X2[train_i], train_pred1))\n",
      "    X_dev2 = np.hstack((X2[dev_i], dev_pred1))\n",
      "    \n",
      "    #X_train2 = X2[train_i]\n",
      "    #X_dev2 = X2[dev_i]\n",
      "    \n",
      "    # scale the new X matrix\n",
      "    scaler = StandardScaler()\n",
      "    X_train2 = scaler.fit_transform(X_train2)\n",
      "    X_dev2 = scaler.transform(X_dev2)\n",
      "    \n",
      "    # train the second model\n",
      "    model2.fit(X_train2, y[train_i])\n",
      "    dev_pred2 = model2.predict(X_dev2)\n",
      "\n",
      "    roc_auc_i = roc_auc_score(y[dev_i], dev_pred2)\n",
      "    roc_auc_list.append(roc_auc_i)\n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "     print 'Mean: %f, Median: %f' %(np.mean(roc_auc_list), np.median(roc_auc_list))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.51912718124052504)\n",
        "('ROC AUC:', 0.51661789897300969)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.56638553525617386)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.56174955662850989)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.52287614506211566)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.537351, Median: 0.522876\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(kf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine title and text approaches\n",
      "\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "\n",
      "X1 = titles\n",
      "X2 = bodies\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "\n",
      "model = lsvc\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_both = []\n",
      "    \n",
      "for train_i, dev_i in kf:\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    if do_titles:\n",
      "        X_train = tv.fit_transform(X1[train_i])\n",
      "        X_dev = tv.transform(X1[dev_i])\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.005)\n",
      "            model.fit(X_train, y[train_i])\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'l2', C=5)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y[train_i])\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y[dev_i], pred)\n",
      "        roc_auc_list_titles.append(roc_auc_i)\n",
      "        \n",
      "        \n",
      "        X_train1 = X_train\n",
      "        X_dev1 = X_dev\n",
      "    \n",
      "    if do_bodies:\n",
      "        X_train = tv.fit_transform(X2[train_i])\n",
      "        X_dev = tv.transform(X2[dev_i])\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.0025)\n",
      "            model.fit(X_train, y[train_i])\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'l2', C=1)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y[train_i])\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y[dev_i], pred)\n",
      "        roc_auc_list_bodies.append(roc_auc_i)\n",
      "        \n",
      "        X_train2 = X_train\n",
      "        X_dev2 = X_dev\n",
      "    \n",
      "    if do_bodies and do_titles:\n",
      "        X_train = scipy.sparse.hstack((X_train1,X_train2),'csr')\n",
      "        X_dev = scipy.sparse.hstack((X_dev1,X_dev2),'csr')\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=1)\n",
      "            model.fit(X_train, y[train_i])\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'l2', C=1)\n",
      "        \n",
      "        model.fit(X_train, y[train_i])\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y[dev_i], pred)\n",
      "        roc_auc_list_both.append(roc_auc_i)\n",
      "    \n",
      "    \n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "    print 'Titles: Mean: %f, Median: %f' %(np.mean(roc_auc_list_titles), np.median(roc_auc_list_titles))\n",
      "    print 'Bodies: Mean: %f, Median: %f' %(np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies))\n",
      "    print 'Both: Mean: %f, Median: %f' %(np.mean(roc_auc_list_both), np.median(roc_auc_list_both))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40/12971\n",
        "('ROC AUC:', 0.60909206027605423)\n",
        "40/12896"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58001388226107331)\n",
        "42/12927"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57166746297181081)\n",
        "47/12873"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58841973066773023)\n",
        "51/12672"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5548219287715086)\n",
        "Titles: Mean: nan, Median: nan\n",
        "Bodies: Mean: 0.580803, Median: 0.580014\n",
        "Both: Mean: nan, Median: nan\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
        "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def timify(feature_df):\n",
      "    req_dt = np.array([dt.datetime.fromtimestamp(timei) for timei in feature_df['unix_timestamp_of_request']])\n",
      "    hour = np.array([dti.hour for dti in req_dt])\n",
      "    dow = np.array([dti.weekday() for dti in req_dt])\n",
      "    month = np.array([dti.month for dti in req_dt])\n",
      "\n",
      "    req_dt_utc = np.array([dt.datetime.fromtimestamp(timei) for timei in feature_df['unix_timestamp_of_request_utc']])\n",
      "    hour_utc = np.array([dti.hour for dti in req_dt_utc])\n",
      "    dow_utc = np.array([dti.weekday() for dti in req_dt_utc])\n",
      "    #hour_utc = np.array([dt.datetime.fromtimestamp(timei).hour  for timei in all_train_df['unix_timestamp_of_request_utc']])\n",
      "    \n",
      "    time_df = pd.DataFrame({'request_datetime':req_dt,\n",
      "                            'request_hour':hour,\n",
      "                            'request_dow':dow,\n",
      "                            'request_month':month})\n",
      "\n",
      "    return time_df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_df = timify(all_train_df)\n",
      "X_time = time_df[['request_dow','request_hour']].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hour = time_df.request_hour\n",
      "hour_pos = hour[all_train_labels]\n",
      "hour_neg = hour[np.logical_not(all_train_labels)]\n",
      "pd.Series(hour_pos).hist(bins=24, alpha=0.2, normed=True)\n",
      "pd.Series(hour_neg).hist(bins=24, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x1a6ec470>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG8FJREFUeJzt3X9wXfV55/H3g4khipOoHTUmAa+ViT3EdMqP4LIuXQUz\nYWvF24V0mi3rSRvMbArMxEma2e4QNjNr/unskp1sWMoUvIu7Jl2mnoVOG+8Wo9106sY7OBA5/gmS\nbQVkbAMSWuJSLCQk/Owf98rP1UW695wj3XuPdD6vGU84555z71ef3j766rnnfK+5OyIisrhd1OoB\niIhI46nYi4gUgIq9iEgBqNiLiBSAir2ISAGo2IuIFEDdYm9m3WbWb2YnzOzeGR7/tJntM7MxM/vX\nac4VEZHmsFrX2ZvZEuAYcAtwBvgJsMnd+yqO+SVgJfAF4Ofu/t2k54qISHPUm9nfAAy4+6C7TwA7\ngdsqD3D3N9y9F5hIe66IiDRHvWJ/OXCqYvt0eV8SczlXRETmUb1iP5e1FLQOg4hITlxc5/EzwIqK\n7RWUZuhJJDrXzPRLQUQkA3e3pMfWm9n3AqvNrNPMlgK3A7tmObb6RROf6+76587WrVtbPoa8/FMW\nykJZ1P6XVs2ZvbtPmtkWoAdYAmx39z4zu7v8+DYzu4zSlTYfAc6b2TeAq9z97ZnOTT3CAhkcHGz1\nEHJDWQRlEZRFdvXaOLj7bmB31b5tFf/9OtPbNTXPFRGR5tMdtDmyefPmVg8hN5RFUBZBWWRX86aq\npgzAzFs9BhFZWPbu3c/oaLpz2tqgq+v6xgyoBcwMT/EBbd02jjTPnj17WL9+fauHkQvKIiiLMJXF\n6Ch0dKQr3CMj+xs0qoVBbRwRkQJQG0dE5s3eH+9l9N10/ZW2pW10retKdU5Pz/5MM/sNG9TGERGZ\ns9F3R+lY1ZHqnJGBkQaNRiqpjZMje/bsafUQckNZBGURlEV2KvYiIgWgYp8juuIiKIugLIKyyE7F\nXkSkAFTsc0T9yKAsgrIIyiI7FXsRkQJQsc8R9SODsgjKIiiL7FTsRUQKQMU+R9SPDMoiKIugLLJT\nsRcRKQAV+xxRPzIoi6AsgrLITmvjiEhLHTl6At5Jt57OkSPHufnmxbOoWTOo2OeI1i0PyiIs9izG\nx5OvTd/bu4e1a9czPn68waNafFTsRWRGWZYrPtJ/hJtX3dygEclcqNjnyGKevaWlLEKrssiyXPH4\nkfEGjaZk7dr1DX3+xUwf0IqIFICKfY7oGuKgLIKyCL29e1o9hAVLxV5EpABU7HNEfeqgLIKyCOrZ\nZ6diLyJSACr2OaLebFAWQVkE9eyzU7EXESkAFfscUW82KIugLIJ69tnppioRmTeDg2fYt68v3Tkv\nn4bPNmhAcoGKfY4s9jVQ0lAWYSFlMTEJ7e1r0p0zcTTxsVNr40h6auOIiBSAin2OLJTZWzMoi6As\ngmb12anYi4gUgIp9juh66qAsgrIIus4+O31AK1IQaden19r0i0vdYm9m3cCDwBLgMXd/YIZjHgI+\nD4wCm939QHn/fcDvAueBI8Cd7t7YBa8XMPVmg7II85VF2vXpG702fRbq2WdXs41jZkuAh4Fu4Cpg\nk5mtqTpmI7DK3VcDdwGPlPd3Ar8PfMbdf4XSL4t/Oc/jFxGRBOrN7G8ABtx9EMDMdgK3AZV3TdwK\nPA7g7s+ZWbuZLQfeAiaANjN7D2gDzszv8BeXhXQ9daMpi9CqLLLcIDX0+kiDRlOi6+yzq1fsLwdO\nVWyfBv5xgmMud/efmtl3gVeAd4Aed//hHMcrIk2S5QapycldDRrN3B3tPwAfTPfLqG1pG13ruho0\nouaqV+w94fPY+3aYfQr4A6AT+HvgSTP7krs/UX3s5s2b6ezsBKC9vZ1rr732wkxm6kqEImyvX78+\nV+PRdn62p8z1+Xqf7QVg7Y1rE233HSxtr7k22fabw8P0HexNfHzfwV5eP/PKhZ9v6mqbqdl79Xbl\nviTHV26PnR9jcHgw1c//N0/+De+Nvdfy//tP1YYdO3YAXKiXaZj77PXczNYB97t7d3n7PuB85Ye0\nZvYosMfdd5a3+4GbgPXAP3X3r5T3/x6wzt2/WvUaXmsMIjI/en7Uk+oD2j/+j9u55Tf/VarXePzR\nB7jjnntTnfPDp57ka1/5D6nOefSxrVyz9sZU5xx6YTf3/OHvpTpnZGCEDZ/dkOqcZjEz3P19E+3Z\n1LvOvhdYbWadZrYUuB2o/jttF/Dl8ouvA866+xBwDFhnZh80MwNuAV5MOrAiqp7FFZmyCMoiTM3U\nJxinvbMj1b8Jf7e1g2+xmm0cd580sy1AD6Wraba7e5+Z3V1+fJu7P21mG81sADgH3Fl+7KCZfZ/S\nL4zzwE+B/9LAn0VERGZR9zp7d98N7K7at61qe8ss534H+M5cBlgkU306URaVlEXQlTjZabkEEZEC\nULHPEfVmg7IIyiJobZzstDaOiLTU0Mgp9h3sSXTsiYFDTFw8ztDIqfoHyzQq9jmi3mwoUhZ79+5n\ntOb6ZB+mp2f/tD1tbdDVdX1Dx9Usk0zQ3pnsktBf7fzchXMkHRV7kRYbHYWOjnSFe2Rkf/2DRCqo\n2OfIHq0Hc4GyCPO1HsyRoye49I03Eh/f6HVusqi8O1fSUbEXKYjxcbgsxVo3eV7nRtJTsc8RzWSD\nsggzzeqPHu1P/TyDL59m5dXzMKAW0qw+OxV7kQWof+A4l7QnX+cG4PRruoKlyFTsc0R96qAswkw9\n+6m1YdJYDFewqGefnW6qEhEpABX7HNFMNiiLoPVggmb12anYi4gUgIp9jmgNlKAsgtaDCVPfdCXp\nqdiLiBSAin2OqE8dlEVQzz6oZ5+dir2ISAGo2OeI+tRBWQT17IN69tmp2IuIFICKfY6oTx2URVDP\nPqhnn52KvYhIAajY54j61EFZBPXsg3r22anYi4gUgIp9jqhPHZRFUM8+qGefnYq9iEgBqNjniPrU\nQVkE9eyDevbZqdiLiBSAin2OqE8dlEVQzz6oZ5+dir2ISAHoO2hzRN+7GoqUxdH+A1yybGTWx0/0\nH2L1p6+Ztm9opJhfHq7voM1OxV6kxcbOj7G8xpeHLzv70fd9ufhi+PJwaS61cXKkKDPZJJRF0Ew2\nKIvsVOxFRApAbZwcKVKfuh5lEdSnDnPJYuj1N9i3ry/VOWNDZ9nw2Q2ZXi9vVOxFpBAmJ4329jWp\nzjn5yr4Gjab56rZxzKzbzPrN7ISZ3TvLMQ+VHz9kZtdV7G83s6fMrM/MXjSzdfM5+MVGM9mgLIJm\n9UFZZFez2JvZEuBhoBu4CthkZmuqjtkIrHL31cBdwCMVD/9n4Gl3XwNcDaT7G0pEROZFvZn9DcCA\nuw+6+wSwE7it6phbgccB3P05oN3MlpvZR4Eud//T8mOT7v738zv8xUXrwQRlEbQeTFAW2dUr9pcD\nlXdvnC7vq3fMFcAngTfM7L+Z2U/N7L+aWdtcBywiIunVK/ae8HlshvMuBj4D/Im7fwY4B3wr3fCK\nRX3qoCyC+tRBWWRX72qcM8CKiu0VlGbutY65orzPgNPu/pPy/qeYpdhv3ryZzs5OANrb27n22msv\n/D/71J/z2tb2Yt1+aeAEK6/+NSDaFFNFbbbtKUmPb9b2m8PD0y6PTHL+m8PDqX+eZv38Lw2cmHYZ\ncCvfL3v27GHHjh0AF+plGuY+++TdzC4GjgGfA14Fngc2uXtfxTEbgS3uvrF8tc2D7r6u/NiPgK+4\n+3Ezux/4oLvfW/UaXmsMRaJry0ORsvij7z18odjPZKZryx9/9AHuuGfGi+NmlfacZrxG2nOmsmjW\n2E4e3se3v7kl1TnNYma4e3VXZVY1Z/buPmlmW4AeYAmw3d37zOzu8uPb3P1pM9toZgOUWjV3VjzF\n14AnzGwp8LOqx0REpEnq3lTl7ruB3VX7tlVtz/irz90PAb86lwEWSVFmskkoi6A+dVAW2WltHBGR\nAtByCTlSpD51PQs1i7179zM6mu6cwZdPs/Lq2R/X2jhBWWSnYi8yj0ZHoaPj+lTnTEw82aDRiAQV\n+xxZiDPZRslDFllm6T94ehdXXT37t07NpN63TmkmG5RFdir2IrPIMksffffJ932rVD361ilpBn1A\nmyNaDyYoi6D1YIKyyE7FXkSkAFTscyQPfeq8UBZBfeqgLLJTz15EZBaDg6/Q07M/8fFtbdDVle5z\nnmZRsc+RhXpteSMoi6Bry0Ozs5h496JUH9KPjCT/xdBsKvYiIrMYGjnFvoM9iY8ff/sUGzZoZi91\naCYblEXQrD40O4tJJlJdSnvy8IkGjmZu9AGtiEgBqNjniK4tD8oi6NryoCyyU7EXESkAFfscUZ86\nKIugnn1QFtmp2IuIFICKfY6oTx2URVCfOiiL7FTsRUQKQMU+R9SnDsoiqE8dlEV2KvYiIgWgYp8j\n6lMHZRHUpw7KIjsVexGRAlCxzxH1qYOyCOpTB2WRnYq9iEgBqNjniPrUQVkE9amDsshOxV5EpABU\n7HNEfeqgLIL61EFZZJeLLy85/rPjqY5v/3A7H/vYxxo0GhGRxScXxf7UO6cSHzv2zhifOv+pRVns\n9b2rYb6z2Lt3P6Oj6c45cuQ4N9/c+q+Y03fQBmWRXS6Kffsvtic+9q2zbzVwJLJYjY6S6oujAcbH\n0/3FKZJn6tnniGb1QVkEzWSDsshOxV5EpABU7HNE15YHZRF0bXlQFtmp2IuIFICKfY6oTx2URVCf\nOiiL7OoWezPrNrN+MzthZvfOcsxD5ccPmdl1VY8tMbMDZvY/52vQIiKSTs1ib2ZLgIeBbuAqYJOZ\nrak6ZiOwyt1XA3cBj1Q9zTeAFwGfr0EvVupTB2UR1KcOyiK7ejP7G4ABdx909wlgJ3Bb1TG3Ao8D\nuPtzQLuZLQcwsyuAjcBjgM3nwEVEJLl6xf5yoPL21tPlfUmP+R7wb4DzcxhjYahPHZRFUJ86KIvs\n6t1Bm7T1Uj1rNzP7TWDY3Q+Y2frUIxNpscHT/ew72JPqnKGR5Et/iDRTvWJ/BlhRsb2C0sy91jFX\nlPf9NnBruad/KfARM/u+u3+5+kW2/sFWPrHiEwAs+8gyrvzlK1l7Y+k3eO+zpR7d1PbB5w9y6uJT\nfHr1p4Ho7U7NBBfydmWfOg/jaeX21L6ZHj98+BhXXll6Pxw6VHp/XHNN7e2LLvoIN998Pb29pedb\nu7b0fLW2JxjntbODQMwop3rGs20PD5+Ztn5LveP7Dvby5vDwhZ95psdPDhyj+4tfmvZ4reNbuf3m\n8PC8//yV28889QQrV13ZtJ8/7c/z0sCJaes6zXd92LFjBwCdnZ2kZe6zT97N7GLgGPA54FXgeWCT\nu/dVHLMR2OLuG81sHfCgu6+rep6bgD90938+w2t475nkH7q8dfYtPv6Bj18o9ouJFkILtbLo6dmf\nep2bZ575c7q7N6U6548f+xa3fPFfpDrn8Ucf4I57ZrxoLfM5My3+1YjXmevxzThnKos8jg3g5OF9\nfPubW1K9RlZmhrsn/iy05sze3SfNbAvQAywBtrt7n5ndXX58m7s/bWYbzWwAOAfcOdvTJR1UUanQ\nB2UR1KcOyiK7uqteuvtuYHfVvm1V2zV/lbn73wF/l2WAIiIyd7lY4lhK1MYJ853FQv6wVWu4B2WR\nnYq9FMIE47R3dqQ6Z5KJBo1GpPm0Nk6OaFYflEXQTDYoi+xU7EVECkDFPke0HkxQFkHrwQRlkZ2K\nvYhIAeTiA9paN3bN5diFRn3qoCyC+tRBWWSXi2L/4x8fS3zs6NvnuPry84vyDloRkUbJRbFvb09e\nuN8dP8PhF45gS99L9RptS9voWteVdmhNpevsg7IIurY8KIvsclHs0xo/P0bHqnTXTI8MjDRoNCIi\n+acPaHNEM9mgLIJmskFZZKdiLyJSACr2OaJry4OyCLq2PCiL7FTsRUQKQMU+R9SnDsoiqE8dlEV2\nKvYiIgWgYp8j6lMHZRHUpw7KIjsVexGRAlCxzxH1qYOyCOpTB2WRnYq9iEgBqNjniPrUQVkE9amD\nsshOxV5EpABU7HNEfeqgLIL61EFZZKdiLyJSACr2OaI+dVAWQX3qoCyyU7EXESkAFfscUZ86KIug\nPnVQFtmp2IuIFICKfY6oTx2URVCfOiiL7FTsRUQKQMU+R9SnDsoiqE8dlEV2KvYiIgVwcasH0CxH\nXzya+py2pW10retqwGhmtmfPHs1oy5RF6DvYqxltmbLIrjDFfuy9MTpWdaQ6Z2RgpEGjERFprsIU\n+4VAM9lQK4uj/Qe4ZFm6X8RDI6fmOKLW0Uw2KIvsVOxlwRk7P8byznR/pU0y0aDRiCwMiT6gNbNu\nM+s3sxNmdu8sxzxUfvyQmV1X3rfCzP7WzF4ws6Nm9vX5HPxio2vLg7IIurY8KIvs6hZ7M1sCPAx0\nA1cBm8xsTdUxG4FV7r4auAt4pPzQBPBNd/9lYB3w1epzRUSk8ZLM7G8ABtx90N0ngJ3AbVXH3Ao8\nDuDuzwHtZrbc3V9394Pl/W8DfcAn5m30i4x69kFZBPWpg7LILkmxvxyo/HTrdHlfvWOuqDzAzDqB\n64Dn0g5SRETmJskHtJ7wuWy288xsGfAU8I3yDH+abQ9s5ZcuK0342z60jJWrrrzwG3yqRze1PfDi\nEcZ+Pnjh3N5nS4+vvXFtze20x09tT/WOp2aajdyu7FM34/Uasf3QQ9sYG4Nrrinld+hQKc9a25de\nCl//+t3Tnq86k8rXe2ngBCuv/jXg/e+P2banJD0+6/abw8PTrgVPcv6bw8M1x3dy4BjdX/xSS36e\nPPz8ldvPPPUEK1dd2bSfP+3P89LAiWn3iMx3fdixYwcAnZ2dpGXutWu5ma0D7nf37vL2fcB5d3+g\n4phHgT3uvrO83Q/c5O5DZvYB4H8Bu939wRme3//sh8k/dBkeOsO5oRfYePtvJD4H4Jm/fIbu3+pO\ndc7IwAgbPrsh1TlzsRhuJOrp2U9Hx/WpzhkZ2c+GDdPPqZXFH33v4QvFPqnHH32AO+6Z8dqC3J8z\n041EzRhbXn7+SlNZ5HFsACcP7+Pb39yS6jWyMjPcvXqSPaskbZxeYLWZdZrZUuB2YFfVMbuAL5cH\nsA44Wy70BmwHXpyp0Mt0C73QzydlEdSnDsoiu7ptHHefNLMtQA+wBNju7n1mdnf58W3u/rSZbTSz\nAeAccGf59F8Hfhc4bGYHyvvuc/dn5jLolwfPsG9fX6pzBgfPzOUlRUQWtEQ3Vbn7bmB31b5tVdvv\n+9vF3f8vDVhsbWLCaG9PdwXnxOSz8z2MebcY2jjzRVkErQcTlEV2hbmDduj1N1L/NTA2dLapPXsR\nkUYpTLGfnEz/18DJV/Y1aDQzK+pM9mj/Afhg1Vo3F0HPj3pmPH7w1EDqD2gXMs1kg7LIrjDFXvKr\n/6XjXHLFpYmPP/3aqw0cjcjipC8vyZGirgcz8e5FtLevmfbvtcFz79s39W9ystUjbi6tBxOURXYq\n9iIiBaBinyNF7dnPRL3ZoCyCsshOxV5EpABU7HOkqD37mag3G5RFUBbZqdiLiBSAin2OqGcf1JsN\nyiIoi+xU7EVECkDFPkfUsw/qzQZlEZRFdrqDtobBV3426y37s2lb2kbXuq4GjSj/jvYf4JJlI/UP\nrDA0cqr+QSIyJyr2NUwwTseqjlTnjAykK3SVFkPPfuz8GMs702U2ycT79qk3G5RFUBbZqY0jIlIA\nKvY5op59UG82KIugLLJTG2ee/eCvn6F3/4nEx3+07VK23P2VBo5IRETFft6Nvjueaq31k4djzfzF\n0LOfL+rNBmURlEV2KvYtpit+RKQZVOxryPJVhkOvp7sap/KKn95ne1l7Y/2Zy1yu+Fko9F2jQVkE\nZZGdin0NWb7KcHJyV4NGIyKSnYp9jiSZ1QMcffFo6udeaK0fzd6CsgjKIjsV+wVo7L2xpt7sJSIL\nn4p9jiTt2TfL3r37GR1Nd87gy6dZefXcX1u92aAsgrLITsVeZjU6Ch0d16c6Z2LiyQaNRkTmQsU+\nR/I0q4fWLmqm2VtQFkFZZKdiXxBZPtTtP3mUm/7ZdanOmWlRMxFpPRX7HGlkzz7Lh7oT/m5DxpKE\nerNBWQRlkZ0WQhMRKQAV+xzJW8++lTR7C8oiKIvs1MYpiMHBMw1f+kFE8kvFPkca2bOfmGRBLf2g\n3mxQFkFZZKc2johIAWhm32LTVta0DyVqtex77iAfvWxlytdZWC0Zzd6CsgjKIjsV+xbLsrLm+Du7\nFlRLRkRar24bx8y6zazfzE6Y2b2zHPNQ+fFDZnZdmnMl6Ps1g7IIyiIoi+xqFnszWwI8DHQDVwGb\nzGxN1TEbgVXuvhq4C3gk6bky3cmBY60eQm4oi6AsgrLIrt7M/gZgwN0H3X0C2AncVnXMrcDjAO7+\nHNBuZpclPFcqjJ57u9VDyA1lEZRFUBbZ1Sv2lwOVK1udLu9LcswnEpwrIiJNUO8DWk/4PDaXQZw8\nfjjxseNjY1xkc3q53Hrj9VdbPYTcUBZBWQRlkZ25z17PzWwdcL+7d5e37wPOu/sDFcc8Cuxx953l\n7X7gJuCT9c4t70/6C0VERCq4e+KZb72ZfS+w2sw6gVeB24FNVcfsArYAO8u/HM66+5CZ/b8E56Ya\nrIiIZFOz2Lv7pJltAXqAJcB2d+8zs7vLj29z96fNbKOZDQDngDtrndvIH0ZERGZWs40jIiKLQ0vX\nxtFNV8HMBs3ssJkdMLPnWz2eZjKzPzWzITM7UrHvF83s/5jZcTP732bW3soxNsssWdxvZqfL740D\nZtbdyjE2i5mtMLO/NbMXzOyomX29vL9w740aWSR+b7RsZl++6eoYcAtwBvgJsKmorR4zexm43t3f\nbPVYms3MuoC3ge+7+6+U930HGHH375QnAr/g7t9q5TibYZYstgL/4O7/qaWDa7Ly/TqXuftBM1sG\n7Ae+QKlVXKj3Ro0sfoeE741Wzux109X7FfLDanffC/y8aveFm/XK//uFpg6qRWbJAgr43nD31939\nYPm/3wb6KN2rU7j3Ro0sIOF7o5XFPskNW0XiwA/NrNfMfr/Vg8mB5e4+VP7vIWB5KweTA18rrz21\nvQhti2rlq/quA56j4O+Niix+XN6V6L3RymKvT4an+3V3vw74PPDV8p/zAnip11jk98sjlO5buRZ4\nDfhua4fTXOW2xV8A33D3f6h8rGjvjXIWT1HK4m1SvDdaWezPACsqtldQmt0Xkru/Vv7fN4C/pNTm\nKrKhcp8SM/s4MNzi8bSMuw97GfAYBXpvmNkHKBX6P3P3vyrvLuR7oyKL/z6VRZr3RiuL/YUbtsxs\nKaWbrgq56LqZtZnZh8v//SHgN4Ajtc9a9HYBd5T/+w7gr2ocu6iVC9qU36Ig7w0zM2A78KK7P1jx\nUOHeG7Nlkea90dLr7M3s88CDxE1X/75lg2khM/skpdk8lG50e6JIWZjZn1NaYqODUg/23wE/AP4H\n8I+AQeB33P1sq8bYLDNksRVYT+nPdAdeBu6u6FkvWmb2T4AfAYeJVs19wPMU7L0xSxb/ltKqBIne\nG7qpSkSkAPSF4yIiBaBiLyJSACr2IiIFoGIvIlIAKvYiIgWgYi8iUgAq9iIiBaBiLyJSAP8f9mEK\n1ikCIjsAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3976d30>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dow = time_df.request_dow\n",
      "dow_pos = dow[all_train_labels]\n",
      "dow_neg = dow[np.logical_not(all_train_labels)]\n",
      "pd.Series(dow_pos).hist(bins=7, alpha=0.2, normed=True)\n",
      "pd.Series(dow_neg).hist(bins=7, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x1c238748>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1a563a58>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "#X_time = np.array([hour,dow]).T\n",
      "\n",
      "scorer = make_scorer(roc_auc_score)\n",
      "scores = cross_val_score(etc, X_time, all_train_labels, cv=KFold(n_all, n_folds = 10), scoring=scorer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.median(scores)\n",
      "print np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.524119492031\n",
        "0.51968111378\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine title and text approaches\n",
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "time_df = timify(all_train_df)\n",
      "times = time_df[['request_dow','request_hour','request_month']].values\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 2000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "do_acts = True\n",
      "do_times = True\n",
      "\n",
      "model = lsvc\n",
      "scaler = StandardScaler()\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_acts = []\n",
      "roc_auc_list_times = []\n",
      "roc_auc_list_all = []\n",
      "    \n",
      "for train_i, dev_i in KFold(n_all, n_folds = 10):\n",
      "    X_dict = {}\n",
      "    #X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    #X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    #X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['times'] = {'train' : times[train_i], 'dev' : times[dev_i], 'model' : etc}\n",
      "\n",
      "    \n",
      "    X_all = {'train':(), 'dev':()}\n",
      "    \n",
      "    y_train = y[train_i]\n",
      "    y_dev = y[dev_i]\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    if do_titles:\n",
      "        X_train = tv.fit_transform(X_dict['titles']['train'])\n",
      "        X_dev = tv.transform(X_dict['titles']['dev'])\n",
      "        model = X_dict['titles']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.003)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=5)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_titles.append(roc_auc_i)\n",
      "        \n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    if do_bodies:\n",
      "        X_train = tv.fit_transform(X_dict['bodies']['train'])\n",
      "        X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "        model = X_dict['bodies']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.0025)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=1)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_bodies.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_acts:\n",
      "        X_train = scaler.fit_transform(X_dict['acts']['train'])\n",
      "        X_dev = scaler.transform(X_dict['acts']['dev'])\n",
      "        model = X_dict['acts']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_acts.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_times:\n",
      "        X_train = X_dict['times']['train']\n",
      "        X_dev = X_dict['times']['dev']\n",
      "        model = model = X_dict['times']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_times.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    if do_bodies + do_titles + do_acts > 1:\n",
      "        #X_train = scaler.fit_transform(scipy.sparse.hstack(X_all['train'],'csr').toarray())\n",
      "        X_train = scipy.sparse.hstack(X_all['train'],'csr').toarray()\n",
      "        #X_dev = scaler.transform(scipy.sparse.hstack( X_all['dev'],'csr').toarray())\n",
      "        X_dev = scipy.sparse.hstack( X_all['dev'],'csr').toarray()\n",
      "\n",
      "        \n",
      "        \n",
      "        if prune_features:\n",
      "            model = lsvc\n",
      "            model.set_params(loss = 'hinge', C=5)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "          \n",
      "        model = svc\n",
      "        model.set_params(C=1)\n",
      "        \n",
      "        model = etc\n",
      "        \n",
      "        \n",
      "        model.fit(X_train,y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_all.append(roc_auc_i)\n",
      "    \n",
      "    \n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "    print 'Titles: Mean: %f, Median: %f' %(np.mean(roc_auc_list_titles), np.median(roc_auc_list_titles))\n",
      "    print 'Bodies: Mean: %f, Median: %f' %(np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies))\n",
      "    print 'Activities: Mean: %f, Median: %f' %(np.mean(roc_auc_list_acts), np.median(roc_auc_list_acts))\n",
      "    print 'All: Mean: %f, Median: %f' %(np.mean(roc_auc_list_all), np.median(roc_auc_list_all))\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "51/13569\n",
        "60/63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.62309833024118744)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52/13703"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62/64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.61241883116883111)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51/13730"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61/63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.66736214605067057)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50/13708"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58/62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57145175951175164)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53/13593"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62/65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57417513822008204)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58/13761"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "69/70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.60561399217221135)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51/13835"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60/63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.62541254125412538)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54/13706"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66/66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.6423213021939137)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53/13770"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64/65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.60883879954413067)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55/13742"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64/67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.56586409844836805)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Titles: Mean: nan, Median: nan\n",
        "Bodies: Mean: 0.575401, Median: 0.568078\n",
        "Activities: Mean: 0.549092, Median: 0.560776\n",
        "All: Mean: 0.609656, Median: 0.610629\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
        "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine numeric and text approaches\n",
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "time_df = timify(all_train_df)\n",
      "times = time_df[['request_dow','request_hour']].values\n",
      "time_df_submit = timify(submit_df)\n",
      "times_submit = time_df_submit[['request_dow','request_hour']].values\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "do_acts = True\n",
      "\n",
      "model = lsvc\n",
      "scaler = StandardScaler()\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_acts = []\n",
      "roc_auc_list_times = []\n",
      "roc_auc_list_all = []\n",
      "    \n",
      "y_train = y\n",
      "    \n",
      "X_dict = {}\n",
      "X_dict['titles'] = {'train' : titles, 'dev' : submit_df['request_title'].values, 'model' : lsvc}\n",
      "X_dict['bodies'] = {'train' : bodies, 'dev' : submit_df['request_text_edit_aware'].values, 'model' : lsvc}\n",
      "X_dict['acts'] = {'train' : acts, 'dev' : submit_df[activity_var].values, 'model' : svc}\n",
      "X_dict['times'] = {'train' : times, 'dev' : times_submit, 'model' : etc}\n",
      "\n",
      "X_all = {'train':(), 'dev':()}\n",
      "\n",
      "# get initial training / dev data for text model\n",
      "if do_titles:\n",
      "    X_train = tv.fit_transform(X_dict['titles']['train'])\n",
      "    X_dev = tv.transform(X_dict['titles']['dev'])\n",
      "    model = X_dict['titles']['model']\n",
      "\n",
      "    if prune_features:\n",
      "        model.set_params(loss = 'hinge', C=.005)\n",
      "        model.fit(X_train, y_train)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "        print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "        X_train = X_train[:,sig_coef]\n",
      "        X_dev = X_dev[:,sig_coef]\n",
      "        model.set_params(loss = 'squared_hinge', C=5)\n",
      "\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "\n",
      "if do_bodies:\n",
      "    X_train = tv.fit_transform(X_dict['bodies']['train'])\n",
      "    X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "    model = X_dict['bodies']['model']\n",
      "\n",
      "    if prune_features:\n",
      "        model.set_params(loss = 'hinge', C=.0025)\n",
      "        model.fit(X_train, y_train)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "        print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "        X_train = X_train[:,sig_coef]\n",
      "        X_dev = X_dev[:,sig_coef]\n",
      "        model.set_params(loss = 'squared_hinge', C=1)\n",
      "\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "\n",
      "if do_acts:\n",
      "    X_train = scaler.fit_transform(X_dict['acts']['train'])\n",
      "    X_dev = scaler.transform(X_dict['acts']['dev'])\n",
      "    model = X_dict['acts']['model']\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    \n",
      "if do_times:\n",
      "    X_train = X_dict['times']['train']\n",
      "    X_dev = X_dict['times']['dev']\n",
      "    model = X_dict['times']['model']\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    #roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "    #roc_auc_list_times.append(roc_auc_i)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "\n",
      "\n",
      "if do_bodies + do_titles + do_acts > 1:\n",
      "    #X_train = scaler.fit_transform(scipy.sparse.hstack(X_all['train'],'csr').toarray())\n",
      "    X_train = scipy.sparse.hstack(X_all['train'],'csr').toarray()\n",
      "    #X_dev = scaler.transform(scipy.sparse.hstack( X_all['dev'],'csr').toarray())\n",
      "    X_dev = scipy.sparse.hstack( X_all['dev'],'csr').toarray()\n",
      "\n",
      "    #if prune_features:\n",
      "    if True:\n",
      "        model = lsvc\n",
      "        model.set_params(loss = 'hinge', C=5)\n",
      "        model.fit(X_train, y_train)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "        print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "        X_train = X_train[:,sig_coef]\n",
      "        X_dev = X_dev[:,sig_coef]\n",
      "\n",
      "    model = svc\n",
      "    model.set_params(C=1)\n",
      "\n",
      "    model = etc\n",
      "\n",
      "\n",
      "    model.fit(X_train,y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "66/14509\n",
        "73/77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_submission_csv(pred, submit_df['request_id'].values, 'request_text_and_activity_and_time_ETClassifier')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class SnowballStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = SnowballStemmer('english')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "class PorterStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine title, body, time, numeric\n",
      "# Add text lemmazation & stemming\n",
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "time_df = timify(all_train_df)\n",
      "times = time_df[['request_dow','request_hour','request_month']].values\n",
      "\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "text_derivs = np.hstack((title_len, body_len))\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False, tokenizer=SnowballStemTokenizer())\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 2000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "balance = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "do_acts = True\n",
      "do_times = True\n",
      "do_text_derivs = True\n",
      "\n",
      "model = lsvc\n",
      "scaler = StandardScaler()\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_acts = []\n",
      "roc_auc_list_times = []\n",
      "roc_auc_list_text_derivs = []\n",
      "roc_auc_list_all = []\n",
      "    \n",
      "for train_i, dev_i in KFold(n_all, n_folds = 10):\n",
      "    if balance:\n",
      "        train_i_orig = train_i\n",
      "        y_train = y[train_i_orig]\n",
      "        train_i = train_i_orig[balance_samples(y_train)]\n",
      "    \n",
      "    X_dict = {}\n",
      "    #X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    #X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    #X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['times'] = {'train' : times[train_i], 'dev' : times[dev_i], 'model' : etc}\n",
      "    X_dict['text_derivs'] = {'train' : text_derivs[train_i], 'dev' : text_derivs[dev_i], 'model' : etc}\n",
      "    \n",
      "    X_all = {'train':(), 'dev':()}\n",
      "    \n",
      "    y_train = y[train_i]\n",
      "    y_dev = y[dev_i]\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    if do_titles:\n",
      "        X_train = tv.fit_transform(X_dict['titles']['train'])\n",
      "        X_dev = tv.transform(X_dict['titles']['dev'])\n",
      "        model = X_dict['titles']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.003)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=5)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_titles.append(roc_auc_i)\n",
      "        \n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    if do_bodies:\n",
      "        X_train = tv.fit_transform(X_dict['bodies']['train'])\n",
      "        X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "        model = X_dict['bodies']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.0025)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=1)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_bodies.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_acts:\n",
      "        X_train = scaler.fit_transform(X_dict['acts']['train'])\n",
      "        X_dev = scaler.transform(X_dict['acts']['dev'])\n",
      "        model = X_dict['acts']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_acts.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_times:\n",
      "        X_train = X_dict['times']['train']\n",
      "        X_dev = X_dict['times']['dev']\n",
      "        model = model = X_dict['times']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_times.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_text_derivs:\n",
      "        X_train = X_dict['text_derivs']['train']\n",
      "        X_dev = X_dict['text_derivs']['dev']\n",
      "        model = model = X_dict['text_derivs']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_text_derivs.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    \n",
      "    if do_bodies + do_titles + do_acts + do_times + do_text_derivs > 1:\n",
      "        #X_train = scaler.fit_transform(scipy.sparse.hstack(X_all['train'],'csr').toarray())\n",
      "        X_train = scipy.sparse.hstack(X_all['train'],'csr').toarray()\n",
      "        #X_train = np.hstack(X_all['train'])\n",
      "        #X_dev = scaler.transform(scipy.sparse.hstack( X_all['dev'],'csr').toarray())\n",
      "        X_dev = scipy.sparse.hstack( X_all['dev'],'csr').toarray()\n",
      "        #X_dev = np.hstack(X_all['dev'])\n",
      "\n",
      "        \n",
      "        \n",
      "        #if prune_features:\n",
      "        if False:\n",
      "            model = lsvc\n",
      "            model.set_params(loss = 'hinge', C=5)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "          \n",
      "        model = svc\n",
      "        model.set_params(C=1)\n",
      "        \n",
      "        model = gbc\n",
      "        \n",
      "        \n",
      "        model.fit(X_train,y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_all.append(roc_auc_i)\n",
      "    \n",
      "    \n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "    print 'Titles: Mean: %f, Median: %f' %(np.mean(roc_auc_list_titles), np.median(roc_auc_list_titles))\n",
      "    print 'Bodies: Mean: %f, Median: %f' %(np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies))\n",
      "    print 'Activities: Mean: %f, Median: %f' %(np.mean(roc_auc_list_acts), np.median(roc_auc_list_acts))\n",
      "    print 'Text Derivs: Mean: %f, Median: %f' %(np.mean(roc_auc_list_acts), np.median(roc_auc_list_text_derivs))\n",
      "    print 'All: Mean: %f, Median: %f' %(np.mean(roc_auc_list_all), np.median(roc_auc_list_all))\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "192/9965\n",
        "('ROC AUC:', 0.58622256849759768)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "201/9953"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.62816944959802101)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "196/9878"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.63203935659357269)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "205/9931"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.64175776754890679)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "193/9900"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57243589743589751)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "193/9907"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58075851589211092)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "199/9911"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.56728381124407312)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "199/9851"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.59975546605293439)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "211/9832"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58622256849759768)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "202/9757"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.54148936170212758)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Titles: Mean: nan, Median: nan\n",
        "Bodies: Mean: 0.578508, Median: 0.581978\n",
        "Activities: Mean: 0.550280, Median: 0.546909\n",
        "Text Derivs: Mean: 0.550280, Median: 0.567664\n",
        "All: Mean: 0.593613, Median: 0.586223\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.choice(all_train_df.request_title.values,20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 147,
       "text": [
        "array([u'[request] just a hungry guy',\n",
        "       u'[OFFER, REQUEST] Might be the wrong place...',\n",
        "       u'[Request] Looking to give my kids a hot meal.',\n",
        "       u'[Request] Newborn baby squalls and my roommate and I are tired. Please help?',\n",
        "       u'[Request] Medical bills and stress piling up, could use a nice pizza for my kids',\n",
        "       u'[Request] For my brother and I (south FL)',\n",
        "       u'Shana Tova [REQUEST] A pie for me and my nana? \\nImma 19 y.o. student. Does this work? Just started \\nschool and out of pocket money and rent . ne help? \\nHappy Jewish new Year',\n",
        "       u'[request] need some assistance',\n",
        "       u'[REQUEST] Sydney student going through rough patch.',\n",
        "       u'[request] broke and stressed would love a delicious pizza',\n",
        "       u'[REQUEST] PHX,AZ US Two starving awesome musicians looking to eat some food tonight!',\n",
        "       u\"[Request] Hays, KS: Do a bro a solid, it's rural out here\",\n",
        "       u\"[Request] Broke post-grad between paychecks, struggling to make ends meet. Can't really afford groceries this week, so I haven't been eating much. Would very much love a delicious pizza to brighten my day and fill my belly. (Cambridge, MA)\",\n",
        "       u'[Request] Cincinnati, OH need piksa plox',\n",
        "       u'[Request] Binge watching breaking bad in underwear, show is too good to leave couch, need pizza',\n",
        "       u'[REQUEST] My wife and I just moved to Austin, TX, and an unfortunate mistake have left us with nothing but Ramen until tomorrow afternoon. A pizza would be greatly appreciated!',\n",
        "       u'[Request] Edinburgh, Scotland. Will record song of your choice on acoustic guitar + vocals!',\n",
        "       u'[Request]  The University of Southern Indiana is on spring break, all restaurants on campus are closed. I spent last paycheck on final payment of tuition. Can you help a brother out?',\n",
        "       u'[Request] Dealing with a tough break-up',\n",
        "       u\"[request] california, getting ready for foster kids \\nand don't have time to get food and dont have to \\nextra money until next paycheck.\"], dtype=object)"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "X_text_deriv_train = np.hstack((title_len, body_len))\n",
      "\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 200,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "svc = SVC()\n",
      "\n",
      "test_kfolds(X_text_deriv_train, all_train_labels, KFold(n_all, n_folds = 2), gbc, balance=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.53020489374312074)\n",
        "('ROC AUC:', 0.50816010428045955)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.519182, Median: 0.519182\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[0.53020489374312074, 0.50816010428045955]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "keywords = {\n",
      "    'sad_food': ['hungry', 'starving', 'no food', 'grocer', 'eaten', 'hunger', 'ramen', 'empty', 'fridge', 'refrig'],\n",
      "    'money': ['broke', 'paid', 'money', 'unemployed', 'lost', 'job', 'bill', 'wage', 'work', 'payday', 'paycheck', 'funds', 'cash', 'bank', 'laid off', 'poor', 'payroll'],\n",
      "    'sad': ['worst', 'awful', 'sick', 'problem', 'catch a break', 'cheer', 'hospital', 'bad', 'shitty', 'stress', 'luck', ':(', 'rough', 'tough'],\n",
      "    'military': ['miliatary', 'veteran'],\n",
      "    'happy': ['celebrate', 'birthday', 'party', 'new year', 'bday', 'engage', 'annivers'],\n",
      "    'nice': ['please', 'help', 'thank you', ':)'],\n",
      "    'honest': ['sob story', 'honest', 'just want', 'just because'],\n",
      "    'parent': ['family', 'kids', 'parent', 'mom', 'mommy', 'mother', 'dad', 'father', 'baby', 'boy', 'girl'],\n",
      "    'relationship': ['husband', 'wife', 'girlfriend', 'boyfriend', 'fianc'],\n",
      "    'test': ['study', 'test', 'final', 'midterm']\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_tag_words(keywords, text):\n",
      "    word_dict = {}\n",
      "    tag_dict = {}\n",
      "\n",
      "    for tag, words in keywords.iteritems():\n",
      "\n",
      "        tag_count = None\n",
      "\n",
      "        for word in words:\n",
      "            has_word = np.array([(1 if word in t else 0) for t in text])\n",
      "            word_dict[word] = has_word\n",
      "\n",
      "            if tag_count is None:\n",
      "                tag_count = has_word\n",
      "            else:\n",
      "                tag_count = tag_count +  has_word\n",
      "\n",
      "        tag_dict[tag] = tag_count\n",
      "\n",
      "    return (tag_dict, word_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_unicode = np.array([x.lower() for x in all_train_df.request_title.values])\n",
      "body_unicode = np.array([x.lower() for x in all_train_df.request_text_edit_aware.values])\n",
      "\n",
      "title_tag_dict, title_word_dict = find_tag_words(keywords, title_unicode)\n",
      "body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "times = time_df[['request_dow','request_hour','request_month']].values\n",
      "\n",
      "X_tag = np.hstack((pd.DataFrame(body_tag_dict).values, pd.DataFrame(title_tag_dict).values, pd.DataFrame(title_word_dict).values, X_text_deriv_train, times, acts))\n",
      "\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 2,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=5)\n",
      "\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "test_kfolds(X_tag, all_train_labels, KFold(n_all, n_folds = 10), gbc, balance=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.57884972170686455)\n",
        "('ROC AUC:', 0.56662608225108235)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.61384335154826963)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.6007985975847292)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5700196183342251)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.607937866927593)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.66996699669967008)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.61847133757961792)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.62675699632771942)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.56916354556803994)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.602243, Median: 0.604368\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "[0.57884972170686455,\n",
        " 0.56662608225108235,\n",
        " 0.61384335154826963,\n",
        " 0.6007985975847292,\n",
        " 0.5700196183342251,\n",
        " 0.607937866927593,\n",
        " 0.66996699669967008,\n",
        " 0.61847133757961792,\n",
        " 0.62675699632771942,\n",
        " 0.56916354556803994]"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Body PCA\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "\n",
      "\n",
      "\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "\n",
      "text_derivs = np.hstack((title_len, body_len))\n",
      "acts = all_train_df[activity_var].values\n",
      "times = time_df[['request_dow','request_hour','request_month']].values\n",
      "\n",
      "title_unicode = np.array([x.lower() for x in all_train_df.request_title.values])\n",
      "body_unicode = np.array([x.lower() for x in all_train_df.request_text_edit_aware.values])\n",
      "\n",
      "title_tag_dict, title_word_dict = find_tag_words(keywords, title_unicode)\n",
      "body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
      "\n",
      "body_tags = pd.DataFrame(body_tag_dict).values\n",
      "title_tags = pd.DataFrame(title_tag_dict).values\n",
      "title_words = pd.DataFrame(title_word_dict).values\n",
      "\n",
      "X_other = np.hstack((text_derivs, acts, times, body_tags, title_tags, title_words))\n",
      "\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=True, tokenizer=SnowballStemTokenizer())\n",
      "\n",
      "gbc = GradientBoostingClassifier(n_estimators = 20,\n",
      "                           max_depth = 4,\n",
      "                           min_samples_split=10)\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 2000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "balance = True\n",
      "\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_all = []\n",
      "\n",
      "model = gbc\n",
      "\n",
      "for train_i, dev_i in KFold(n_all, n_folds = 10):\n",
      "    \n",
      "    if balance:\n",
      "        train_i_orig = train_i\n",
      "        y_train = y[train_i_orig]\n",
      "        train_i = train_i_orig[balance_samples(y_train)]\n",
      "    \n",
      "    y_train = y[train_i]\n",
      "    y_dev = y[dev_i]\n",
      "    \n",
      "    X_train = tv.fit_transform(bodies[train_i]).toarray()\n",
      "    X_dev = tv.transform(bodies[dev_i]).toarray()\n",
      "\n",
      "    pca = RandomizedPCA(n_components=5)\n",
      "\n",
      "    X_train = pca.fit_transform(X_train)\n",
      "    \n",
      "    X_dev = pca.transform(X_dev)\n",
      "        \n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "    roc_auc_list_bodies.append(roc_auc_i)\n",
      "    \n",
      "    print roc_auc_i\n",
      "    \n",
      "    X_train = np.hstack((X_train, X_other[train_i]))\n",
      "    X_dev = np.hstack((X_dev, X_other[dev_i]))\n",
      "    \n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "    roc_auc_list_all.append(roc_auc_i)\n",
      "\n",
      "    print roc_auc_i\n",
      "\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.568058132344\n",
        "0.582220160792"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.600852272727"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.598214285714"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.638118893857"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.665722801788"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.552915205817"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.592585378522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.547922240057"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.554877831282"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.532901174168"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.642000978474"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.561056105611"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.635313531353"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.586730360934"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.62169143666"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.551475243763"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.611244776497"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.551221687177"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.596522204387"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies)\n",
      "print np.mean(roc_auc_list_all), np.median(roc_auc_list_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.569125131646 0.556985655714\n",
        "0.610039338547 0.604729531106\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Body PCA\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1), lowercase=False, tokenizer=SnowballStemTokenizer())\n",
      "\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "\n",
      "X_train = tv.fit_transform(bodies)\n",
      "#X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "#model = X_dict['bodies']['model']\n",
      "\n",
      "pca = RandomizedPCA(n_components=10)\n",
      "\n",
      "X_train = pca.fit_transform(X_train.toarray())\n",
      "print pca.explained_variance_ratio_\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.02139223  0.01124761  0.00751729  0.00688017  0.00603291  0.00543915\n",
        "  0.00535426  0.00501908  0.00458796  0.00455722]\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "bottom = np.argsort(pca.components_[0])[:20]\n",
      "top = np.argsort(pca.components_[0])[-20:]\n",
      "\n",
      "print np.array(tv.get_feature_names())[bottom]\n",
      "print np.array(tv.get_feature_names())[top]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'!' u'*' u'http' u':' u'love' u'?' u'&' u';' u'titl' u'op' u'crave' u'['\n",
        " u']' u'lt' u'appreci' u'hungri' u'pie' u'yall' u'deliv' u'pizza']\n",
        "[u'do' u'get' u'in' u'this' u'that' u'but' u'of' u\"'m\" u'it' u'have' u\"n't\"\n",
        " u'for' u'the' u'a' u'and' u'my' u'to' u'.' u',' u'i']\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}