{
 "metadata": {
  "name": "",
  "signature": "sha256:d114e8857bcd5946aa06b369c865d95d4ab21cca72a174a41165a31a77c5eb2b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Playing with text analysis seen in all messages submitted. \n",
      "# Right now I have left out the title of each message and am just focusing on the message itself. \n",
      "# My thought is that the title of the post would be a moot factor in what determines a message to be selected for a pizza. \n",
      "# This clearly might be a wrong assumption, so later research should include finding out if the title has an impact on accuracy."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I will be using similar code to that of ML Project #2. \n",
      "\n",
      "# Bag-of-Words Analysis\n",
      "\n",
      "# Loading relevant packages. \n",
      "# Standard libaries.\n",
      "import sys\n",
      "import re\n",
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import random as rand\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "# SK-learn libraries for feature extraction from text.\n",
      "from sklearn.feature_extraction.text import *\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard load and submission generation functions from @Ross\n",
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = 'C:/Users/Sarah/Documents/GitHub/brokenBNZ/predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "\n",
      "# Make submission csv\n",
      "#submit_id = [x['request_id'] for x in submit_dict_list]\n",
      "#base_predict = lr_base.predict(np.array([[1]]*n_submit))\n",
      "#make_submission_csv(base_predict, submit_id, 'baseline')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uploading the raw json data.\n",
      "all_train_data = load_json_file('C:/Users/Sarah/Anaconda/Scripts/train.json')\n",
      "test_data =  load_json_file('C:/Users/Sarah/Anaconda/Scripts/test.json')\n",
      "\n",
      "# Randomize the data\n",
      "rand.shuffle(all_train_data)\n",
      "\n",
      "all_train_data_df = pd.DataFrame(all_train_data)\n",
      "test_data_df = pd.DataFrame(test_data)\n",
      "\n",
      "\n",
      "# Message Data from train and test sets\n",
      "all_train_message_data = all_train_data_df[\"request_text_edit_aware\"]\n",
      "test_message_data = test_data_df[\"request_text_edit_aware\"]\n",
      "\n",
      "# Generate the training labels:\n",
      "all_train_labels = np.array(all_train_data_df[\"requester_received_pizza\"])\n",
      "\n",
      "\n",
      "# Check in with the training and test data shapes:\n",
      "print \"Overall training data shape is: \" + str(all_train_data_df.shape)\n",
      "print \"Test data shape is: \" + str(test_data_df.shape)\n",
      "\n",
      "n_train_all = len(all_train_data)\n",
      "n_test = len(test_data)\n",
      "\n",
      "# === Randomize training data and split into train and dev sets ===\n",
      "# set size of dev set\n",
      "pct_dev = 0.25\n",
      "n_dev = int(n_train_all * pct_dev)\n",
      "n_train = n_train_all - n_dev\n",
      "\n",
      "# Split the training dataframe into train and dev\n",
      "train_df = all_train_data_df[n_dev:]\n",
      "dev_df = test_data_df[:n_dev]\n",
      "print \"\\nThe shape of train_df is\", train_df.shape\n",
      "print train_df\n",
      "print \"\\nThe shape of dev_df is\", dev_df.shape\n",
      "print dev_df\n",
      "\n",
      "# Also process, then split, the labels\n",
      "train_labels = all_train_labels[n_dev:]\n",
      "dev_labels = all_train_labels[:n_dev]\n",
      "print \"\\nThe shape of train_labels is\", train_labels.shape\n",
      "print train_labels\n",
      "print \"\\nThe shape of dev_labels is\", dev_labels.shape\n",
      "print dev_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, building our vectorizer to turn raw training data into feature vectors.\n",
      "vectorizer = CountVectorizer()  #min_df = 1\n",
      "vectors_all_train = vectorizer.fit_transform(all_train_message_data)\n",
      "print vectors_all_train.shape\n",
      "\n",
      "vectorizer = CountVectorizer()  #min_df = 1\n",
      "vectors_dev = vectorizer.fit_transform(test_message_data)\n",
      "print vectors_dev.shape\n",
      "\n",
      "\n",
      "# The size of the vocabulary created:\n",
      "print \"The number of vocabulary terms created is %d.\" % len(vectorizer.vocabulary_)\n",
      "# So we know that there should be 12,317 different words. \n",
      "\n",
      "\n",
      "# Later I will find the number of words and the top words when the message received a pizza.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Additional ways to examine our vocabulary (taken from project 2)\n",
      "\n",
      "# Finding the average number of non-zero features:\n",
      "non_zero = vectors_all_train.nnz / float(vectors_all_train.shape[0])\n",
      "print \"The average number of non-zero features are %.3f \\n\" % non_zero\n",
      "\n",
      "# Fraction of entries that are non-zero:  \n",
      "fraction_non_zero = (vectors_all_train.nnz / (float(vectors_all_train.shape[0]*vectors_all_train.shape[1])))\n",
      "print \"The fraction of entries in the matrix that are non-zero are %.4f \\n\" % fraction_non_zero\n",
      "\n",
      "\n",
      "# Finding the first and last feature strings within our vocabulary vector. (alphabetical order)\n",
      "feature_vector = np.sort(vectorizer.get_feature_names())\n",
      "first_word = feature_vector[0] \n",
      "print \"The first word in our vocabulary vector is: \",first_word\n",
      "last_word = feature_vector[-1]\n",
      "print \"The last word in our vocabulary vector is: \", last_word\n",
      "print\n",
      "\n",
      "\"\"\"\n",
      "# Now, we find the number of words in the dev_data that are not in the train_data.\n",
      "total_vocabulary = ((len(vectors_all_train.vocabulary_)) + (len(vectors_dev.vocabulary_)))\n",
      "train_vocabulary = len(vectors_all_train.vocabulary_)\n",
      "dev_vocabulary = len(vectors_dev.vocabulary_)\n",
      "print \"The fraction of words in the dev data that are not seen in the training data are: %.3f \\n\" \\\n",
      "    % float(((total_vocabulary) - (train_vocabulary))/float(dev_vocabulary))\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Establishing my own vocabulary. I am taking this from my initial hypotheses of words that might be more inclined to earn a pizza\n",
      "# when mentioned in the body of a message. \n",
      "\n",
      "print \"Making up our own vocabulary :) \\n\"\n",
      "vocabulary = [\"unemployed\", \"broke\", \"hungry\", \"children\", \"student\", \"family\", \"military\", \"help\", \"please\"]\n",
      "vectorizer = CountVectorizer(vocabulary = vocabulary, ngram_range=(1,3))\n",
      "new_vector_9vocab = vectorizer.fit_transform(train_df[\"request_text_edit_aware\"])\n",
      "dev_vector_9vocab = vectorizer.transform(dev_df[\"request_text_edit_aware\"])\n",
      "\n",
      "# Confirming our new vector is appropriately shaped\n",
      "print \"Confirming our new vector is appropriately shaped: \", new_vector_9vocab.shape\n",
      "print\n",
      "\n",
      "# Finding the average number of non-zero features for our new_vector:\n",
      "non_zero = new_vector_9vocab.nnz / float(new_vector_9vocab.shape[0])\n",
      "print \"The average number of non-zero features in our new vector are: %.3f\" % non_zero\n",
      "print \n",
      "\n",
      "# It is nice to see that these words seem to make sense as our average number of non-zero features went from 0.0043 to 0.807\n",
      "\n",
      "# Logistic Regression\n",
      "log_res = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for value in log_res:\n",
      "    lr_vocab = LogisticRegression(C=value)\n",
      "    lr_vocab.fit(new_vector_9vocab, train_labels)\n",
      "    lr_score = metrics.roc_auc_score(dev_labels, lr_vocab.predict(dev_vector_9vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Logistic Regression model with a C value of %.3f has roc_auc of %.3f.\" % (value, lr_score)\n",
      "\n",
      "# Linear SVC\n",
      "c = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for new_value in c:\n",
      "    lsvc_vocab = LinearSVC(C=new_value)\n",
      "    lsvc_vocab.fit(new_vector_9vocab, train_labels)\n",
      "    lsvc_score = metrics.roc_auc_score(dev_labels, lsvc_vocab.predict(dev_vector_9vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Linear SVC model with a C value of %.3f has roc_auc of %.3f.\" % (new_value, lsvc_score)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Seeing how the custom vocabulary didn't seem to do too well, now I will go through and look at the top 50, or top 100 \n",
      "# words that occur in the winner's comments to see if this is an ideal way of predicting a pizza being awarded.\n",
      "\n",
      "all_train_messages = train_df[\"request_text_edit_aware\"]\n",
      "all_dev_messages = dev_df[\"request_text_edit_aware\"]\n",
      "\n",
      "\n",
      "vectorizer = CountVectorizer()  #min_df = 1\n",
      "vectors_all_train = vectorizer.fit_transform(all_train_messages)\n",
      "print vectors_all_train.shape\n",
      "\n",
      "vectorizer = CountVectorizer()  #min_df = 1\n",
      "vectors_dev = vectorizer.fit_transform(all_dev_messages)\n",
      "print vectors_dev.shape     \n",
      "        \n",
      "# One of the best ways to do this is to see not only what words were seen most in the winners' comments, but more importantly \n",
      "# what words were more likely to be seen in the winners' comments and less likely to be seen in the other comments.\n",
      "\n",
      "# This will also help take care of the words that appear very often, or in very few cases. But better yet, instead of throwing\n",
      "# out the words that are seen in very few posts it will keep the words that arent used often except when they are mainly \n",
      "# seen in the winners' posts.\n",
      "\n",
      "\n",
      "# Taken from project 2.\n",
      "# First, we will train our logistic regression model, and use Count Vectorizer for our unigram and bigram features.\n",
      "# Unigram\n",
      "\n",
      "unigram_vect = CountVectorizer()\n",
      "uni_LogRes_class = LogisticRegression(penalty='l2', C=0.5) \n",
      "train_vectors_LR = unigram_vect.fit_transform(all_train_messages)\n",
      "dev_vectors_LR = unigram_vect.transform(all_dev_messages)\n",
      "uni_LogRes_class.fit(train_vectors_LR, train_df['requester_received_pizza']) \n",
      "\n",
      "\n",
      "# Then we get all of the feature names for Unigram and Bigram Features.\n",
      "\n",
      "unigram_feat = unigram_vect.get_feature_names()\n",
      "weight_indexes = uni_LogRes_class.coef_[0].argsort()\n",
      "print \"TOP 10 Words:\"\n",
      "for each in weight_indexes[-10:][::-1]:\n",
      "    print unigram_feat[each], \"\\t\", uni_LogRes_class.coef_[0][each]\n",
      "\n",
      "print \"\\nBOTTOM 10 Words:\"\n",
      "for each in weight_indexes[0:10]:\n",
      "    print unigram_feat[each], \"\\t\", uni_LogRes_class.coef_[0][each]\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Bigram\n",
      "# Notice here that within our CountVectorizer bigram model we are letting our analyzer default to 'word' as opposed to 'char' \n",
      "# as seen within problem 2. My guess is that this will make the comparsion of unigram and bigram features more comparable, and \n",
      "# there was nothing specifically mentioned within the problem itself.\n",
      "\n",
      "bigram_vect = CountVectorizer()\n",
      "bi_LogRes_class = LogisticRegression(penalty='l2', C=0.5) \n",
      "train_vectors_LR = unigram_vect.fit_transform(all_train_messages)\n",
      "dev_vectors_LR = bigram_vect.transform(all_dev_messages)\n",
      "bi_LogRes_class.fit(train_vectors_LR, train_df['requester_received_pizza']) \n",
      "\n",
      "\n",
      "\n",
      "bigram_feat = bigram_vect.get_feature_names()\n",
      "\n",
      "weight_indexes = uni_LogRes_class.coef_[0].argsort()\n",
      "print \"TOP 10 Words:\"\n",
      "for each in weight_indexes[-10:][::-1]:\n",
      "    print bigram_feat[each], \"\\t\", uni_LogRes_class.coef_[0][each]\n",
      "\n",
      "print \"\\nBOTTOM 10 Words:\"\n",
      "for each in weight_indexes[0:10]:\n",
      "    print bigram_feat[each], \"\\t\", uni_LogRes_class.coef_[0][each]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now running a custom vocab for just those top five words.\n",
      "print \"Making up our own vocabulary :) \\n\"\n",
      "vocabulary = [\"surprised\", \"losing\", \"hurting\", \"pockets\", \"christmas\"]\n",
      "vectorizer = CountVectorizer(vocabulary = vocabulary)\n",
      "new_vector_5vocab = vectorizer.fit_transform(train_df[\"request_text_edit_aware\"])\n",
      "dev_vector_5vocab = vectorizer.transform(dev_df[\"request_text_edit_aware\"])\n",
      "\n",
      "# Confirming our new vector is appropriately shaped\n",
      "print \"Confirming our new vector is appropriately shaped: \", new_vector_5vocab.shape\n",
      "print\n",
      "\n",
      "# Finding the average number of non-zero features for our new_vector:\n",
      "non_zero = new_vector_9vocab.nnz / float(new_vector_5vocab.shape[0])\n",
      "print \"The average number of non-zero features in our new vector are: %.3f\" % non_zero\n",
      "print \n",
      "\n",
      "# It is nice to see that these words seem to make sense as our average number of non-zero features went from 0.0043 to 0.807\n",
      "\n",
      "# Logistic Regression\n",
      "log_res = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for value in log_res:\n",
      "    lr_vocab = LogisticRegression(C=value)\n",
      "    lr_vocab.fit(new_vector_5vocab, train_labels)\n",
      "    lr_score = metrics.roc_auc_score(dev_labels, lr_vocab.predict(dev_vector_5vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Logistic Regression model with a C value of %.3f has roc_auc of %.3f.\" % (value, lr_score)\n",
      "\n",
      "# Linear SVC\n",
      "c = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for new_value in c:\n",
      "    lsvc_vocab = LinearSVC(C=new_value)\n",
      "    lsvc_vocab.fit(new_vector_5vocab, train_labels)\n",
      "    lsvc_score = metrics.roc_auc_score(dev_labels, lsvc_vocab.predict(dev_vector_5vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Linear SVC model with a C value of %.3f has roc_auc of %.3f.\" % (new_value, lsvc_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############## might need to tweak with this section a bit.\n",
      "\n",
      "# Now running a custom vocab for TOP 50 words.\n",
      "print \"Making up our own vocabulary :) \\n\"\n",
      "vocabulary = unigram_feat[-50:][::-1]\n",
      "vectorizer = CountVectorizer(vocabulary = vocabulary)\n",
      "new_vector_top50vocab = vectorizer.fit_transform(train_df[\"request_text_edit_aware\"])\n",
      "dev_vector_top50vocab = vectorizer.transform(dev_df[\"request_text_edit_aware\"])\n",
      "\n",
      "\n",
      "# Confirming our new vector is appropriately shaped\n",
      "print \"Confirming our new vector is appropriately shaped: \", new_vector_top50vocab.shape\n",
      "print\n",
      "\n",
      "# Finding the average number of non-zero features for our new_vector:\n",
      "non_zero = new_vector_top50vocab.nnz / float(new_vector_top50vocab.shape[0])\n",
      "print \"The average number of non-zero features in our new vector are: %.3f\" % non_zero\n",
      "print \n",
      "\n",
      "# It is nice to see that these words seem to make sense as our average number of non-zero features went from 0.0043 to 0.807\n",
      "\n",
      "# Logistic Regression\n",
      "log_res = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for value in log_res:\n",
      "    lr_vocab = LogisticRegression(C=value)\n",
      "    lr_vocab.fit(new_vector_top50vocab, train_labels)\n",
      "    lr_score = metrics.roc_auc_score(dev_labels, lr_vocab.predict(dev_vector_top50vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Logistic Regression model with a C value of %.3f has roc_auc of %.3f.\" % (value, lr_score)\n",
      "\n",
      "# Linear SVC\n",
      "c = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for new_value in c:\n",
      "    lsvc_vocab = LinearSVC(C=new_value)\n",
      "    lsvc_vocab.fit(new_vector_top50vocab, train_labels)\n",
      "    lsvc_score = metrics.roc_auc_score(dev_labels, lsvc_vocab.predict(dev_vector_top50vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Linear SVC model with a C value of %.3f has roc_auc of %.3f.\" % (new_value, lsvc_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now running a custom vocab for TOP 100 words.\n",
      "print \"Making up our own vocabulary :) \\n\"\n",
      "vocabulary = unigram_feat[-100:][::-1]\n",
      "vectorizer = CountVectorizer(vocabulary = vocabulary)\n",
      "new_vector_top100vocab = vectorizer.fit_transform(train_df[\"request_text_edit_aware\"])\n",
      "dev_vector_top100vocab = vectorizer.transform(dev_df[\"request_text_edit_aware\"])\n",
      "\n",
      "\n",
      "# Confirming our new vector is appropriately shaped\n",
      "print \"Confirming our new vector is appropriately shaped: \", new_vector_top100vocab.shape\n",
      "print\n",
      "\n",
      "# Finding the average number of non-zero features for our new_vector:\n",
      "non_zero = new_vector_top100vocab.nnz / float(new_vector_top100vocab.shape[0])\n",
      "print \"The average number of non-zero features in our new vector are: %.3f\" % non_zero\n",
      "print \n",
      "\n",
      "# It is nice to see that these words seem to make sense as our average number of non-zero features went from 0.0043 to 0.807\n",
      "\n",
      "# Logistic Regression\n",
      "log_res = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for value in log_res:\n",
      "    lr_vocab = LogisticRegression(C=value)\n",
      "    lr_vocab.fit(new_vector_top100vocab, train_labels)\n",
      "    lr_score = metrics.roc_auc_score(dev_labels, lr_vocab.predict(dev_vector_top100vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Logistic Regression model with a C value of %.3f has roc_auc of %.3f.\" % (value, lr_score)\n",
      "\n",
      "# Linear SVC\n",
      "c = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 10.0, 50.0, 100.0, 200.0]\n",
      "for new_value in c:\n",
      "    lsvc_vocab = LinearSVC(C=new_value)\n",
      "    lsvc_vocab.fit(new_vector_top100vocab, train_labels)\n",
      "    lsvc_score = metrics.roc_auc_score(dev_labels, lsvc_vocab.predict(dev_vector_top100vocab))\n",
      "    print \"===== Logistic Regression =====\"\n",
      "    print \"The Linear SVC model with a C value of %.3f has roc_auc of %.3f.\" % (new_value, lsvc_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}