{
 "metadata": {
  "name": "",
  "signature": "sha256:a128dcf338a8cd519c68e52b542cf74b1a813b444d41713226fb2b245075cecd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import json\n",
      "import csv\n",
      "import numpy as np\n",
      "import random as rand\n",
      "import pandas as pd\n",
      "import scipy as scipy\n",
      "import datetime as dt\n",
      "import time\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import make_scorer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_json_file(path):\n",
      "    with open(path) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def make_submission_csv(predictions, ids, submission_name, path = '../../predictions'):\n",
      "    with open(path+'/'+submission_name+'.csv', 'w') as csvfile:\n",
      "        field_names = ['request_id', 'requester_received_pizza']\n",
      "        writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
      "        writer.writeheader()\n",
      "        csv_data = zip(ids, predictions)\n",
      "        for row in csv_data:\n",
      "            writer.writerow({field_names[0]:row[0], field_names[1]:int(row[1])})\n",
      "\n",
      "def test_kfolds(X, y, kf, model, verbose=True):\n",
      "    roc_auc_list = []\n",
      "    \n",
      "    for train_i, dev_i in kf:\n",
      "        X_train = X[train_i]\n",
      "        X_dev = X[dev_i]\n",
      "\n",
      "        model.fit(X_train, y[train_i])\n",
      "\n",
      "        dev_pred = model.predict(X_dev)\n",
      "        \n",
      "        roc_auc_i = roc_auc_score(y[dev_i], dev_pred)\n",
      "        roc_auc_list.append(roc_auc_i)\n",
      "        if verbose:\n",
      "            print('ROC AUC:',roc_auc_i)\n",
      "            \n",
      "    if verbose:\n",
      "        print 'Mean: %f, Median: %f' %(np.mean(roc_auc_list), np.median(roc_auc_list))\n",
      "        \n",
      "    return roc_auc_list\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data from JSON file as list of dicts\n",
      "all_train_dict_list = load_json_file('../../data/train.json')\n",
      "submit_dict_list =  load_json_file('../../data/test.json')\n",
      "\n",
      "n_all = len(all_train_dict_list)\n",
      "n_submit = len(submit_dict_list)\n",
      "\n",
      "# shuffle data to avoid biased split of train / dev data\n",
      "rand.shuffle(all_train_dict_list)\n",
      "\n",
      "# set up kFolds\n",
      "kf = KFold(n_all, n_folds = 5)\n",
      "\n",
      "# process labels\n",
      "all_train_labels = np.array([x['requester_received_pizza'] for x in all_train_dict_list])\n",
      "\n",
      "# pandas is useful for turning dicts in to matrix-like objects\n",
      "# where each column is an numpy array\n",
      "submit_df = pd.DataFrame(submit_dict_list)\n",
      "all_train_df = pd.DataFrame(all_train_dict_list)\n",
      "\n",
      "# limit train to columns available in submit_df\n",
      "submit_cols = submit_df.columns\n",
      "all_train_df = all_train_df[submit_cols]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# which columns are numeric activity variables\n",
      "activity_var = ['requester_account_age_in_days_at_request',\n",
      "                'requester_days_since_first_post_on_raop_at_request',\n",
      "                'requester_number_of_comments_at_request',\n",
      "                'requester_number_of_comments_in_raop_at_request',\n",
      "                'requester_number_of_posts_at_request',\n",
      "                'requester_number_of_posts_on_raop_at_request',\n",
      "                'requester_number_of_subreddits_at_request',\n",
      "                'requester_upvotes_minus_downvotes_at_request',\n",
      "                'requester_upvotes_plus_downvotes_at_request'\n",
      "                ]\n",
      "# look at correlations of requester activity variables:\n",
      "activity_all = all_train_df.loc[:,activity_var]\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_act = scaler.fit_transform(activity_all)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example classification\n",
      "svc = SVC(class_weight='auto')\n",
      "lr = LogisticRegression(class_weight='auto')\n",
      "\n",
      "all_res = test_kfolds(X_act, all_train_labels, kf, svc)\n",
      "\n",
      "all_res = test_kfolds(X_act, all_train_labels, kf, lr)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.57437094155844148)\n",
        "('ROC AUC:', 0.53687660302080364)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.54735793847893977)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.54659786329218962)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5470115233633025)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.550443, Median: 0.547012\n",
        "('ROC AUC:', 0.55117018398268403)\n",
        "('ROC AUC:', 0.53486951919553805)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5197160818641553)\n",
        "('ROC AUC:', 0.55191370189814326)\n",
        "('ROC AUC:', 0.54918006838039768)\n",
        "Mean: 0.541370, Median: 0.549180\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Notes\n",
      "# results slightly better w/ lowercase = False (when unigrams only)\n",
      "# bigrams added no value on unigrams\n",
      "\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "\n",
      "cv = CountVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "lsvc_pipe = Pipeline([('cv',cv),('lsvc',lsvc)])\n",
      "\n",
      "test_kfolds(titles, all_train_labels, kf, lsvc_pipe)\n",
      "\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 2)\n",
      "lsvc_pipe = Pipeline([('tv',tv),('lsvc',lsvc)])\n",
      "\n",
      "test_kfolds(titles, all_train_labels, kf, lsvc_pipe)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.54528544372294374)\n",
        "('ROC AUC:', 0.55413426698693169)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.52409697804224353)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.48031843169795663)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.52787450930733182)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.526342, Median: 0.527875\n",
        "('ROC AUC:', 0.53439529220779225)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.54996946627040666)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.51923660117862092)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.4792379766967465)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.50554007851082694)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.517676, Median: 0.519237\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[0.53439529220779225,\n",
        " 0.54996946627040666,\n",
        " 0.51923660117862092,\n",
        " 0.4792379766967465,\n",
        " 0.50554007851082694]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try to mix text and numeric via output of text only model\n",
      "#NOTES\n",
      "# For text model that is input in to the final model, LinearSVC isn't great\n",
      "# because it only gives binary predictions \n",
      "\n",
      "X1 = all_train_df['request_title'].values\n",
      "X2 = all_train_df.loc[:,activity_var].values\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "cv = CountVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = .1)\n",
      "mnb = MultinomialNB(alpha=50)\n",
      "model1 = Pipeline([('cv',cv),('lsvc',lsvc)])\n",
      "#model1 = Pipeline([('cv',cv),('mnb',mnb)])\n",
      "model2 = SVC(class_weight='auto', C=1)\n",
      "\n",
      "roc_auc_list = []\n",
      "    \n",
      "for train_i, dev_i in kf:\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    X_train1 = X1[train_i]\n",
      "    X_dev1 = X1[dev_i]\n",
      "    \n",
      "    # fit initial text model\n",
      "    model1.fit(X_train1, y[train_i])\n",
      "    \n",
      "    if False:\n",
      "        #if predict has proba\n",
      "        train_pred1 = model1.predict_log_proba(X_train1)[:,0].reshape((train_i.shape[0],1))\n",
      "        dev_pred1 = model1.predict_log_proba(X_dev1)[:,0].reshape((dev_i.shape[0],1))\n",
      "        #print train_pred1\n",
      "    else:\n",
      "        train_pred1 = model1.decision_function(X_train1).reshape((train_i.shape[0],1))\n",
      "        dev_pred1 = model1.decision_function(X_dev1).reshape((dev_i.shape[0],1))\n",
      "    \n",
      "    # add output of initial model in to second X matrix\n",
      "    X_train2 = np.hstack((X2[train_i], train_pred1))\n",
      "    X_dev2 = np.hstack((X2[dev_i], dev_pred1))\n",
      "    \n",
      "    #X_train2 = X2[train_i]\n",
      "    #X_dev2 = X2[dev_i]\n",
      "    \n",
      "    # scale the new X matrix\n",
      "    scaler = StandardScaler()\n",
      "    X_train2 = scaler.fit_transform(X_train2)\n",
      "    X_dev2 = scaler.transform(X_dev2)\n",
      "    \n",
      "    # train the second model\n",
      "    model2.fit(X_train2, y[train_i])\n",
      "    dev_pred2 = model2.predict(X_dev2)\n",
      "\n",
      "    roc_auc_i = roc_auc_score(y[dev_i], dev_pred2)\n",
      "    roc_auc_list.append(roc_auc_i)\n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "     print 'Mean: %f, Median: %f' %(np.mean(roc_auc_list), np.median(roc_auc_list))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.55421401515151525)\n",
        "('ROC AUC:', 0.54917558930098109)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.55557877826682067)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.51121944473256575)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.55014562492085606)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.544067, Median: 0.550146\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(kf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine title and text approaches\n",
      "\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "\n",
      "X1 = titles\n",
      "X2 = bodies\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "\n",
      "model = lsvc\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_both = []\n",
      "    \n",
      "for train_i, dev_i in kf:\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    if do_titles:\n",
      "        X_train = tv.fit_transform(X1[train_i])\n",
      "        X_dev = tv.transform(X1[dev_i])\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.005)\n",
      "            model.fit(X_train, y[train_i])\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'l2', C=5)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y[train_i])\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y[dev_i], pred)\n",
      "        roc_auc_list_titles.append(roc_auc_i)\n",
      "        \n",
      "        \n",
      "        X_train1 = X_train\n",
      "        X_dev1 = X_dev\n",
      "    \n",
      "    if do_bodies:\n",
      "        X_train = tv.fit_transform(X2[train_i])\n",
      "        X_dev = tv.transform(X2[dev_i])\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.0025)\n",
      "            model.fit(X_train, y[train_i])\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'l2', C=1)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y[train_i])\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y[dev_i], pred)\n",
      "        roc_auc_list_bodies.append(roc_auc_i)\n",
      "        \n",
      "        X_train2 = X_train\n",
      "        X_dev2 = X_dev\n",
      "    \n",
      "    if do_bodies and do_titles:\n",
      "        X_train = scipy.sparse.hstack((X_train1,X_train2),'csr')\n",
      "        X_dev = scipy.sparse.hstack((X_dev1,X_dev2),'csr')\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=1)\n",
      "            model.fit(X_train, y[train_i])\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'l2', C=1)\n",
      "        \n",
      "        model.fit(X_train, y[train_i])\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y[dev_i], pred)\n",
      "        roc_auc_list_both.append(roc_auc_i)\n",
      "    \n",
      "    \n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "    print 'Titles: Mean: %f, Median: %f' %(np.mean(roc_auc_list_titles), np.median(roc_auc_list_titles))\n",
      "    print 'Bodies: Mean: %f, Median: %f' %(np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies))\n",
      "    print 'Both: Mean: %f, Median: %f' %(np.mean(roc_auc_list_both), np.median(roc_auc_list_both))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l1' has been deprecated in favor of loss='hinge' as of 0.16. Backward compatibility for the loss='l1' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l1' has been deprecated in favor of loss='hinge' as of 0.16. Backward compatibility for the loss='l1' will be removed in 1.0\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "44/12836\n",
        "('ROC AUC:', 0.58174377705627711)\n",
        "44/12891"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l1' has been deprecated in favor of loss='hinge' as of 0.16. Backward compatibility for the loss='l1' will be removed in 1.0\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58159833896511015)\n",
        "41/12952"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l1' has been deprecated in favor of loss='hinge' as of 0.16. Backward compatibility for the loss='l1' will be removed in 1.0\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5618694009360119)\n",
        "43/13021"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l1' has been deprecated in favor of loss='hinge' as of 0.16. Backward compatibility for the loss='l1' will be removed in 1.0\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57347958372229713)\n",
        "40/12625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.6007502849183235)\n",
        "Titles: Mean: nan, Median: nan\n",
        "Bodies: Mean: 0.579888, Median: 0.581598\n",
        "Both: Mean: nan, Median: nan\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\sklearn\\svm\\classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
        "  DeprecationWarning)\n",
        "C:\\Users\\Ross\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
        "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def timify(feature_df):\n",
      "    req_dt = np.array([dt.datetime.fromtimestamp(timei) for timei in feature_df['unix_timestamp_of_request']])\n",
      "    hour = np.array([dti.hour for dti in req_dt])\n",
      "    dow = np.array([dti.weekday() for dti in req_dt])\n",
      "    month = np.array([dti.month for dti in req_dt])\n",
      "\n",
      "    req_dt_utc = np.array([dt.datetime.fromtimestamp(timei) for timei in feature_df['unix_timestamp_of_request_utc']])\n",
      "    hour_utc = np.array([dti.hour for dti in req_dt_utc])\n",
      "    dow_utc = np.array([dti.weekday() for dti in req_dt_utc])\n",
      "    #hour_utc = np.array([dt.datetime.fromtimestamp(timei).hour  for timei in all_train_df['unix_timestamp_of_request_utc']])\n",
      "    \n",
      "    time_df = pd.DataFrame({'request_datetime':req_dt,\n",
      "                            'request_hour':hour,\n",
      "                            'request_dow':dow,\n",
      "                            'request_month':month})\n",
      "\n",
      "    return time_df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_df = timify(all_train_df)\n",
      "X_time = time_df[['request_dow','request_hour']].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hour_pos = hour[all_train_labels]\n",
      "hour_neg = hour[np.logical_not(all_train_labels)]\n",
      "pd.Series(hour_pos).hist(bins=24, alpha=0.2, normed=True)\n",
      "pd.Series(hour_neg).hist(bins=24, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x42cbe710>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1ikCIjsAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x42cbe0f0>"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dow_pos = dow[all_train_labels]\n",
      "dow_neg = dow[np.logical_not(all_train_labels)]\n",
      "pd.Series(dow_pos).hist(bins=7, alpha=0.2, normed=True)\n",
      "pd.Series(dow_neg).hist(bins=7, alpha=0.2, normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x35ecc898>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x49b120b8>"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "#X_time = np.array([hour,dow]).T\n",
      "\n",
      "scorer = make_scorer(roc_auc_score)\n",
      "scores = cross_val_score(etc, X_time, all_train_labels, cv=KFold(n_all, n_folds = 10), scoring=scorer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.median(scores)\n",
      "print np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.517785576162\n",
        "0.521479486514\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine title and text approaches\n",
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "time_df = timify(all_train_df)\n",
      "times = time_df[['request_dow','request_hour','request_month']].values\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 2000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "do_acts = True\n",
      "do_times = True\n",
      "\n",
      "model = lsvc\n",
      "scaler = StandardScaler()\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_acts = []\n",
      "roc_auc_list_times = []\n",
      "roc_auc_list_all = []\n",
      "    \n",
      "for train_i, dev_i in KFold(n_all, n_folds = 10):\n",
      "    X_dict = {}\n",
      "    #X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    #X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    #X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['times'] = {'train' : times[train_i], 'dev' : times[dev_i], 'model' : etc}\n",
      "\n",
      "    \n",
      "    X_all = {'train':(), 'dev':()}\n",
      "    \n",
      "    y_train = y[train_i]\n",
      "    y_dev = y[dev_i]\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    if do_titles:\n",
      "        X_train = tv.fit_transform(X_dict['titles']['train'])\n",
      "        X_dev = tv.transform(X_dict['titles']['dev'])\n",
      "        model = X_dict['titles']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.003)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=5)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_titles.append(roc_auc_i)\n",
      "        \n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    if do_bodies:\n",
      "        X_train = tv.fit_transform(X_dict['bodies']['train'])\n",
      "        X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "        model = X_dict['bodies']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.0025)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=1)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_bodies.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_acts:\n",
      "        X_train = scaler.fit_transform(X_dict['acts']['train'])\n",
      "        X_dev = scaler.transform(X_dict['acts']['dev'])\n",
      "        model = X_dict['acts']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_acts.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_times:\n",
      "        X_train = X_dict['times']['train']\n",
      "        X_dev = X_dict['times']['dev']\n",
      "        model = model = X_dict['times']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_times.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    if do_bodies + do_titles + do_acts > 1:\n",
      "        #X_train = scaler.fit_transform(scipy.sparse.hstack(X_all['train'],'csr').toarray())\n",
      "        X_train = scipy.sparse.hstack(X_all['train'],'csr').toarray()\n",
      "        #X_dev = scaler.transform(scipy.sparse.hstack( X_all['dev'],'csr').toarray())\n",
      "        X_dev = scipy.sparse.hstack( X_all['dev'],'csr').toarray()\n",
      "\n",
      "        \n",
      "        \n",
      "        if prune_features:\n",
      "            model = lsvc\n",
      "            model.set_params(loss = 'hinge', C=5)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "          \n",
      "        model = svc\n",
      "        model.set_params(C=1)\n",
      "        \n",
      "        model = etc\n",
      "        \n",
      "        \n",
      "        model.fit(X_train,y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_all.append(roc_auc_i)\n",
      "    \n",
      "    \n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "    print 'Titles: Mean: %f, Median: %f' %(np.mean(roc_auc_list_titles), np.median(roc_auc_list_titles))\n",
      "    print 'Bodies: Mean: %f, Median: %f' %(np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies))\n",
      "    print 'Activities: Mean: %f, Median: %f' %(np.mean(roc_auc_list_acts), np.median(roc_auc_list_acts))\n",
      "    print 'All: Mean: %f, Median: %f' %(np.mean(roc_auc_list_all), np.median(roc_auc_list_all))\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "56/13657\n",
        "67/68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.61507479861910241)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49/13731"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56/61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.61089743589743595)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56/13764"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67/68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.61386138613861385)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49/13677"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59/61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5613232047786002)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50/13774"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61/62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58516436722875143)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47/13726"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57/59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.62216664556160561)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54/13780"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64/66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57561796109404706)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49/13787"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "59/61"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.6021052631578947)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51/13556"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60/63"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.66743970315398893)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50/13640"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61/62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.64975327879496181)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Titles: Mean: nan, Median: nan\n",
        "Bodies: Mean: 0.583764, Median: 0.584844\n",
        "Activities: Mean: 0.552614, Median: 0.552422\n",
        "All: Mean: 0.610340, Median: 0.612379\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine numeric and text approaches\n",
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "time_df = timify(all_train_df)\n",
      "times = time_df[['request_dow','request_hour']].values\n",
      "time_df_submit = timify(submit_df)\n",
      "times_submit = time_df_submit[['request_dow','request_hour']].values\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False)\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators=1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "do_acts = True\n",
      "\n",
      "model = lsvc\n",
      "scaler = StandardScaler()\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_acts = []\n",
      "roc_auc_list_times = []\n",
      "roc_auc_list_all = []\n",
      "    \n",
      "y_train = y\n",
      "    \n",
      "X_dict = {}\n",
      "X_dict['titles'] = {'train' : titles, 'dev' : submit_df['request_title'].values, 'model' : lsvc}\n",
      "X_dict['bodies'] = {'train' : bodies, 'dev' : submit_df['request_text_edit_aware'].values, 'model' : lsvc}\n",
      "X_dict['acts'] = {'train' : acts, 'dev' : submit_df[activity_var].values, 'model' : svc}\n",
      "X_dict['times'] = {'train' : times, 'dev' : times_submit, 'model' : etc}\n",
      "\n",
      "X_all = {'train':(), 'dev':()}\n",
      "\n",
      "# get initial training / dev data for text model\n",
      "if do_titles:\n",
      "    X_train = tv.fit_transform(X_dict['titles']['train'])\n",
      "    X_dev = tv.transform(X_dict['titles']['dev'])\n",
      "    model = X_dict['titles']['model']\n",
      "\n",
      "    if prune_features:\n",
      "        model.set_params(loss = 'hinge', C=.005)\n",
      "        model.fit(X_train, y_train)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "        print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "        X_train = X_train[:,sig_coef]\n",
      "        X_dev = X_dev[:,sig_coef]\n",
      "        model.set_params(loss = 'squared_hinge', C=5)\n",
      "\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "\n",
      "if do_bodies:\n",
      "    X_train = tv.fit_transform(X_dict['bodies']['train'])\n",
      "    X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "    model = X_dict['bodies']['model']\n",
      "\n",
      "    if prune_features:\n",
      "        model.set_params(loss = 'hinge', C=.0025)\n",
      "        model.fit(X_train, y_train)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "        print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "        X_train = X_train[:,sig_coef]\n",
      "        X_dev = X_dev[:,sig_coef]\n",
      "        model.set_params(loss = 'squared_hinge', C=1)\n",
      "\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "\n",
      "if do_acts:\n",
      "    X_train = scaler.fit_transform(X_dict['acts']['train'])\n",
      "    X_dev = scaler.transform(X_dict['acts']['dev'])\n",
      "    model = X_dict['acts']['model']\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    \n",
      "if do_times:\n",
      "    X_train = X_dict['times']['train']\n",
      "    X_dev = X_dict['times']['dev']\n",
      "    model = X_dict['times']['model']\n",
      "\n",
      "    model.fit(X_train, y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n",
      "    #roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "    #roc_auc_list_times.append(roc_auc_i)\n",
      "\n",
      "    X_all['train'] = X_all['train'] + (X_train,)\n",
      "    X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "\n",
      "\n",
      "if do_bodies + do_titles + do_acts > 1:\n",
      "    #X_train = scaler.fit_transform(scipy.sparse.hstack(X_all['train'],'csr').toarray())\n",
      "    X_train = scipy.sparse.hstack(X_all['train'],'csr').toarray()\n",
      "    #X_dev = scaler.transform(scipy.sparse.hstack( X_all['dev'],'csr').toarray())\n",
      "    X_dev = scipy.sparse.hstack( X_all['dev'],'csr').toarray()\n",
      "\n",
      "    #if prune_features:\n",
      "    if True:\n",
      "        model = lsvc\n",
      "        model.set_params(loss = 'hinge', C=5)\n",
      "        model.fit(X_train, y_train)\n",
      "        coef = model.coef_\n",
      "        sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "        print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "        X_train = X_train[:,sig_coef]\n",
      "        X_dev = X_dev[:,sig_coef]\n",
      "\n",
      "    model = svc\n",
      "    model.set_params(C=1)\n",
      "\n",
      "    model = etc\n",
      "\n",
      "\n",
      "    model.fit(X_train,y_train)\n",
      "    pred = model.predict(X_dev)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "66/14509\n",
        "75/77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_submission_csv(pred, submit_df['request_id'].values, 'request_text_and_activity_and_time_ETClassifier')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "class SnowballStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = SnowballStemmer('english')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "class PorterStemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stmr = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stmr.stem(t) for t in word_tokenize(doc)]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combine title, body, time, numeric\n",
      "# Add text lemmazation & stemming\n",
      "\n",
      "acts = all_train_df[activity_var].values\n",
      "titles = all_train_df['request_title'].values\n",
      "bodies = all_train_df['request_text_edit_aware'].values\n",
      "time_df = timify(all_train_df)\n",
      "times = time_df[['request_dow','request_hour','request_month']].values\n",
      "\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "text_derivs = np.hstack((title_len, body_len))\n",
      "\n",
      "tv = TfidfVectorizer(ngram_range=(1,1),lowercase=False, tokenizer=StemTokenizer())\n",
      "lsvc = LinearSVC(class_weight='auto', C = 1)\n",
      "svc = SVC(class_weight='auto')\n",
      "\n",
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "prune_features = True\n",
      "\n",
      "y = all_train_labels\n",
      "verbose = True\n",
      "\n",
      "do_titles = False\n",
      "do_bodies = True\n",
      "do_acts = True\n",
      "do_times = True\n",
      "do_text_deriv = True\n",
      "\n",
      "model = lsvc\n",
      "scaler = StandardScaler()\n",
      "\n",
      "roc_auc_list_titles = []\n",
      "roc_auc_list_bodies = []\n",
      "roc_auc_list_acts = []\n",
      "roc_auc_list_times = []\n",
      "roc_auc_list_text_derivs = []\n",
      "roc_auc_list_all = []\n",
      "    \n",
      "for train_i, dev_i in KFold(n_all, n_folds = 10):\n",
      "    X_dict = {}\n",
      "    #X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    #X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    #X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['titles'] = {'train' : titles[train_i], 'dev' : titles[dev_i], 'model' : lsvc}\n",
      "    X_dict['bodies'] = {'train' : bodies[train_i], 'dev' : bodies[dev_i], 'model' : lsvc}\n",
      "    X_dict['acts'] = {'train' : acts[train_i], 'dev' : acts[dev_i], 'model' : svc}\n",
      "    X_dict['times'] = {'train' : times[train_i], 'dev' : times[dev_i], 'model' : etc}\n",
      "    X_dict['text_derivs'] = {'train' : text_derivs[train_i], 'dev' : text_derivs[dev_i], 'model' : etc}\n",
      "\n",
      "    \n",
      "    X_all = {'train':(), 'dev':()}\n",
      "    \n",
      "    y_train = y[train_i]\n",
      "    y_dev = y[dev_i]\n",
      "    \n",
      "    # get initial training / dev data for text model\n",
      "    if do_titles:\n",
      "        X_train = tv.fit_transform(X_dict['titles']['train'])\n",
      "        X_dev = tv.transform(X_dict['titles']['dev'])\n",
      "        model = X_dict['titles']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.003)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=5)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_titles.append(roc_auc_i)\n",
      "        \n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "    \n",
      "    if do_bodies:\n",
      "        X_train = tv.fit_transform(X_dict['bodies']['train'])\n",
      "        X_dev = tv.transform(X_dict['bodies']['dev'])\n",
      "        model = X_dict['bodies']['model']\n",
      "        \n",
      "        if prune_features:\n",
      "            model.set_params(loss = 'hinge', C=.0025)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "\n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "            model.set_params(loss = 'squared_hinge', C=1)\n",
      "        \n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_bodies.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_acts:\n",
      "        X_train = scaler.fit_transform(X_dict['acts']['train'])\n",
      "        X_dev = scaler.transform(X_dict['acts']['dev'])\n",
      "        model = X_dict['acts']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_acts.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_times:\n",
      "        X_train = X_dict['times']['train']\n",
      "        X_dev = X_dict['times']['dev']\n",
      "        model = model = X_dict['times']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_times.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    if do_text_deriv:\n",
      "        X_train = X_dict['text_derivs']['train']\n",
      "        X_dev = X_dict['text_derivs']['dev']\n",
      "        model = model = X_dict['text_derivs']['model']\n",
      "        \n",
      "        model.fit(X_train, y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_text_derivs.append(roc_auc_i)\n",
      "        \n",
      "        X_all['train'] = X_all['train'] + (X_train,)\n",
      "        X_all['dev'] = X_all['dev'] + (X_dev,)\n",
      "        \n",
      "    \n",
      "    if do_bodies + do_titles + do_acts > 1:\n",
      "        #X_train = scaler.fit_transform(scipy.sparse.hstack(X_all['train'],'csr').toarray())\n",
      "        X_train = scipy.sparse.hstack(X_all['train'],'csr').toarray()\n",
      "        #X_dev = scaler.transform(scipy.sparse.hstack( X_all['dev'],'csr').toarray())\n",
      "        X_dev = scipy.sparse.hstack( X_all['dev'],'csr').toarray()\n",
      "\n",
      "        \n",
      "        \n",
      "        if prune_features:\n",
      "            model = lsvc\n",
      "            model.set_params(loss = 'hinge', C=5)\n",
      "            model.fit(X_train, y_train)\n",
      "            coef = model.coef_\n",
      "            sig_coef = (np.abs(coef) > 0.01)[0]\n",
      "            print '%d/%d' % (np.sum(sig_coef), coef.shape[1])\n",
      "            \n",
      "            X_train = X_train[:,sig_coef]\n",
      "            X_dev = X_dev[:,sig_coef]\n",
      "          \n",
      "        model = svc\n",
      "        model.set_params(C=1)\n",
      "        \n",
      "        model = etc\n",
      "        \n",
      "        \n",
      "        model.fit(X_train,y_train)\n",
      "        pred = model.predict(X_dev)\n",
      "\n",
      "        roc_auc_i = roc_auc_score(y_dev, pred)\n",
      "        roc_auc_list_all.append(roc_auc_i)\n",
      "    \n",
      "    \n",
      "    if verbose:\n",
      "        print('ROC AUC:',roc_auc_i)\n",
      "        \n",
      "if verbose:\n",
      "    print 'Titles: Mean: %f, Median: %f' %(np.mean(roc_auc_list_titles), np.median(roc_auc_list_titles))\n",
      "    print 'Bodies: Mean: %f, Median: %f' %(np.mean(roc_auc_list_bodies), np.median(roc_auc_list_bodies))\n",
      "    print 'Activities: Mean: %f, Median: %f' %(np.mean(roc_auc_list_acts), np.median(roc_auc_list_acts))\n",
      "    print 'Text Derivs: Mean: %f, Median: %f' %(np.mean(roc_auc_list_acts), np.median(roc_auc_list_text_derivs))\n",
      "    print 'All: Mean: %f, Median: %f' %(np.mean(roc_auc_list_all), np.median(roc_auc_list_all))\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50/3919\n",
        "69/9872"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "115/133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.65607019562715774)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48/3886"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65/9890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "108/127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.59166666666666656)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42/3928"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76/9936"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "111/132"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57095709570957087)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42/3848"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "67/9858"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_train_df.request_title.values[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 121,
       "text": [
        "array([u'[Request] Fitchburg MA, new mom to overwhelmed to keep up with meals',\n",
        "       u'[Request] Bozeman, Montana graduate student will pay it forward next week.',\n",
        "       u'[REQUEST] Baton Rouge, LA. Single dad with 2 jobs and no money till Friday!',\n",
        "       u'[REQUEST] Lansing, MI. Would love a pizza for dinner!',\n",
        "       u\"[request] worked all day, working all day tommorw and I'm broke until friday. I don't want to go to bed hungry!\",\n",
        "       u'[Request] Columbus, GA Very prego Momma craving a pizza :)',\n",
        "       u'[REQUEST] Three year redditor needs food, badly',\n",
        "       u'[REQUEST] Planning a surprise party for a friend, forgot to budget for myself to eat for a couple of days. Oakland, CA',\n",
        "       u'[Request] A Table Top RPG Group in Chicago',\n",
        "       u'Request: New York, USA. Would really appreciate some pizza (both gluten free and regular)',\n",
        "       u\"[REQUEST]Just moved in and debit card isn't here yet,  not starving but broke\",\n",
        "       u'[REQUEST] Unemployed w/sick dog &amp; huge vet bills - Phoenix, AZ area',\n",
        "       u'[REQUEST] Ohio - My Wings just lost, a pizza would really help ease the pain.',\n",
        "       u\"[Request] Paid back debt to a friend now I'm too broke for pizza\",\n",
        "       u'(request) Been up all night crying, screaming, and now starving. Jacksonville FL.',\n",
        "       u'[Request]Texas, drinking by myself tonight and would love a pizza',\n",
        "       u\"[REQUEST] I'm hungry and only have frozen foods. I've been eating them everyday and I don't think it's healthy to continue to do so, so can I please have a fresh pizza? I live in Toronto, Canada\",\n",
        "       u'[Request]! Would like to surprise my roommate! She helped me pass my finals!!',\n",
        "       u\"[REQUEST] Pizza for broke me and a friend, tried to buy it with bitcoins and barely didn't have enough\",\n",
        "       u'[Request] Help me give back to my roomies on Friday'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "title_unicode = all_train_df.request_title.values\n",
      "title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
      "body_unicode = all_train_df.request_text_edit_aware.values\n",
      "body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
      "#title_len = [len(str(x)) for x in all_train_df.request_title.values]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "etc = ExtraTreesClassifier(n_estimators = 1000,\n",
      "                           max_depth = 5,\n",
      "                           min_samples_split=10,\n",
      "                           class_weight='auto')\n",
      "\n",
      "test_kfolds(np.hstack((title_len, body_len)), all_train_labels, kf, etc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('ROC AUC:', 0.58292748917748916)\n",
        "('ROC AUC:', 0.56032243618450517)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.57580712582064963)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.5562441655429935)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('ROC AUC:', 0.58838799544130671)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Mean: 0.572738, Median: 0.575807\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "[0.58292748917748916,\n",
        " 0.56032243618450517,\n",
        " 0.57580712582064963,\n",
        " 0.5562441655429935,\n",
        " 0.58838799544130671]"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}