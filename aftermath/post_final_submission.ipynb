{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project Submission\n",
    "### Ross Boberg, Sarah Neff, Sam Zaiss\n",
    "\n",
    "This notebook documents our exploration for the <a href=\"http://www.kaggle.com/c/random-acts-of-pizza\">Random Acts of Pizza</a> kaggle competition as part of the W207 Machine Learning course for UC Berkeley's MIDS program. We document the individual areas of exploration that we completed for this project, followed by the larger model that pulled these explorations together.\n",
    "\n",
    "### Problem Description\n",
    "The goal of the Random Acts of Pizza kaggle projects is to translate requests for pizza on the Reddit group \"Random Acts of Pizza\" in to predictions of whether or not they those requests are fulfilled. The data includes the text request, split in to the title and body of requests for pizza, as well as metadata about the request.\n",
    "\n",
    "These metadata include:\n",
    "<ul>\n",
    "<li>time of request (UTC and local)\n",
    "<li>numeric data about user activity:\n",
    "    <ul>\n",
    "    <li> 'requester_account_age_in_days_at_request'\n",
    "    <li> 'requester_days_since_first_post_on_raop_at_request'\n",
    "    <li> 'requester_number_of_comments_at_request'\n",
    "    <li> 'requester_number_of_comments_in_raop_at_request'\n",
    "    <li> 'requester_number_of_posts_at_request'\n",
    "    <li> 'requester_number_of_posts_on_raop_at_request'\n",
    "    <li> 'requester_number_of_subreddits_at_request'\n",
    "    <li> 'requester_upvotes_minus_downvotes_at_request'\n",
    "    <li> 'requester_upvotes_plus_downvotes_at_request'\n",
    "    </ul>\n",
    "<li>subreddit groups of the user</li>\n",
    "<li>Reddit user id</li>\n",
    "<li>request id to identify the request in submission</li>\n",
    "</ul>\n",
    "\n",
    "There are 4040 samples of requests in the exposed data, 994 of which were successful.\n",
    "Theare are 1631 samples of unlabeled test data, that Kaggle will test our predictions on\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "#### Table of Contents\n",
    "<ol>\n",
    "<li><a href=\"#part1\">Data Import and Base Methods</a></li>\n",
    "<li><a href=\"#part2\">Activity Features</a></li>\n",
    "<li><a href=\"#part3\">Text Bag of Words</a>\n",
    "<ul>\n",
    "<li>Simple</li>\n",
    "<li>L1 Feature Regularization</li>\n",
    "<li>Time</li><br/>\n",
    "</ul>\n",
    "</li>\n",
    "<li><a href=\"#part4\">Time Features</a></li>\n",
    "<li><a href=\"#part5\">Interesting Words &amp; Category Tags</a></li>\n",
    "<li><a href=\"#part6\">Request Quality</a></li>\n",
    "<li><a href=\"#part7\">Text Summary Features</a>\n",
    "</li>\n",
    "<li><a href=\"#part8\">Location Features</a></li>\n",
    "<li><a href=\"#part9\">Parts of Speech</a></li>\n",
    "<li><a href=\"#part10\">Subreddits</a></li>\n",
    "<li><a href=\"#part11\">Final, Composite Model</a></li>\n",
    "<br/>\n",
    "<li><a href=\"#part12\">Notes on Error Analysis</a></li>\n",
    "<li><a href=\"#part13\">Appendix - additional goodness</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "## 1. Data Import and Base Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "import datetime as dt\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import urlopen\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#useful for text processing\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Helper methods that will be used often in this notebook\n",
    "from datautil import load_json_file\n",
    "from datautil import make_submission_csv\n",
    "from datautil import name2index\n",
    "from mlutil import balance_samples\n",
    "from mlutil import oversample_kfold\n",
    "from mlutil import test_kfolds\n",
    "from mlutil import print_scores\n",
    "\n",
    "from tokenizers import LemmaTokenizer\n",
    "from tokenizers import SnowballStemTokenizer\n",
    "from tokenizers import PorterStemTokenizer\n",
    "from tokenizers import PuncTokenizer\n",
    "from tokenizers import SpaceTokenizer\n",
    "from tokenizers import PTBTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up the problem. We set the random seed to 207 (the class listing #!) so our results are replicable. We also shuffle our training data to make sure there are no issues of ordering that confuse the learning algorithms (e.g. all the postiive examples at the beginning). The training data has some variables that the submission data does not, so we make sure to ignore those because they will be useless for prediction.\n",
    "\n",
    "We also set up the K-folds we will use for cross validation in the rest of the notebook. We declare it now to allow us to get consistent results on our experiments. We use 10 fold validation (train our model on 90% of our data and testing it on the other 10%, on each non overlapping 10% of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datautil import load_raop_data\n",
    "\n",
    "### Set up the training and test data to work with throughout the notebook:\n",
    "rseed = 207\n",
    "np.random.seed(rseed)\n",
    "\n",
    "all_train_df, all_train_labels, submit_df = load_raop_data()\n",
    "\n",
    "# useful for sklearn scoring\n",
    "roc_scorer = make_scorer(roc_auc_score)\n",
    "n_all = all_train_df.shape[0]\n",
    "\n",
    "# set up kFolds to be used in the rest of the project\n",
    "kf = KFold(n_all, n_folds = 10, random_state=rseed)\n",
    "\n",
    "y = all_train_labels\n",
    "kf_over = oversample_kfold(kf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "## 2. Activity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import ExtractColumnsTransformer\n",
    "from transformers import ExtractActivities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model for testing the activities demonstrates the importance of accounting for unbalanced classes in this problem.\n",
    "With no adjustment, the model has zero value.\n",
    "\n",
    "We adjust via two methods, either the class_weight parameter if the estimator has it, which weights errors of the less frequent class higher ot make sure the model actually tries to predict them, or by oversampling the minority class in the training set. The function that adjusts the k-folds to do that is at the beginning of the notebook: oversample_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal Class Weights\n",
      "N: 10, Mean: 0.508836, Median: 0.509778, SD: 0.007667\n",
      "\n",
      "Reweighted Classes\n",
      "N: 10, Mean: 0.558866, Median: 0.556987, SD: 0.021346\n",
      "\n",
      "Rebalanced Sample\n",
      "N: 10, Mean: 0.555097, Median: 0.563723, SD: 0.024857\n"
     ]
    }
   ],
   "source": [
    "### Explore models using the activity features only.\n",
    "#Activities = ExtractActivities(all_train_df)\n",
    "\n",
    "# The main concern here is weighting classes appropriately, so we do an investigation of different\n",
    "# approaches and see how well the resulting model performs on 10 folds of the training data.\n",
    "\n",
    "print 'Equal Class Weights'\n",
    "#pipe = Pipeline([('activity',ExtractActivitiesOld()), ('scale', StandardScaler()),('svc', SVC(random_state=rseed))])\n",
    "#print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC(random_state=rseed))])\n",
    "print_scores(cross_val_score(pipe, all_train_df, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "\n",
    "print '\\nReweighted Classes'\n",
    "#wt_pipe = Pipeline([('activity',ExtractActivitiesOld()), ('scale', StandardScaler()),('svc', SVC(random_state=rseed, class_weight='auto'))])\n",
    "#print_scores(cross_val_score(wt_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "wt_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc', SVC(random_state=rseed, class_weight='auto'))])\n",
    "print_scores(cross_val_score(wt_pipe, all_train_df, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "print '\\nRebalanced Sample'\n",
    "rebal_pipe = Pipeline([('activity',ExtractActivities()), ('scale', StandardScaler()),('svc',SVC(random_state=rseed))])\n",
    "\n",
    "# oversample training data in kfolds - function defined above\n",
    "# necessary for some estimators that don't have class weight parameters\n",
    "kf_over = oversample_kfold(kf, all_train_labels)\n",
    "#print_scores(cross_val_score(rebal_pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer))\n",
    "print_scores(cross_val_score(rebal_pipe, all_train_df, all_train_labels, cv=kf_over, scoring=roc_scorer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Activity Features\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "## 3. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3a. Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(np.array([1,2]), '__iter__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define quick classes that we can use to isolate the title and body columns in our data.\n",
    "from transformers import ExtractBody, ExtractTitle, ExtractAllText, ExtractUser\n",
    "from transformers import ConcatStringTransformer, DesparseTransformer, TokenizeTransformer\n",
    "from transformers import TwitterPrep, WordvecTransformer, AverageWordvec, MaxPool, MinPool\n",
    "from transformers import PrepAndVectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#vectorize = Pipeline([('prep', TwitterPrep()),('tknzr', TokenizeTransformer(word_tokenize, rejoin_angle=True)),('wordvec', WordvecTransformer())])\n",
    "body_vecs = Pipeline([('body', ExtractBody()), ('vec', PrepAndVectorize(d=50))]).fit_transform(X=all_train_df,y=1)\n",
    "title_vecs = Pipeline([('title', ExtractTitle()), ('vec', PrepAndVectorize(d=50))]).fit_transform(X=all_train_df,y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "2deepnn_20160504.bin\n",
      "---\n",
      "2016-05-04_22:09:03\n",
      "---\n",
      "Experiment comparing a deeper NN (nnmx2 has a 2 pre-pooled layers and\n",
      "2 post-pool layers) to a shallower one (1 layer each). The deeper NN did not add\n",
      "value. The best learning rates for the deeper NN were .005 and .01, and it did better\n",
      "for higher hidden dimensions (100 was better than 50). Dropout improved the new NN\n",
      "a bit.\n",
      "*********\n",
      "*********\n",
      "alpha_comp_hdim100_dropp0.5_20160426.bin\n",
      "---\n",
      "2016-04-26_21:14:10\n",
      "---\n",
      "Experiment comparing five alphas between 1e-4 to 1e-1\n",
      "where hdim is 100 and drop prob is 50%. 10 kfolds, 50 epochs.\n",
      "*********\n",
      "*********\n",
      "anneal_alpha_test_hdim100_dropp50_20160426.bin\n",
      "---\n",
      "2016-04-26_21:32:41\n",
      "---\n",
      "Experiment comparing 4 annealing alpha schedules to the best\n",
      "static alpha from a previous experiment where hdim is 100 and drop prob is 50%.\n",
      "10 kfolds, 50 epochs. The best annealing strategy (starting alpha = 0.005) peaks\n",
      "around 61.2% median ROC after 120k iterations. This is a small improvement on \n",
      "a static alpha which peaked at 60.5% after 80k iterations. The best part of the annealing\n",
      "strategies is that they overfit less (which make sense because they use a smaller\n",
      "learning rate as time goes on)\n",
      "*********\n",
      "*********\n",
      "bigger_wvecs_100_and_200_orig_20160430.bin\n",
      "---\n",
      "2016-04-30_19:01:40\n",
      "---\n",
      "Compare bigger word vectors, 100 and 200 elements to the prevailing\n",
      "best model with 50 elements and hdim of 100. Compare hdim of 100 and 200. Tested\n",
      "with alpha = 0.005 and rho=1e-05. Overfitting seemed to be a problem with the\n",
      "higher dimension word vectors. So introducing higher regluarization might make sense\n",
      "*********\n",
      "*********\n",
      "first_minibatch_test_20160427.bin\n",
      "---\n",
      "2016-04-27_10:56:30\n",
      "---\n",
      "Experiment comparing minibatches of different sizes (1, 10, 20, 50, 100)\n",
      "for hdim=100, alpha = 0.005, dropp = 0.5. No minibatch (mb=1) was definitely best, and\n",
      "progressively larger minibatches were worse. It's possible that other choices might make\n",
      "minibatches more attractive, for examplie different alpha or equal-weighted class samples,\n",
      "might leave this for later exploration\n",
      "*********\n",
      "*********\n",
      "hdim_100vs200_dropout_rseed_20160427.bin\n",
      "---\n",
      "2016-04-27_08:47:29\n",
      "---\n",
      "Experiment comparing hdim 100 vs 200 and dropout probability of 0% vs 50%\n",
      "Also contains results for two different random seeds to see how that\n",
      "can affect results. 10 kfolds all\n",
      "*********\n",
      "*********\n",
      "rho_test_for_wdim_100_20160430.bin\n",
      "---\n",
      "2016-04-30_20:43:45\n",
      "---\n",
      "Experiment to test improvement in results for larger word vectors with\n",
      "higher regularization constant rho. The best results for wdim=100 did not beat\n",
      "the prevailing model with wdim=50, but improved a bit on the wdim=100 results with\n",
      "the regularization that worked for the smaller models of rho=1e-05\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "from nnutil import open_costs, cost_iter_summary, cost_iter_compare\n",
    "from nnutil import save_experiment, list_experiments, ppdf\n",
    "list_experiments(results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>id</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>n</th>\n",
       "      <th>std</th>\n",
       "      <th>context</th>\n",
       "      <th>hdim</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wdim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.447679</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>10</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511060957</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.462578</td>\n",
       "      <td>0.462561</td>\n",
       "      <td>10</td>\n",
       "      <td>0.030733</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511103609</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.467941</td>\n",
       "      <td>0.469774</td>\n",
       "      <td>10</td>\n",
       "      <td>0.040810</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511100706</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.507411</td>\n",
       "      <td>0.510058</td>\n",
       "      <td>10</td>\n",
       "      <td>0.034040</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511061003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.506471</td>\n",
       "      <td>0.511721</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511100659</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.522094</td>\n",
       "      <td>0.521876</td>\n",
       "      <td>10</td>\n",
       "      <td>0.030207</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511103613</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.535349</td>\n",
       "      <td>0.546553</td>\n",
       "      <td>10</td>\n",
       "      <td>0.030834</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511061008</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.555694</td>\n",
       "      <td>0.557475</td>\n",
       "      <td>10</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511060953</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.592073</td>\n",
       "      <td>0.591559</td>\n",
       "      <td>10</td>\n",
       "      <td>0.024977</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511060957</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.590114</td>\n",
       "      <td>0.592306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511100706</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.594159</td>\n",
       "      <td>0.592640</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511103613</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.590721</td>\n",
       "      <td>0.593776</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511061008</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.593041</td>\n",
       "      <td>0.596662</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027889</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511103609</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.592476</td>\n",
       "      <td>0.597163</td>\n",
       "      <td>10</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511100659</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.592915</td>\n",
       "      <td>0.600767</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511061003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.595915</td>\n",
       "      <td>0.606511</td>\n",
       "      <td>10</td>\n",
       "      <td>0.026133</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511060953</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.590374</td>\n",
       "      <td>0.589319</td>\n",
       "      <td>10</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511100706</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.590687</td>\n",
       "      <td>0.595654</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027563</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511061008</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.594940</td>\n",
       "      <td>0.596308</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511103609</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.595182</td>\n",
       "      <td>0.597239</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027684</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511100659</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.594157</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511060953</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.590623</td>\n",
       "      <td>0.598907</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511061003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.593777</td>\n",
       "      <td>0.599764</td>\n",
       "      <td>10</td>\n",
       "      <td>0.026711</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511060957</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>80000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.597589</td>\n",
       "      <td>0.601035</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511103613</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.589275</td>\n",
       "      <td>0.589831</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028171</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511061003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.599630</td>\n",
       "      <td>0.594074</td>\n",
       "      <td>10</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511103613</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.594471</td>\n",
       "      <td>0.596557</td>\n",
       "      <td>10</td>\n",
       "      <td>0.020254</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511100659</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.590256</td>\n",
       "      <td>0.596946</td>\n",
       "      <td>10</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511100706</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.597259</td>\n",
       "      <td>0.597516</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025320</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511060957</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.596497</td>\n",
       "      <td>0.598563</td>\n",
       "      <td>10</td>\n",
       "      <td>0.029216</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511103609</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.596149</td>\n",
       "      <td>0.599081</td>\n",
       "      <td>10</td>\n",
       "      <td>0.024729</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511061008</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>120000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.598211</td>\n",
       "      <td>0.609193</td>\n",
       "      <td>10</td>\n",
       "      <td>0.027117</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511060953</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.599772</td>\n",
       "      <td>0.593250</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028882</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511103613</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.594133</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511061003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.594528</td>\n",
       "      <td>0.595453</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028247</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511061008</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.596033</td>\n",
       "      <td>0.596866</td>\n",
       "      <td>10</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511100659</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.593575</td>\n",
       "      <td>0.596970</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511100706</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.600140</td>\n",
       "      <td>0.600469</td>\n",
       "      <td>10</td>\n",
       "      <td>0.032578</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511103609</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.597678</td>\n",
       "      <td>0.600741</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511060957</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>160000</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.602562</td>\n",
       "      <td>0.605638</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025224</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511060953</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.582628</td>\n",
       "      <td>0.581470</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021405</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511061003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.593674</td>\n",
       "      <td>0.586105</td>\n",
       "      <td>10</td>\n",
       "      <td>0.035776</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511103613</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.597989</td>\n",
       "      <td>0.595719</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023673</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511100659</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.602486</td>\n",
       "      <td>0.599074</td>\n",
       "      <td>10</td>\n",
       "      <td>0.022338</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511060953</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.594222</td>\n",
       "      <td>0.600209</td>\n",
       "      <td>10</td>\n",
       "      <td>0.029111</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>20160511103609</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.593078</td>\n",
       "      <td>0.601719</td>\n",
       "      <td>10</td>\n",
       "      <td>0.024881</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>20160511100706</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.602977</td>\n",
       "      <td>0.602185</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511060957</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>181800</td>\n",
       "      <td>drop_p=0.5_rseed=207_printevery=40000.0_anneal...</td>\n",
       "      <td>0.599037</td>\n",
       "      <td>0.603478</td>\n",
       "      <td>10</td>\n",
       "      <td>0.029550</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>20160511061008</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count                                                 id      mean  \\\n",
       "30       0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.447679   \n",
       "42       0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.462578   \n",
       "0        0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.467941   \n",
       "12       0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.507411   \n",
       "6        0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.506471   \n",
       "36       0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.522094   \n",
       "18       0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.535349   \n",
       "24       0  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.555694   \n",
       "31   40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.592073   \n",
       "1    40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.590114   \n",
       "37   40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.594159   \n",
       "19   40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.590721   \n",
       "43   40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.593041   \n",
       "7    40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.592476   \n",
       "13   40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.592915   \n",
       "25   40000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.595915   \n",
       "2    80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.590374   \n",
       "20   80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.590687   \n",
       "44   80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.594940   \n",
       "8    80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.595182   \n",
       "26   80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.594157   \n",
       "14   80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.590623   \n",
       "32   80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.593777   \n",
       "38   80000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.597589   \n",
       "15  120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.589275   \n",
       "39  120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.599630   \n",
       "9   120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.594471   \n",
       "3   120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.590256   \n",
       "33  120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.597259   \n",
       "45  120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.596497   \n",
       "21  120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.596149   \n",
       "27  120000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.598211   \n",
       "40  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.599772   \n",
       "16  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.587427   \n",
       "22  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.594528   \n",
       "10  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.596033   \n",
       "4   160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.593575   \n",
       "46  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.600140   \n",
       "34  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.597678   \n",
       "28  160000  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.602562   \n",
       "17  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.582628   \n",
       "41  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.593674   \n",
       "11  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.597989   \n",
       "29  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.602486   \n",
       "47  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.594222   \n",
       "5   181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.593078   \n",
       "35  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.602977   \n",
       "23  181800  drop_p=0.5_rseed=207_printevery=40000.0_anneal...  0.599037   \n",
       "\n",
       "      median   n       std context hdim       timestamp wdim  \n",
       "30  0.450549  10  0.033345       1  200  20160511060957   50  \n",
       "42  0.462561  10  0.030733       2  150  20160511103609   50  \n",
       "0   0.469774  10  0.040810       0  100  20160511100706   50  \n",
       "12  0.510058  10  0.034040       1  100  20160511061003  100  \n",
       "6   0.511721  10  0.028924       0  150  20160511100659   50  \n",
       "36  0.521876  10  0.030207       2  100  20160511103613   50  \n",
       "18  0.546553  10  0.030834       1  200  20160511061008  100  \n",
       "24  0.557475  10  0.016426       1  150  20160511060953   50  \n",
       "31  0.591559  10  0.024977       1  200  20160511060957   50  \n",
       "1   0.592306  10  0.022038       0  100  20160511100706   50  \n",
       "37  0.592640  10  0.027497       2  100  20160511103613   50  \n",
       "19  0.593776  10  0.025590       1  200  20160511061008  100  \n",
       "43  0.596662  10  0.027889       2  150  20160511103609   50  \n",
       "7   0.597163  10  0.022549       0  150  20160511100659   50  \n",
       "13  0.600767  10  0.027273       1  100  20160511061003  100  \n",
       "25  0.606511  10  0.026133       1  150  20160511060953   50  \n",
       "2   0.589319  10  0.020817       0  100  20160511100706   50  \n",
       "20  0.595654  10  0.027563       1  200  20160511061008  100  \n",
       "44  0.596308  10  0.025404       2  150  20160511103609   50  \n",
       "8   0.597239  10  0.027684       0  150  20160511100659   50  \n",
       "26  0.597547  10  0.023611       1  150  20160511060953   50  \n",
       "14  0.598907  10  0.025700       1  100  20160511061003  100  \n",
       "32  0.599764  10  0.026711       1  200  20160511060957   50  \n",
       "38  0.601035  10  0.025397       2  100  20160511103613   50  \n",
       "15  0.589831  10  0.028171       1  100  20160511061003  100  \n",
       "39  0.594074  10  0.026008       2  100  20160511103613   50  \n",
       "9   0.596557  10  0.020254       0  150  20160511100659   50  \n",
       "3   0.596946  10  0.020039       0  100  20160511100706   50  \n",
       "33  0.597516  10  0.025320       1  200  20160511060957   50  \n",
       "45  0.598563  10  0.029216       2  150  20160511103609   50  \n",
       "21  0.599081  10  0.024729       1  200  20160511061008  100  \n",
       "27  0.609193  10  0.027117       1  150  20160511060953   50  \n",
       "40  0.593250  10  0.028882       2  100  20160511103613   50  \n",
       "16  0.594133  10  0.028867       1  100  20160511061003  100  \n",
       "22  0.595453  10  0.028247       1  200  20160511061008  100  \n",
       "10  0.596866  10  0.018319       0  150  20160511100659   50  \n",
       "4   0.596970  10  0.021544       0  100  20160511100706   50  \n",
       "46  0.600469  10  0.032578       2  150  20160511103609   50  \n",
       "34  0.600741  10  0.023623       1  200  20160511060957   50  \n",
       "28  0.605638  10  0.025224       1  150  20160511060953   50  \n",
       "17  0.581470  10  0.021405       1  100  20160511061003  100  \n",
       "41  0.586105  10  0.035776       2  100  20160511103613   50  \n",
       "11  0.595719  10  0.023673       0  150  20160511100659   50  \n",
       "29  0.599074  10  0.022338       1  150  20160511060953   50  \n",
       "47  0.600209  10  0.029111       2  150  20160511103609   50  \n",
       "5   0.601719  10  0.024881       0  100  20160511100706   50  \n",
       "35  0.602185  10  0.025962       1  200  20160511060957   50  \n",
       "23  0.603478  10  0.029550       1  200  20160511061008  100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "likes = ['.*20160511.*']\n",
    "\n",
    "results = cost_iter_compare(likes=likes, verbose=False).sort(['count', 'median'])\n",
    "ppdf(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>annealevery</th>\n",
       "      <th>drop_p</th>\n",
       "      <th>hdim</th>\n",
       "      <th>model</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN1</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>CNN2</td>\n",
       "      <td>1e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha annealevery drop_p hdim model     rho\n",
       "0   0.005           0    0.0  100  CNN1  0.0001\n",
       "1   0.005           0    0.5  100  CNN1   1e-05\n",
       "2    0.01           0    0.5  100  CNN1   1e-05\n",
       "3   0.005          10    0.5  100  CNN1   1e-05\n",
       "4    0.01          10    0.5  100  CNN1   1e-05\n",
       "5   0.005           0    0.5   50  CNN2   1e-05\n",
       "6   0.005           0    0.5   50  CNN1   1e-05\n",
       "7   0.005          10    0.5   50  CNN1   1e-05\n",
       "8    0.01          10    0.5   50  CNN1   1e-05\n",
       "9   0.005           0    0.5   50  CNN2   1e-08\n",
       "10  0.005           0    0.5  100  CNN2   1e-05\n",
       "11   0.01           0    0.5  100  CNN2   1e-05\n",
       "12  0.005           0    0.0   50  CNN1  0.0001\n",
       "13  0.005           0    0.5  100  CNN2   1e-08\n",
       "14  0.005           0    0.5   50  CNN2   1e-05\n",
       "15  0.005           0    0.0   50  CNN1  0.0001\n",
       "16  0.005          10    0.5  100  CNN1   1e-05\n",
       "17  0.005           0    0.5  100  CNN1   1e-05\n",
       "18  0.005           0    0.0  100  CNN1  0.0001\n",
       "19   0.01           0    0.5  100  CNN1   1e-05\n",
       "20   0.01          10    0.5  100  CNN1   1e-05\n",
       "21  0.005           0    0.5  100  CNN2   1e-08\n",
       "22  0.005           0    0.5  100  CNN2   1e-05\n",
       "23  0.005          10    0.5   50  CNN1   1e-05\n",
       "24   0.01          10    0.5   50  CNN1   1e-05\n",
       "25   0.01           0    0.5  100  CNN2   1e-05\n",
       "26  0.005           0    0.5   50  CNN2   1e-08\n",
       "27  0.005           0    0.5   50  CNN1   1e-05\n",
       "28  0.005           0    0.5   50  CNN2   1e-08\n",
       "29  0.005           0    0.5   50  CNN2   1e-05\n",
       "..    ...         ...    ...  ...   ...     ...\n",
       "54   0.01          10    0.5   50  CNN1   1e-05\n",
       "55  0.005          10    0.5   50  CNN1   1e-05\n",
       "56  0.005           0    0.0  100  CNN1  0.0001\n",
       "57  0.005           0    0.0   50  CNN1  0.0001\n",
       "58  0.005           0    0.5   50  CNN1   1e-05\n",
       "59   0.01          10    0.5   50  CNN1   1e-05\n",
       "60   0.01           0    0.5  100  CNN1   1e-05\n",
       "61  0.005           0    0.5   50  CNN2   1e-05\n",
       "62  0.005           0    0.5   50  CNN2   1e-08\n",
       "63  0.005           0    0.5  100  CNN2   1e-08\n",
       "64  0.005          10    0.5   50  CNN1   1e-05\n",
       "65  0.005           0    0.5  100  CNN2   1e-05\n",
       "66   0.01           0    0.5  100  CNN2   1e-05\n",
       "67  0.005          10    0.5  100  CNN1   1e-05\n",
       "68  0.005           0    0.5  100  CNN1   1e-05\n",
       "69   0.01          10    0.5  100  CNN1   1e-05\n",
       "70  0.005           0    0.0  100  CNN1  0.0001\n",
       "71  0.005           0    0.0   50  CNN1  0.0001\n",
       "72  0.005           0    0.5   50  CNN1   1e-05\n",
       "73   0.01           0    0.5  100  CNN1   1e-05\n",
       "74   0.01           0    0.5  100  CNN2   1e-05\n",
       "75  0.005           0    0.5  100  CNN2   1e-05\n",
       "76  0.005           0    0.5  100  CNN2   1e-08\n",
       "77   0.01          10    0.5   50  CNN1   1e-05\n",
       "78   0.01          10    0.5  100  CNN1   1e-05\n",
       "79  0.005           0    0.5  100  CNN1   1e-05\n",
       "80  0.005           0    0.5   50  CNN2   1e-05\n",
       "81  0.005          10    0.5  100  CNN1   1e-05\n",
       "82  0.005          10    0.5   50  CNN1   1e-05\n",
       "83  0.005           0    0.5   50  CNN2   1e-08\n",
       "\n",
       "[84 rows x 6 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likes = []\n",
    "likes += ['(?=model=nnmx.*wdim=50_hdim=100.*).*alpha=0\\.005.*rho=1e-05.*dropp=0\\.5_rseed=207.*mb=False.*alphaiter=default.*devlen=404.*']\n",
    "likes += ['(?=model=nnmx2.*hdim=50.*alpha=0\\.005.*rho=0\\.0001).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=50.*alpha=0\\.01.*rho=0\\.001).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=50.*alpha=0\\.001.*rho=0\\.001).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=50.*alpha=0\\.005.*rho=1e-05).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=50.*alpha=0\\.005.*rho=1e-06).*dropp=0\\.5_rseed=209']\n",
    "\n",
    "likes += ['(?=model=nnmx2.*hdim=100.*alpha=0\\.005.*rho=0\\.0001).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=100.*alpha=0\\.01.*rho=0\\.001).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=100.*alpha=0\\.001.*rho=0\\.001).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=100.*alpha=0\\.005.*rho=1e-05).*dropp=0\\.5_rseed=209']\n",
    "likes += ['(?=model=nnmx2.*hdim=100.*alpha=0\\.005.*rho=1e-06).*dropp=0\\.5_rseed=209']\n",
    "\n",
    "likes += ['(?=.*dropp=0\\.0.*)model=nnmx2.*hdim=100.*alpha=0\\.005.*rho=0\\.0001.*_rseed=209']\n",
    "likes += ['(?=.*dropp=0\\.0.*)model=nnmx2.*hdim=100.*alpha=0\\.01.*rho=0\\.001.*_rseed=209']\n",
    "likes += ['(?=.*dropp=0\\.0.*)model=nnmx2.*hdim=100.*alpha=0\\.001.*rho=0\\.001.*_rseed=209']\n",
    "likes += ['(?=.*dropp=0\\.0.*)(?=model=nnmx2.*hdim=100.*alpha=0\\.005.*rho=1e-05).*rseed=209']\n",
    "likes += ['(?=.*dropp=0\\.0.*)(?=model=nnmx2.*hdim=100.*alpha=0\\.005.*rho=1e-06).*rseed=209']\n",
    "\n",
    "results = cost_iter_compare(likes=likes, verbose=False).sort(['count', 'median'])\n",
    "\n",
    "\n",
    "#save_experiment(results.sort(['count','median']), '2deepnn',\n",
    "#                \"\"\"Experiment comparing a deeper NN (nnmx2 has a 2 pre-pooled layers and\n",
    "#2 post-pool layers) to a shallower one (1 layer each). The deeper NN did not add\n",
    "#value. The best learning rates for the deeper NN were .005 and .01, and it did better\n",
    "#for higher hidden dimensions (100 was better than 50). Dropout improved the new NN\n",
    "#a bit.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results[results['count']==120000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likes = []\n",
    "likes += ['(?=.*wdim=50_hdim=100.*).*alpha=0\\.005.*rho=1e-05.*dropp=0\\.5_rseed=207.*mb=False.*alphaiter=default.*devlen=404.*']\n",
    "likes += ['(?=.*rho=1e-05.*).*wdim=100_hdim=100.*']\n",
    "likes += ['(?=.*rho=0\\.0001.*).*wdim=100_hdim=100.*']\n",
    "likes += ['(?=.*rho=0\\.001.*).*wdim=100_hdim=100.*']\n",
    "likes += ['(?=.*rho=1e-05.*).*wdim=100_hdim=200.*']\n",
    "likes += ['(?=.*rho=0\\.0001.*).*wdim=100_hdim=200.*']\n",
    "likes += ['(?=.*rho=0\\.001.*).*wdim=100_hdim=200.*']\n",
    "\n",
    "results = cost_iter_compare(likes=likes, verbose=False).sort(['count', 'median'])\n",
    "\n",
    "# save_experiment(results.sort(['count','median']), 'rho_test_for_wdim_100',\n",
    "#                \"\"\"Experiment to test improvement in results for larger word vectors with\n",
    "# higher regularization constant rho. The best results for wdim=100 did not beat\n",
    "# the prevailing model with wdim=50, but improved a bit on the wdim=100 results with\n",
    "# the regularization that worked for the smaller models of rho=1e-05\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "likes = []\n",
    "likes += ['(?=.*alpha=0\\.005.*alphaiter=default).*hdim=100.*dropp=0\\.5_rseed=207.*devlen=404.*']\n",
    "likes += ['(?=.*alpha=0\\.005.*alphaiter=anneal18180).*hdim=100.*dropp=0\\.5_rseed=207.*devlen=404.*']\n",
    "likes += ['(?=.*alpha=0\\.01.*alphaiter=anneal18180).*hdim=100.*dropp=0\\.5_rseed=207.*devlen=404.*']\n",
    "likes += ['(?=.*alpha=0\\.005.*alphaiter=anneal36360).*hdim=100.*dropp=0\\.5_rseed=207.*devlen=404.*']\n",
    "likes += ['(?=.*alpha=0\\.005.*alphaiter=anneal72720).*hdim=100.*dropp=0\\.5_rseed=207.*devlen=404.*']\n",
    "\n",
    "results = cost_iter_compare(likes=likes, verbose=False)\n",
    "results.sort(['count','median'])\n",
    "\n",
    "#save_experiment(results.sort(['count','median']), 'anneal_alpha_test_hdim100_dropp50',\n",
    "#                \"\"\"Experiment comparing 4 annealing alpha schedules to the best\n",
    "#static alpha from a previous experiment where hdim is 100 and drop prob is 50%.\n",
    "#10 kfolds, 50 epochs. The best annealing strategy (starting alpha = 0.005) peaks\n",
    "#around 61.2% median ROC after 120k iterations. This is a small improvement on \n",
    "#a static alpha which peaked at 60.5% after 80k iterations. The best part of the annealing\n",
    "#strategies is that they overfit less (which make sense because they use a smaller\n",
    "#learning rate as time goes on)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_experiments(results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MaxPool().fit_transform(X=body_vecs, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MaxPool().fit_transform(X=title_vecs, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(class_weight='auto', C = 2, random_state=rseed)\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "gbc = GradientBoostingClassifier(n_estimators = 200,\n",
    "                            learning_rate=0.01,\n",
    "                           max_depth = 3,\n",
    "                           min_samples_split=10,\n",
    "                           random_state = rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_avg = AverageWordvec().fit_transform(title_vecs, y=1)\n",
    "body_avg = AverageWordvec().fit_transform(body_vecs, y=1)\n",
    "title_max = MaxPool().fit_transform(title_vecs, y=1)\n",
    "body_max = MaxPool().fit_transform(body_vecs, y=1)\n",
    "title_min = MinPool().fit_transform(title_vecs, y=1)\n",
    "body_min = MinPool().fit_transform(body_vecs, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate((title_avg, body_avg),axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('wvec', AverageWordvec()),('lsvc', lsvc)]), title_vecs, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('wvec', AverageWordvec()),('lsvc', lsvc)]), body_vecs, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('lsvc', lsvc)]), np.concatenate((title_avg, body_avg),axis=1), all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('wvec', AverageWordvec()),('etc', etc)]), title_vecs, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('wvec', AverageWordvec()),('etc', etc)]), body_vecs, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('etc', etc)]), np.concatenate((title_avg, body_avg),axis=1), all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('wvec', MaxPool()),('etc', etc)]), body_vecs, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('wvec',FeatureUnion([('max', MaxPool()), ('min', MinPool())])),('etc', etc)]), body_vecs, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_scores(cross_val_score(Pipeline([('etc', etc)]), np.concatenate((title_max, body_max, title_min, body_min),axis=1), all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO / Notes\n",
    "<ul>\n",
    "<li> DONE Look at success of title + body - not a major determinant of success or not\n",
    "<li> DONE Try word vec vars with L1 regularization - did not add much value over body max + min with ETC\n",
    "<li> DONE Implement a nonlinear layer over each word vector then max pool the results of that layer then pass to softmax / LR layer - Ran on One Train/Test split and there seems to be significant improvement peaking at ROC AUC of 0.619 around 50 - 70 epochs.\n",
    "<li> DONE Script to save experiments. Goal - recover inputs to experiments and results. Execution - save final NN, any training inputs, and occasional results on train and test.\n",
    "<li> DONE Try NN on all k-folds - not an improvement over bag of words - best median kfold was about a ROC AUC of 0.58 after about 40k training steps\n",
    "<li> DONE: add regularization\n",
    "<li> DONE: train and test kfolds with regularizations - regularizaiton with rho = 1e-3 is a slight improvement, leading to mean/median ROC AUC of about 0.584 after 40k training steps instead of 0.58\n",
    "<li> DONE: add dropout - after adding dropout and 200 dimension hidden layer, I got a median ROC AUC of 60 (after 40k examples), but the 0th iteration ROC AUC was 57 which is confusing - maybe just a really good intial weight configuration?\n",
    "<li> DONE: try 200 dim hidden layer without drop out and 100 dim hidden layer with drop out for comparison - 100 dim hidden layer with drop out seemed to be best\n",
    "<li> DONE: try changing the random seed and re running hdim 200 with drop out to see if results differ - indeed results were worse. with the original random seed (207) roc auc plateaued around 60 after 40k-80k examples. with another random seed (414) roc auc plateaued around 58.2 after 40k-80k examples. I think the problem is random weight initialization. Another symptom of this problem is that iteration 0 scores for train set differed in a big way between random seeds - suggsets to me that random weights were too high after increasing number of dimensions. Trying with new random weight scheme (fan in / fan out adjusted instead of just a random number).\n",
    "<li> DONE: random weight adjusted by fan in / fan out - dev set results seems to fluctuate less given differnt random seed after changing this, results slightly more consistent. seed 207 still plateaus around 60 between 40k-80k. same for seed 414.\n",
    "<li> DONE: minibatch training - No minibatch (mb=1) was definitely best, and progressively larger minibatches were worse. It's possible that other choices might make minibatches more attractive, for examplie different alpha or equal-weighted class samples, might leave this for later exploration.\n",
    "<li> DONE: try different learning rates and annealed learning. I investigated different learning rats and annealed learning. The best static rates are between 0.005 and 0.001. Annealed learning works best starting at 0.005 and annealing every 10 epochs or so.\n",
    "<li> DONE: try bigger word vectors - did not add any value. higher regularization improved results a bit for bigger word vectors. did not try different learning rates.\n",
    "<li> TODO: add more convolution (bi grams etc)\n",
    "<li> DONE: deeper layers before max pooling\n",
    "<li> TODO: dropout softmax layer too\n",
    "<li> DONE: deeper layers after max pooling\n",
    "<li> DONE: IMplement CNN\n",
    "<li> TODO: Test CNN\n",
    "<li> TODO: Improve testing so that all kfolds are saved in a single folder which gets around the painful regex I currently do. Folder name should be the opts that were called and have the time at the end. This makes it easy to group things that were called together\n",
    "<li> TODO: Implement RCNN in Lai Et.Al 2015\n",
    "<li> NOTE: Overfitting is not a problem when there are two hidden layers before max pooling. The train set ROC AUC seems to max out around 60, whether I use drop out or not. If I drop one of those hidden layers it becomes a problem again (especially if no dropout), even if I keep an extra hidden layer after max pooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3574533 , -0.01407516, -0.38890564, -0.2832947 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.13978047,  0.80192316,\n",
       "         1.22392316,  1.40596724,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.9315785 ,  2.13177116,  1.68812958,  1.41823789],\n",
       "       [ 1.13978047,  0.80192316,  1.22392316,  1.40596724, -0.3574533 ,\n",
       "        -0.01407516, -0.38890564, -0.2832947 ,  1.9315785 ,  2.13177116,\n",
       "         1.68812958,  1.41823789,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  2.98767746,  3.09027405,  3.23420206,  3.36251272],\n",
       "       [ 1.9315785 ,  2.13177116,  1.68812958,  1.41823789,  1.13978047,\n",
       "         0.80192316,  1.22392316,  1.40596724,  2.98767746,  3.09027405,\n",
       "         3.23420206,  3.36251272, -0.3574533 , -0.01407516, -0.38890564,\n",
       "        -0.2832947 ,  3.40379251,  4.02647582,  3.6753537 ,  4.21780272],\n",
       "       [ 2.98767746,  3.09027405,  3.23420206,  3.36251272,  1.9315785 ,\n",
       "         2.13177116,  1.68812958,  1.41823789,  3.40379251,  4.02647582,\n",
       "         3.6753537 ,  4.21780272,  1.13978047,  0.80192316,  1.22392316,\n",
       "         1.40596724,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 3.40379251,  4.02647582,  3.6753537 ,  4.21780272,  2.98767746,\n",
       "         3.09027405,  3.23420206,  3.36251272,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.9315785 ,  2.13177116,  1.68812958,\n",
       "         1.41823789,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([np.random.normal(i,0.25,4) for i in range(5)])\n",
    "wdim = X.shape[1]\n",
    "for i in range(2):\n",
    "    X = np.hstack((X,\n",
    "                        np.vstack((np.zeros((i+1,wdim)), X[:-(i+1),:wdim])),\n",
    "                        np.vstack((X[(i+1):,:wdim], np.zeros((i+1,wdim))))\n",
    "                        ))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a=1_b=2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_'.join(['{k}={v}'.format(k=k,v=v) for k,v in {'a':1,'b':2}.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenvec('mum',topdir,dirdepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Reusable method for quick BOW investigations:\n",
    "def simple_text(do_all=True, do_count=False,do_tfidf=False, do_titles=False, do_bodies=False, do_both=False, lowercase=False, tokenizer=None, stop_words=None):\n",
    "\n",
    "    # Notes\n",
    "    # results slightly better w/ lowercase = False (when unigrams only)\n",
    "    # bigrams added no value on unigrams\n",
    "\n",
    "    tv = TfidfVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
    "    cv = CountVectorizer(ngram_range=(1,1),lowercase=lowercase, tokenizer=tokenizer, stop_words=stop_words)\n",
    "    lsvc = LinearSVC(class_weight='auto', C = 2, random_state=rseed)\n",
    "    \n",
    "    body_cv = Pipeline([('body',ExtractBody()),('cv', cv)])\n",
    "    body_tv = Pipeline([('body',ExtractBody()),('tv', tv)])\n",
    "    \n",
    "    title_cv = Pipeline([('title',ExtractTitle()),('cv', cv)])\n",
    "    title_tv = Pipeline([('title',ExtractTitle()),('tv', tv)])\n",
    "\n",
    "    if do_titles or do_all:\n",
    "        if do_count or do_all:\n",
    "            # Count Vectorizer Titles\n",
    "            print '\\nCount Vectorizer on Titles'\n",
    "            \n",
    "            pipe = Pipeline([('tranform',title_cv),('model',lsvc)])\n",
    "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "        if do_tfidf or do_all:\n",
    "            # TFIDF Vectorizer TItles\n",
    "            print '\\nTFIDF Vectorizer on Titles'\n",
    "            \n",
    "            pipe = Pipeline([('tranform',title_tv),('model',lsvc)])\n",
    "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "    if do_bodies or do_all:\n",
    "        if do_count or do_all:\n",
    "            # Count Vectorizer Bodies\n",
    "            print '\\nCount Vectorizer on Bodies'\n",
    "            \n",
    "            pipe = Pipeline([('tranform',body_cv),('model',lsvc)])\n",
    "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "        if do_tfidf or do_all:\n",
    "            # TFIDF Vectorizer Bodies\n",
    "            print '\\nTFIDF Vectorizer on Bodies'\n",
    "            \n",
    "            pipe = Pipeline([('tranform',body_tv),('model',lsvc)])\n",
    "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "        \n",
    "    if do_both or do_all:\n",
    "        if do_count or do_all:\n",
    "\n",
    "            # Count Vectorizer Titles and Bodies\n",
    "            print '\\nCount Vectorizer on Titles and Bodies'\n",
    "            \n",
    "            pipe = Pipeline([\n",
    "                ('features',FeatureUnion([\n",
    "                    ('tranform_title',title_cv),\n",
    "                    ('tranform_body',body_cv)\n",
    "                ])),\n",
    "                ('model',lsvc)])\n",
    "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "            \n",
    "        if do_tfidf or do_all:\n",
    "            # TFIDF Vectorizer Titles and Bodies\n",
    "            print '\\nTFIDF Vectorizer on Titles and Bodies'\n",
    "            \n",
    "            pipe = Pipeline([\n",
    "                ('features',FeatureUnion([\n",
    "                    ('tranform_title',title_tv),\n",
    "                    ('tranform_body',body_tv)\n",
    "                ])),\n",
    "                ('model',lsvc)])\n",
    "            print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our text model counts how many times each word in a vocabulary (learned from the text) is present in each request. We adjust that term frequency by inverse document frequency (how often that term occurs throughout the document) to overweight uncommon words, which often have more explanatory power.\n",
    "\n",
    "We also play around with a few different tokenizers to see which get the best results. These tokenizers turn lots of text (like pizza requests) in to a series of words (tokens). The tokens can be \"stemmed\" which adjusts the word so that, for example, different tenses of the same verb have the same representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Experimentation with different tokenizers\n",
    "\n",
    "print \"Examination of best vectorizer:\"\n",
    "\n",
    "print '\\nDefault Vectorizer:'\n",
    "simple_text(do_all = False, do_both=True, do_tfidf=True, do_count=True)\n",
    "\n",
    "print '=================='\n",
    "\n",
    "print 'Examination of best tokenizer'\n",
    "\n",
    "print \"\\nSnowball Stem Tokenizer:\"\n",
    "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=SnowballStemTokenizer())\n",
    "\n",
    "print \"\\Lemma Tokenizer:\"\n",
    "simple_text(do_all=False, do_both=True, do_tfidf=True, tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Simple Bag of Words\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. L1 Feature Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Reusable class to process important terms\n",
    "class LinearWeightFeatureThreshold(TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model = LinearSVC(class_weight='auto', loss='squared_hinge', penalty='l1', dual=False, random_state=rseed),\n",
    "        return_dense = True, #dense or sparse matrix\n",
    "        C = 1, # C for L1\n",
    "        threshold = 0.01, # threshold to keep\n",
    "        verbose = 1 #tell how many features were kept\n",
    "        ):\n",
    "        self.model = model\n",
    "        self.return_dense = return_dense\n",
    "        self.C = C\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        model = self.model\n",
    "        threshold = self.threshold\n",
    "        verbose = self.verbose\n",
    "        C = self.C\n",
    "        \n",
    "        model.set_params(C=C)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # check which coefficients to keep\n",
    "        coef = model.coef_\n",
    "        sig_coef = (np.abs(coef) > threshold)[0]\n",
    "        n_coef = np.sum(sig_coef)\n",
    "        \n",
    "        \n",
    "        if verbose > 0:\n",
    "            print 'kept %d/%d features' % (n_coef, coef.shape[1])\n",
    "        \n",
    "        # so we never return an empty vector if C was too low\n",
    "        if n_coef == 0:\n",
    "            sig_coef[0] = 1\n",
    "        \n",
    "        # save the significant coefficients\n",
    "        self.sig_coef_  = sig_coef\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        sig_coef = self.sig_coef_\n",
    "        return_dense = self.return_dense\n",
    "        \n",
    "        X_new = X[:,sig_coef]\n",
    "        \n",
    "        if return_dense and (type(X_new) != type(np.array(1))):\n",
    "            X_new = X_new.toarray()\n",
    "            \n",
    "        return X_new\n",
    "    \n",
    "    # methods needed to make this grid searchable\n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {'C':self.C, 'threshold':self.threshold}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text models above result in lots of variables, many of which will be completely useless. This noise can overwhelm the models, so we want to find ways to \"regularize\" or reduce the number of variables to only significant ones. We do this by \"L1\" regularization. L1 regularizaiton uses a model that calculates linear errors, which happens to result in a sparse number of variables actually used. We take the variables that were used in this sparser model, then feed them in to a model that uses squared errors, to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found L1 regularization with a Linear SVC to be an effective method for reducing the number features, especially coming out of term frequency matrices. We chose C=0.15 ('regularization' term that controls how sparse the variables in the model are) via grid search (by trying lots of different values and seeing what does best), and it tends to return about 40 features on our sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Try ExtraTreesClassifier for the BOW models:\n",
    "\n",
    "# C=0.15 arrived at via grid search, but took a long time so not included here.\n",
    "l1 = LinearWeightFeatureThreshold(C=0.15)\n",
    "tv = TfidfVectorizer(tokenizer=SnowballStemTokenizer())\n",
    "lsvc = LinearSVC(class_weight='auto', C = 2, random_state=rseed)\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "pipe_lsvc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',lsvc)])\n",
    "pipe_etc = Pipeline([('extract', ExtractBody()), ('tv',tv), ('features',l1), ('clf',etc)])\n",
    "\n",
    "\n",
    "print '\\nL1 Feature Reduction on Bodies w/ LSVC'\n",
    "print_scores(cross_val_score(pipe_lsvc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print '\\nL1 Feature Reduction on Bodies w/ ETC'\n",
    "tv = TfidfVectorizer(tokenizer=SnowballStemTokenizer())\n",
    "lsvc = LinearSVC(class_weight='auto', C = 2, random_state=rseed)\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "print_scores(cross_val_score(pipe_etc, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Bag of Words w/ Feature Reduction\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "## 4. Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Reusable class for time features\n",
    "### Reusable class for time features\n",
    "DATE_TIME_COLUMN_DEFAULT = np.where(all_train_df.columns == 'unix_timestamp_of_request')[0][0]\n",
    "\n",
    "class TimeTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, date_time_column=DATE_TIME_COLUMN_DEFAULT, do_second=True, do_minute=True, do_hour=True, do_dow=True, do_day=True, do_month=True):\n",
    "        self.date_time_column = date_time_column\n",
    "        self.do_second = do_second\n",
    "        self.do_minute = do_minute\n",
    "        self.do_hour = do_hour\n",
    "        self.do_dow = do_dow\n",
    "        self.do_day = do_day\n",
    "        self.do_month = do_month\n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def extract_from_date_time_(self, dt, do_second, do_minute, do_hour, do_dow, do_day, do_month):\n",
    "        extract = []\n",
    "        if do_second:\n",
    "            extract.append(dt.second)\n",
    "        \n",
    "        if do_minute:\n",
    "            extract.append(dt.minute)\n",
    "            \n",
    "        if do_hour:\n",
    "            extract.append(dt.hour)\n",
    "            \n",
    "        if do_dow:\n",
    "            extract.append(dt.weekday())\n",
    "            \n",
    "        if do_day:\n",
    "            extract.append(dt.day)\n",
    "            \n",
    "        if do_month:\n",
    "            extract.append(dt.month)\n",
    "            \n",
    "        return extract\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        date_time_column = self.date_time_column\n",
    "        do_second = self.do_second\n",
    "        do_minute = self.do_minute\n",
    "        do_hour = self.do_hour\n",
    "        do_dow = self.do_dow\n",
    "        do_day = self.do_day\n",
    "        do_month = self.do_month\n",
    "        extract_from_date_time = self.extract_from_date_time_\n",
    "        \n",
    "        features = np.array([\n",
    "            extract_from_date_time(dt.datetime.fromtimestamp(timei),\n",
    "                                   do_second=do_second,\n",
    "                                   do_minute=do_minute,\n",
    "                                   do_hour=do_hour,\n",
    "                                   do_dow=do_dow,\n",
    "                                   do_day=do_day,\n",
    "                                   do_month=do_month) for timei in X[:,date_time_column]\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'do_second':self.do_second,\n",
    "                'do_minute':self.do_minute,\n",
    "                'do_hour':self.do_hour,\n",
    "                'do_dow':self.do_dow,\n",
    "                'do_day':self.do_day,\n",
    "                'do_month':self.do_month}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Time Variables\n",
    "\n",
    "We create several features derived from the date & time, visualize them here, and see how they perfrom in a simple model on their own. We checked each time transformation against logistic regression (a linear model) and a tree ensemble (a nonlinear model), because a linear method might not capture all the information in time.\n",
    "\n",
    "Decision trees (a bunch of consecutive binary splits of the data based on variable values) can be a useful way to explore models where features my be nonlinear as may be the case with time features. For example, hour 23.5 (late at night) and 0.5 (so early in the morning it's still late at night) may be treated similarly by a linear model, but a decision tree, can create a couple splites and capture it easily (hour > 23 and hour < 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = LogisticRegression(random_state=rseed, class_weight='auto', fit_intercept=True)\n",
    "etc = ExtraTreesClassifier(n_estimators=200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Visualizations to inspect time variables\n",
    "# Exploring time features, it looks like requests are not as succesful at late nights /\n",
    "# early mornings or on Mondays / Fridays...\n",
    "# Though that could be because there's more requests on those days.\n",
    "\n",
    "tt = TimeTransformer(do_minute=False, do_day=False, do_second=False, do_hour=False, do_dow=False, do_month=True)\n",
    "print 'Logistic Regression:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',lr)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'Extra Trees:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',etc)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "# look at month success\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "month = tt.transform(all_train_df.values).flatten()\n",
    "month_pos = month[all_train_labels]\n",
    "month_neg = month[np.logical_not(all_train_labels)]\n",
    "pd.Series(month_pos).hist(bins=12, alpha=0.2, normed=True, label='Winner Values Greater than All')\n",
    "pd.Series(month_neg).hist(bins=12, alpha=0.2, normed=True, label='Winner Values Less than All')\n",
    "plt.title(\"RAOP Month of Message Submissions \\n\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = TimeTransformer(do_minute=False, do_day=True, do_second=False, do_hour=False, do_dow=False, do_month=False)\n",
    "print 'Logistic Regression:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',lr)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'Extra Trees:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',etc)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "\n",
    "# look at day success\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "day = tt.transform(all_train_df.values).flatten()\n",
    "day_pos = day[all_train_labels]\n",
    "day_neg = day[np.logical_not(all_train_labels)]\n",
    "pd.Series(day_pos).hist(bins=31, alpha=0.2, normed=True, label='Winner Values Greater than All')\n",
    "pd.Series(day_neg).hist(bins=31, alpha=0.2, normed=True, label='Winner Values Less than All')\n",
    "plt.title(\"RAOP Day Message Submissions \\n\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = TimeTransformer(do_minute=False, do_day=False, do_second=False, do_hour=False, do_dow=True, do_month=False)\n",
    "print 'Logistic Regression:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',lr)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'Extra Trees:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',etc)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "# look at day of week success\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "dow = tt.transform(all_train_df.values).flatten()\n",
    "dow_pos = dow[all_train_labels]\n",
    "dow_neg = dow[np.logical_not(all_train_labels)]\n",
    "pd.Series(dow_pos).hist(bins=7, alpha=0.2, normed=True, label='Winner Values Greater than All')\n",
    "pd.Series(dow_neg).hist(bins=7, alpha=0.2, normed=True, label='Winner Values Less than All')\n",
    "plt.title(\"RAOP Day of Week Message Submissions \\n\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = TimeTransformer(do_minute=False, do_day=False, do_second=False, do_hour=True, do_dow=False, do_month=False)\n",
    "print 'Logistic Regression:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',lr)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'Extra Trees:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',etc)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "# look at hourly success\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "hour = tt.transform(all_train_df.values).flatten()\n",
    "hour_pos = hour[all_train_labels]\n",
    "hour_neg = hour[np.logical_not(all_train_labels)]\n",
    "pd.Series(hour_pos).hist(bins=24, alpha=0.2, normed=True, label='Winner Values Greater than All')\n",
    "pd.Series(hour_neg).hist(bins=24, alpha=0.2, normed=True, label='Winner Values Less than All')\n",
    "plt.title(\"RAOP Hour of Message Submissions \\n\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = TimeTransformer(do_minute=True, do_day=False, do_second=False, do_hour=False, do_dow=False, do_month=False)\n",
    "print 'Logistic Regression:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',lr)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'Extra Trees:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',etc)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "\n",
    "# look at minute success\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "minute = tt.transform(all_train_df.values).flatten()\n",
    "minute_pos = minute[all_train_labels]\n",
    "minute_neg = minute[np.logical_not(all_train_labels)]\n",
    "pd.Series(minute_pos).hist(bins=60, alpha=0.2, normed=True, label='Winner Values Greater than All')\n",
    "pd.Series(minute_neg).hist(bins=60, alpha=0.2, normed=True, label='Winner Values Less than All')\n",
    "plt.title(\"RAOP Minute of Message Submissions \\n\")\n",
    "plt.xlabel(\"Minute\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = TimeTransformer(do_minute=False, do_day=False, do_second=True, do_hour=False, do_dow=False, do_month=False)\n",
    "print 'Logistic Regression:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',lr)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'Extra Trees:'\n",
    "print_scores(cross_val_score(Pipeline([('tt',tt),('model',etc)]), all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "\n",
    "# look at second success\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "second = tt.transform(all_train_df.values).flatten()\n",
    "second_pos = second[all_train_labels]\n",
    "second_neg = second[np.logical_not(all_train_labels)]\n",
    "pd.Series(second_pos).hist(bins=60, alpha=0.2, normed=True, label='Winner Values Greater than All')\n",
    "pd.Series(second_neg).hist(bins=60, alpha=0.2, normed=True, label='Winner Values Less than All')\n",
    "plt.title(\"RAOP Second of Message Submissions \\n\")\n",
    "plt.xlabel(\"Second\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Time Variables Together\n",
    "\n",
    "Months, day of the month, and hour of the day all appear to be at least somewhat effective predictors.\n",
    "It's worth noting that the hour of the message only adds value with the nonlinear classifier.\n",
    "This suggests our final classifier should be nonlinear if it includes this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Try a couple classifiers for time features to find a good choice.\n",
    "# Turns out time features don't perform well by themselves.\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "# only do month, day, hour\n",
    "tt = TimeTransformer(do_minute=False, do_day=True, do_second=True, do_hour=False, do_dow=False, do_month=True)\n",
    "# the day was borderline, so try one without\n",
    "tt_noday = TimeTransformer(do_minute=False, do_day=False, do_second=True, do_hour=False, do_dow=False, do_month=True)\n",
    "\n",
    "print '\\nExtra Tree Ensemble Month, Day, Hour'\n",
    "\n",
    "etc_pipe = Pipeline([\n",
    "    ('time',tt),\n",
    "    ('model',etc)\n",
    "    ])\n",
    "\n",
    "print_scores(cross_val_score(etc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print '\\nExtra Tree Ensemble Month, Hour'\n",
    "\n",
    "etc_pipe = Pipeline([\n",
    "    ('time',tt_noday),\n",
    "    ('model',etc)\n",
    "    ])\n",
    "\n",
    "print_scores(cross_val_score(etc_pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Time Features\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part5\"></a>\n",
    "## 5. Interesting Words & Category Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Reusable class for interesting words\n",
    "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
    "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
    "\n",
    "# Useful method for getting length of text:\n",
    "def lenArray(text, no_zero = True):\n",
    "    lens = np.array([[float(len(x.encode('utf-8'))) for x in text]]).T\n",
    "    lens[lens==0]=1\n",
    "    return lens\n",
    "\n",
    "class InterestingWordsTransformer(TransformerMixin):\n",
    "    def __init__(self, title_col = TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True, do_tags=True, do_words=True):\n",
    "        self.do_title = do_title\n",
    "        self.do_body = do_body\n",
    "        self.do_tags = do_tags\n",
    "        self.do_words = do_words\n",
    "        self.title_col = title_col\n",
    "        self.body_col = body_col\n",
    "        \n",
    "        # dictionary of keys = tags and values = word to find\n",
    "        self.keywords = {\n",
    "            'sad_food': ['hungry', 'starving', 'no food', 'grocer', 'eaten', 'hunger', 'ramen', 'empty', 'fridge', 'refrig'],\n",
    "            'money': ['broke', 'paid', 'money', 'unemployed', 'lost', 'job', 'bill', 'wage', 'work', 'payday', 'paycheck', 'funds', 'cash', 'bank', 'laid off', 'poor', 'payroll'],\n",
    "            'sad': ['worst', 'awful', 'sick', 'problem', 'catch a break', 'cheer', 'hospital', 'bad', 'shitty', 'stress', 'luck', ':(', 'rough', 'tough', 'battle', 'reasons', 'losing'],\n",
    "            'military': ['military', 'veteran', 'soldier', 'army', 'navy', 'marine', 'air force', 'iraq', 'afghanis'],\n",
    "            'happy': ['celebrate', 'birthday', 'party', 'new year', 'bday', 'engage', 'annivers', 'surprise', 'loves', 'best'],\n",
    "            'nice': ['please', 'help', 'thank', ':)', 'helping', 'aid', 'exchange', 'spare', ':D',':-)'],\n",
    "            'honest': ['sob story', 'honest', 'just want', 'just because'],\n",
    "            'parent': ['family', 'kids', 'parent', 'mom', 'mommy', 'mother', 'dad', 'father', 'baby', 'boy', 'girl'],\n",
    "            'relationship': ['husband', 'wife', 'girlfriend', 'boyfriend', 'fianc', 'roommate', 'married'],\n",
    "            'test': ['study', 'test', 'final', 'midterm','student'],\n",
    "            'time': ['yesterday', 'lately', 'never', 'during', 'sunday', 'constantly']\n",
    "        }\n",
    "    \n",
    "    def find_tag_words(self, keywords, text):\n",
    "        word_dict = {}\n",
    "        tag_dict = {}\n",
    "\n",
    "        for tag, words in keywords.iteritems():\n",
    "\n",
    "            tag_count = None\n",
    "\n",
    "            for word in words:\n",
    "                # check for the word in the text\n",
    "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
    "                word_dict[word] = has_word\n",
    "                \n",
    "                # count the words with the tag\n",
    "                if tag_count is None:\n",
    "                    tag_count = has_word\n",
    "                else:\n",
    "                    tag_count = tag_count +  has_word\n",
    "\n",
    "            tag_dict[tag] = tag_count\n",
    "\n",
    "        return (tag_dict, word_dict)\n",
    "    \n",
    "    # manually create keywords with categories\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        do_title = self.do_title\n",
    "        do_tags = self.do_tags\n",
    "        do_words = self.do_words\n",
    "        do_body = self.do_body\n",
    "        keywords = self.keywords\n",
    "        find_tag_words = self.find_tag_words\n",
    "        body_col = self.body_col\n",
    "        title_col = self.title_col\n",
    "        \n",
    "        features = []\n",
    "        feature_names = []\n",
    "\n",
    "        # find keywords and tags\n",
    "        if do_title and not do_body:\n",
    "            title_unicode = np.array([x.lower() for x in X[:,title_col]])\n",
    "            title_tag_dict, title_word_dict = find_tag_words(keywords, title_unicode)\n",
    "            \n",
    "            # normalize appearence of important words by character length of text\n",
    "            # because longer requests should have more hits\n",
    "            lens = lenArray(X[:,body_col])\n",
    "            \n",
    "            # add frequency of tags\n",
    "            if do_tags:\n",
    "                features.append(pd.DataFrame(title_tag_dict).values/lens)\n",
    "                feature_names.append('title_tags')\n",
    "                \n",
    "            # add frequency of words\n",
    "            if do_words:\n",
    "                features.append(pd.DataFrame(title_word_dict).values/lens)\n",
    "                feature_names.append('title_words')\n",
    "\n",
    "        if do_body and not do_title:\n",
    "            body_unicode = np.array([x.lower() for x in X[:,body_col]])\n",
    "            body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
    "            \n",
    "            # normalize appearence of important words by character length of text\n",
    "            # because longer requests should have more hits\n",
    "            lens = lenArray(X[:,body_col])\n",
    "            \n",
    "            # add frequency of tags\n",
    "            if do_tags:\n",
    "                features.append(pd.DataFrame(body_tag_dict).values/lens)\n",
    "                feature_names.append('body_tags')\n",
    "            \n",
    "            # add frequency of words\n",
    "            if do_words:\n",
    "                features.append(pd.DataFrame(body_word_dict).values/lens)\n",
    "                feature_names.append('body_words')\n",
    "                \n",
    "        if do_body and do_title:\n",
    "            body_unicode = np.array([x.lower() for x in ConcatStringTransformer().transform(X[:,[body_col,title_col]])])\n",
    "            body_tag_dict, body_word_dict = find_tag_words(keywords, body_unicode)\n",
    "            \n",
    "            # normalize appearence of important words by character length of text\n",
    "            # because longer requests should have more hits\n",
    "            lens = lenArray(X[:,body_col])\n",
    "            \n",
    "            # add frequency of tags\n",
    "            if do_tags:\n",
    "                features.append(pd.DataFrame(body_tag_dict).values/lens)\n",
    "                feature_names.append('body_tags')\n",
    "            \n",
    "            # add frequency of words\n",
    "            if do_words:\n",
    "                features.append(pd.DataFrame(body_word_dict).values/lens)\n",
    "                feature_names.append('body_words')\n",
    "\n",
    "        return np.hstack(tuple(features))\n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        #do nothing\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'do_words': self.do_words, 'do_tags':self.do_tags, 'do_body':self.do_body, 'do_title':self.do_title}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section finds words that we thought were \"interesting\" (potentially explanatory) in request text. It also puts each word in to a category, because a request where someone bemoans their lack of money may include the workd \"broke\" or \"lost job\", but maybe only one of those too. Since the lack of money could be driving the response, we want to combine those features together. The interesting words and their categories are defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Explore different models focusing on interesting words in tags and text:\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=10,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto', random_state=rseed))])\n",
    "\n",
    "#lsvc_pca = Pipeline([\n",
    "#    ('scale', StandardScaler()),\n",
    "#    ('pca', RandomizedPCA(n_components=3,random_state=rseed)),\n",
    "#    ('clf', LinearSVC(class_weight='auto',random_state=rseed))\n",
    "#])\n",
    "\n",
    "\n",
    "models = {'Extra Trees':etc, 'Linear SVC':lsvc}\n",
    "\n",
    "print '\\n##############'\n",
    "print 'Body & Title Tags'\n",
    "trans = InterestingWordsTransformer(do_words=False)\n",
    "\n",
    "for name, model in models.iteritems():\n",
    "    print '\\n%s' % name\n",
    "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
    "    pipe = Pipeline([('trans',trans),('model',model)])\n",
    "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print '\\n##############'\n",
    "print 'Body & Title Words'\n",
    "trans = InterestingWordsTransformer(do_tags=False)\n",
    "\n",
    "for name, model in models.iteritems():\n",
    "    print '\\n%s' % name\n",
    "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
    "    pipe = Pipeline([('trans',trans),('model',model)])\n",
    "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print '\\n##############'\n",
    "print 'Body & Title Words & Tags'\n",
    "trans = InterestingWordsTransformer()\n",
    "\n",
    "for name, model in models.iteritems():\n",
    "    print '\\n%s' % name\n",
    "    kfi = kf_over if (name=='Gradient Boosting') else kf\n",
    "    pipe = Pipeline([('trans',trans),('model',model)])\n",
    "    print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kfi, scoring=roc_scorer))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Interesting Words\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-fold in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Interesting Word Tags in Request</td>\n",
    "<td>0.5509</td>\n",
    "<td>0.5582</td>\n",
    "<td>0.0363</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part6\"></a>\n",
    "## 6. Request Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from sklearn.feature_extraction import text as sklearn_text\n",
    "brown_words = np.unique(np.array(brown.words()))\n",
    "brown_words = np.unique(np.array([x.lower() for x in brown_words]))\n",
    "brown_word2tag = {word.lower(): tag for word, tag in brown.tagged_words()}\n",
    "brown_tags = set([tag for word, tag in brown.tagged_words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEFAULT_WORD2TAG = brown_word2tag\n",
    "DEFAULT_TAG_SET = brown_tags\n",
    "DEFAULT_WORD_SET = set(brown_words)\n",
    "DEFAULT_STOP_WORDS = set(['request'])\n",
    "DEFAULT_TOKENIZER = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
    "\n",
    "# This calcualtes how many of the words are in the Brown corpus,\n",
    "# the idea is that this may capture more well written requests\n",
    "class InCorpusTransformer(TransformerMixin):\n",
    "    def __init__(self, word_set=DEFAULT_WORD_SET, tokenizer=DEFAULT_TOKENIZER, stop_words=DEFAULT_STOP_WORDS, normalize=True):\n",
    "        self.word_set = word_set\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def count_tokens(self, tokens):\n",
    "        if len(tokens) == 0:\n",
    "            return 2\n",
    "        else:\n",
    "            return sum(np.array([token.lower() in self.word_set for token in tokens]))/float(len(tokens))\n",
    "    \n",
    "    def tokenize_and_count(self, text):\n",
    "        tokens = [x.lower() for x in self.tokenizer.tokenize(text) if x.lower() not in self.stop_words]\n",
    "        return self.count_tokens(tokens)\n",
    "    \n",
    "    def process_vector(self, texts):\n",
    "        return np.array([[self.tokenize_and_count(text) for text in texts]]).T\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        if len(X.shape) == 1:\n",
    "            lens = lenArray(X)\n",
    "            if self.normalize:\n",
    "                return self.process_vector(X)/lens\n",
    "            else:\n",
    "                return self.process_vector(X)\n",
    "        else:\n",
    "            features = []\n",
    "            for col in range(X.shape[1]):\n",
    "                lens = lenArray(X[:,col])\n",
    "                if self.normalize:\n",
    "                    features.append(self.process_vector(X[:,col])/lens)\n",
    "                else:\n",
    "                    features.append(self.process_vector(X[:,col]))\n",
    "            return np.hstack(tuple(features))\n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        #do nothing\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'normalize':self.normalize}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature transformer calculates whether words in the request are also present in the Brown University Standard Corpus of Present Day American English. We noticed that \"well written\" requests tended to perform better, and thought that perhaps requests that used mroe \"standard english\" words may be perceived as more well written. Additionally, the Brown corpus tags words by part of speech, and we used this information to create another feautre set below. \n",
    "\n",
    "It's also worth noting that via analysis of our errors we noted that this feature overly favored long requests. This made us realize we should adjust by the length of the request which led to better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incorpus = Pipeline([('all_text', ExtractAllText()),('concat', ConcatStringTransformer()),('in',InCorpusTransformer())])\n",
    "incorpus_raw = Pipeline([('all_text', ExtractAllText()),('concat', ConcatStringTransformer()),('in',InCorpusTransformer(normalize=False))])\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto', random_state=rseed))])\n",
    "\n",
    "print 'LSVC on unnormalized counts'\n",
    "pipe = Pipeline([('incorpus', incorpus_raw), ('model', lsvc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'LSVC on normalized counts'\n",
    "\n",
    "pipe = Pipeline([('incorpus', incorpus), ('model', lsvc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'ETC on unnormalized counts'\n",
    "pipe = Pipeline([('incorpus', incorpus_raw), ('model', etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print 'ETC on normalized counts'\n",
    "\n",
    "pipe = Pipeline([('incoprus', incorpus), ('model', etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Spelling Mistakes\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Interesting Word Tags in Request</td>\n",
    "<td>0.5509</td>\n",
    "<td>0.5582</td>\n",
    "<td>0.0363</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (LSVC)</td>\n",
    "<td>0.5706</td>\n",
    "<td>0.5800</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (ETC)</td>\n",
    "<td>0.5639</td>\n",
    "<td>0.5695</td>\n",
    "<td>0.0248</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part7\"></a>\n",
    "## 7. Text Summary Features: Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Reusable class for text length:\n",
    "\n",
    "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
    "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
    "\n",
    "class TextSummaryTransformer(TransformerMixin):\n",
    "    def __init__(self, title_col=TITLE_COLUMN, body_col=BODY_COLUMN, do_title=True, do_body=True):\n",
    "        self.do_title = do_title\n",
    "        self.do_body = do_body\n",
    "        self.title_col = title_col\n",
    "        self.body_col = body_col\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        do_title = self.do_title\n",
    "        do_body = self.do_body\n",
    "        title_col = self.title_col\n",
    "        body_col = self.body_col\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        if do_title:\n",
    "            title_unicode = X[:, title_col]\n",
    "            title_len = np.array([[len(x.encode('utf-8')) for x in title_unicode]]).T\n",
    "            features.append(title_len)\n",
    "            \n",
    "        if do_body:\n",
    "            body_unicode = X[:, body_col]\n",
    "            body_len = np.array([[len(x.encode('utf-8')) for x in body_unicode]]).T\n",
    "            features.append(body_len)\n",
    "        \n",
    "        return np.hstack(tuple(features))\n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        #do nothing\n",
    "        return self \n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features simply check how long the request title and body were, since respondents may be more or less likely to grant long eloquent requests or short concise requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "lsvc = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC(class_weight='auto', random_state=rseed))])\n",
    "\n",
    "\n",
    "print '\\nExtra Trees Classifier on title and body length'\n",
    "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('etc', etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))\n",
    "\n",
    "print '\\Linear SVC on title and body length'\n",
    "pipe = Pipeline([('text_summary', TextSummaryTransformer()), ('lsvc', lsvc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Text Summary Features\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Interesting Word Tags in Request</td>\n",
    "<td>0.5509</td>\n",
    "<td>0.5582</td>\n",
    "<td>0.0363</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (LSVC)</td>\n",
    "<td>0.5706</td>\n",
    "<td>0.5800</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (ETC)</td>\n",
    "<td>0.5639</td>\n",
    "<td>0.5695</td>\n",
    "<td>0.0248</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Title and Body Length</td>\n",
    "<td>0.5787</td>\n",
    "<td>0.5813</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part8\"></a>\n",
    "## 8. Location Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Collect location name metadata\n",
    "\n",
    "MANUAL_GEOS = [\n",
    "{'loc':'nyc', 'g1':'ny', 'g2':'us'},\n",
    "{'loc':'sf', 'g1':'ca', 'g2':'us'},\n",
    "{'loc':'uk', 'g1':'uk', 'g2':'non_us'},\n",
    "{'loc':'australia', 'g1':'aus', 'g2':'non_us'},\n",
    "{'loc':'canada', 'g1':'can', 'g2':'non_us'},\n",
    "{'loc':'ottawa', 'g1':'can', 'g2':'non_us'},\n",
    "{'loc':'toronto', 'g1':'can', 'g2':'non_us'},\n",
    "{'loc':'vancouver', 'g1':'can', 'g2':'non_us'},\n",
    "{'loc':'montreal', 'g1':'can', 'g2':'non_us'}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def make_geo(other_geos=MANUAL_GEOS, filter_loc=[]):\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib import urlopen\n",
    "    import re\n",
    "    \n",
    "    ######################\n",
    "    # Scrape wikipedia list of us cities\n",
    "    \n",
    "    # TODO save local\n",
    "    webpage = urlopen('http://en.wikipedia.org/wiki/List_of_United_States_cities_by_population')\n",
    "    \n",
    "    # parse webpage to find the table\n",
    "    soup=BeautifulSoup(webpage, \"html.parser\")\n",
    "    table = soup.find('table', {'class' : 'wikitable sortable'})\n",
    "    \n",
    "    # stroe the first 200 US cities\n",
    "    us_cities = []\n",
    "    rows = table.findAll('tr')\n",
    "    for row in rows[1:200]:\n",
    "        cells = row.findAll('td')\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for i, cell in enumerate(cells):\n",
    "            if i < 4:\n",
    "                text = cell.text.strip().lower()\n",
    "                if i == 0:\n",
    "                    text = int(text)\n",
    "                if i == 1 or i == 2:\n",
    "                    text = re.sub(r\"\\[.*\\]|'\",'',text)\n",
    "                if i == 3:\n",
    "                    text = int(re.sub(r',','',text))\n",
    "                output.append(text)\n",
    "        us_cities.append(output)\n",
    "\n",
    "    us_cities = pd.DataFrame(np.array(us_cities),columns=['rank','city','state','pop'])\n",
    "    \n",
    "    ###########################\n",
    "    # tuple list of state abbreviations\n",
    "    \n",
    "    state_abr_raw = [(\"Alabama\",\"AL\"),(\"Alaska\",\"AK\"),(\"Arizona\",\"AZ\"),\n",
    "                     (\"Arkansas\",\"AR\"),(\"California\",\"CA\"),(\"Colorado\",\"CO\"),\n",
    "                     (\"Connecticut\",\"CT\"),(\"Delaware\",\"DE\"),(\"District of Columbia\",\"DC\"),\n",
    "                     (\"Florida\",\"FL\"),(\"Georgia\",\"GA\"),(\"Hawaii\",\"HI\"),\n",
    "                     (\"Idaho\",\"ID\"),(\"Illinois\",\"IL\"),(\"Indiana\",\"IN\"),\n",
    "                     (\"Iowa\",\"IA\"),(\"Kansas\",\"KS\"),(\"Kentucky\",\"KY\"),\n",
    "                     (\"Louisiana\",\"LA\"),(\"Maine\",\"ME\"),(\"Montana\",\"MT\"),\n",
    "                     (\"Nebraska\",\"NE\"),(\"Nevada\",\"NV\"),(\"New Hampshire\",\"NH\"),\n",
    "                     (\"New Jersey\",\"NJ\"),(\"New Mexico\",\"NM\"),(\"New York\",\"NY\"),\n",
    "                     (\"North Carolina\",\"NC\"),(\"North Dakota\",\"ND\"),(\"Ohio\",\"OH\"),\n",
    "                     (\"Oklahoma\",\"OK\"),(\"Oregon\",\"OR\"),(\"Maryland\",\"MD\"),\n",
    "                     (\"Massachusetts\",\"MA\"),(\"Michigan\",\"MI\"),(\"Minnesota\",\"MN\"),\n",
    "                     (\"Mississippi\",\"MS\"),(\"Missouri\",\"MO\"),(\"Pennsylvania\",\"PA\"),\n",
    "                     (\"Rhode Island\",\"RI\"),(\"South Carolina\",\"SC\"),(\"South Dakota\",\"SD\"),\n",
    "                     (\"Tennessee\",\"TN\"),(\"Texas\",\"TX\"),(\"Utah\",\"UT\"),\n",
    "                     (\"Vermont\",\"VT\"),(\"Virginia\",\"VA\"),(\"Washington\",\"WA\"),\n",
    "                     (\"West Virginia\",\"WV\"),(\"Wisconsin\",\"WI\"),(\"Wyoming\",\"WY\")]\n",
    "    \n",
    "    ############################\n",
    "    # manupulate state abreviations\n",
    "    state_abr = []\n",
    "    for st, abr in state_abr_raw:\n",
    "        state_abr.append([st.lower(), abr.lower()])\n",
    "    state_abr = pd.DataFrame(np.array(state_abr), columns = ['state','abr'])\n",
    "    \n",
    "    #############################\n",
    "    # make US Geos\n",
    "    us_city_state = pd.merge(us_cities,state_abr)\n",
    "    \n",
    "    # US geos\n",
    "    usgeo = us_city_state.loc[:,['city','abr']]\n",
    "    usgeo.columns = ['loc','g1']\n",
    "    usgeo = pd.concat([usgeo,pd.DataFrame({'loc':state_abr.abr,'g1':state_abr.abr})])\n",
    "    usgeo = pd.concat([usgeo,pd.DataFrame({'loc':state_abr.state,'g1':state_abr.abr})])\n",
    "    usgeo['g2'] = 'us'\n",
    "    \n",
    "    \n",
    "    geo = pd.concat([usgeo, pd.DataFrame(other_geos)])\n",
    "    \n",
    "    # get rid of auto generated locations with confusiong names\n",
    "    #geo = geo[[not x in filter_loc for x in geo['loc']]]\n",
    "    \n",
    "    return geo\n",
    "\n",
    "geo = make_geo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reusable class for finding location names and aggregating them for different metadata\n",
    "\n",
    "DEFAULT_TOKENIZER = RegexpTokenizer(r'[\\s\\.\\,\\:\\-\\;\\(\\)\\[\\]\\{\\}\\!\\?]+',gaps=True)\n",
    "FILTER_DEFAULT = ['in', 'hi', 'me', 'ok', 'HI', 'OK', 'or']\n",
    "DEFAULT_GEO = geo\n",
    "\n",
    "class GeoTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, geo=DEFAULT_GEO, level=2, tokenizer=DEFAULT_TOKENIZER, stop_words=FILTER_DEFAULT, total_only=True, normalize=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.geo = geo\n",
    "        self.level = level\n",
    "        self.normalize = normalize\n",
    "        self.total_only = total_only\n",
    "        \n",
    "    def find_words(self, words, texts, g1=None, g2=None, lower=True):\n",
    "        word_dict = {}\n",
    "        do_g1 = not g1 is None\n",
    "        do_g2 = not g2 is None\n",
    "        \n",
    "        if do_g1:\n",
    "            g1_dict = {}\n",
    "            \n",
    "        if do_g2:\n",
    "            g2_dict = {}\n",
    "        \n",
    "        #token_list = [[x.lower() for x in self.tokenizer.tokenize(text) if x not in self.stop_words] for text in texts]\n",
    "    \n",
    "        i = 0\n",
    "        for word in words:\n",
    "            if not word in self.stop_words:\n",
    "                regex = re.compile('\\\\b('+word+')\\\\b')\n",
    "                has_word = np.array([(1 if regex.search(text.lower()) else 0) for text in texts])\n",
    "                word_dict[word] = has_word\n",
    "                if do_g1:\n",
    "                    g1i = g1.iloc[i]\n",
    "                    if g1i in g1_dict:\n",
    "                        g1_dict[g1i] += has_word\n",
    "                    else:\n",
    "                        g1_dict[g1i] = has_word\n",
    "                        \n",
    "                if do_g2:\n",
    "                    g2i = g2.iloc[i]\n",
    "                    if g2i in g2_dict:\n",
    "                        g2_dict[g2i] += has_word\n",
    "                    else:\n",
    "                        g2_dict[g2i] = has_word\n",
    "                        \n",
    "            i += 1\n",
    "\n",
    "        return (word_dict, g1_dict, g2_dict)\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        geo = self.geo\n",
    "        find_words = self.find_words\n",
    "        level = self.level\n",
    "        normalize = self.normalize\n",
    "        \n",
    "        words = geo['loc']\n",
    "        g1 = geo['g1']\n",
    "        g2 = geo['g2']\n",
    "        \n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        \n",
    "        if len(X.shape) > 1:\n",
    "            cols = X.shape[1]\n",
    "            for i in cols:\n",
    "                locs, g1s, g2s = find_words(words, X[:,i], g1s, g2s)\n",
    "                \n",
    "                if level == 0:\n",
    "                    df = pd.DataFrame(locs)\n",
    "                elif level == 1:\n",
    "                    df = pd.DataFrame(g1s)\n",
    "                elif level > 1:\n",
    "                    df = pd.DataFrame(g2s)\n",
    "                    \n",
    "                #df = pd.DataFrame(locss)\n",
    "                features.append(df.values)\n",
    "        else:\n",
    "            lens = lenArray(X)\n",
    "            \n",
    "            locs, g1s, g2s = find_words(words, X, g1, g2)\n",
    "            \n",
    "            if level == 0:\n",
    "                df = pd.DataFrame(locs)\n",
    "            elif level == 1:\n",
    "                df = pd.DataFrame(g1s)\n",
    "            elif level > 1:\n",
    "                df = pd.DataFrame(g2s)\n",
    "            \n",
    "            #df = pd.DataFrame(locss)\n",
    "            if normalize:\n",
    "                features.append(df.values/lens)\n",
    "            else:\n",
    "                features.append(df.values)\n",
    "            #print df.values/lens\n",
    "        \n",
    "        ret_array = np.hstack(tuple(features))\n",
    "        \n",
    "        total = np.reshape(np.sum(ret_array,1),(ret_array.shape[0],1))\n",
    "        \n",
    "        if self.total_only:\n",
    "            self.feature_names_ = np.array([u'Total'])\n",
    "            \n",
    "            return total\n",
    "        else:\n",
    "            self.feature_names_ = np.hstack((df.columns.values,u'Total'))\n",
    "\n",
    "            return np.hstack((ret_array, total))\n",
    "\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        #do nothing\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {'level':self.level, 'normalize':self.normalize}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through our errors, we noticed that we were missing alot of successful requests that included location names. We also noticed on the Reddit page that you have to include a location for your request to be granted (duh). So we scraped a list of US city names and their associated states. We also created a list of us states and their abbreviations. Finally, we added a few locations of our own. This transformer, looks for matches of these location names and aggregates them, usually based on state (based on country for international locations)\n",
    "\n",
    "\n",
    "UNFORTUNATELY, it didn't actually add much value... but thought we'd include it here anyway, cuz it was a lot of work and could be interesting for others to use in other problems or improve here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_trans = GeoTransformer(geo, level = 1, total_only = False, normalize = False)\n",
    "geo_title = geo_trans.transform(ExtractTitle().transform(all_train_df.values))\n",
    "geo_body = geo_trans.transform(ExtractBody().transform(all_train_df.values))\n",
    "\n",
    "\n",
    "print 'Individual occurences in title:'\n",
    "print zip(geo_trans.feature_names_, np.sum(geo_title,0))\n",
    "\n",
    "\n",
    "print 'Individual occurences in body:'\n",
    "print zip(geo_trans.feature_names_, np.sum(geo_body,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state = rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "level = 1\n",
    "\n",
    "title = Pipeline([('text',ExtractTitle()),('geo',GeoTransformer(geo, level=level))])\n",
    "body = Pipeline([('text',ExtractBody()),('geo',GeoTransformer(geo, level=level))])\n",
    "all_text = Pipeline([('text',ExtractAllText()),('combine',ConcatStringTransformer()),('geo',GeoTransformer(geo,level=level))])\n",
    "\n",
    "pipe = Pipeline([('features',body),('model',etc)])\n",
    "\n",
    "print 'Body:'\n",
    "pipe = Pipeline([('features',body),('model',etc)])\n",
    "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
    "print_scores(scores)\n",
    "\n",
    "print 'Title:'\n",
    "pipe = Pipeline([('features',title),('model',etc)])\n",
    "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
    "print_scores(scores)\n",
    "\n",
    "print 'On Concat Title/Body:'\n",
    "pipe = Pipeline([('features',all_text),('model',etc)])\n",
    "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Text Summary Features\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 5 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Interesting Word Tags in Request</td>\n",
    "<td>0.5509</td>\n",
    "<td>0.5582</td>\n",
    "<td>0.0363</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (LSVC)</td>\n",
    "<td>0.5706</td>\n",
    "<td>0.5800</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (ETC)</td>\n",
    "<td>0.5639</td>\n",
    "<td>0.5695</td>\n",
    "<td>0.0248</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Title and Body Length</td>\n",
    "<td>0.5787</td>\n",
    "<td>0.5813</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Location Features on Title and Body Length</td>\n",
    "<td>0.5207</td>\n",
    "<td>0.5157</td>\n",
    "<td>0.0252</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part9\"></a>\n",
    "## 9. Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# For Part of Speech Tagging\n",
    "class Word2TagTransformer(TransformerMixin):\n",
    "    def __init__(self, tag_set=DEFAULT_TAG_SET, word_set=DEFAULT_WORD_SET, word2tag=DEFAULT_WORD2TAG, tokenizer=DEFAULT_TOKENIZER, stop_words=DEFAULT_STOP_WORDS):\n",
    "        self.tag_set = tag_set\n",
    "        self.word_set = word_set\n",
    "        self.word2tag = word2tag\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tags_dict = {tag: 0 for tag in tag_set}\n",
    "    \n",
    "    def tag_tokens(self, tokens):\n",
    "        tag_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if (token in self.word_set) and (token not in self.stop_words):\n",
    "                tag_tokens.append(self.word2tag[token])\n",
    "        \n",
    "        return ' '.join(tag_tokens)\n",
    "        \n",
    "    \n",
    "    def tokenize_and_tag(self, text):\n",
    "        tokens = [x.lower() for x in self.tokenizer.tokenize(text) if x.lower() not in self.stop_words]\n",
    "        return self.tag_tokens(tokens)\n",
    "    \n",
    "    def process_vector(self, texts):\n",
    "        return np.array([[self.tokenize_and_tag(text) for text in texts]]).T\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        if len(X.shape) == 1:\n",
    "            return self.process_vector(X).flatten()\n",
    "        else:\n",
    "            features = []\n",
    "            for col in range(X.shape[1]):\n",
    "                features.append(self.process_vector(X[:,col]))\n",
    "            return np.hstack(tuple(features))\n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        #do nothing\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of feature transformers replaces words with their parts of speech as tagged in the Brown corpus. I then counts the term frequency on the parts of speech tags. Our thinking was again that higher quality or lower quality requests may use different semantic constructs and that this analysis would extract that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv_space = TfidfVectorizer(ngram_range=(1,1),lowercase=True, token_pattern=u'[^\\s-]')\n",
    "all_text = Pipeline([('all_text', ExtractAllText()),('concat', ConcatStringTransformer())])\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "\n",
    "lsvc = LinearSVC(class_weight='auto', random_state=rseed)\n",
    "\n",
    "pipe_nol1 = Pipeline([\n",
    "    ('text',all_text),\n",
    "    ('word2tag', Word2TagTransformer()),\n",
    "    ('tv',tv_space)\n",
    "])\n",
    "\n",
    "print '\\nLinear SVC on Brown corpus word tags:'\n",
    "pipe = Pipeline([('process', pipe_nol1),('model',lsvc)])\n",
    "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
    "print_scores(scores)\n",
    "\n",
    "print '\\nExtra Trees on Brown corpus word tags:'\n",
    "pipe = Pipeline([('process', pipe_nol1),('model',etc)])\n",
    "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
    "print_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part10\"></a>\n",
    "## 10 Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reusable class that transforms the list of subreddits for each user in to space seperated string for use by \n",
    "# the tfidf vectorizer\n",
    "SUBREDDITS_COLUMN = np.where(all_train_df.columns == 'requester_subreddits_at_request')[0][0]\n",
    "\n",
    "class SubredditTransformer(TransformerMixin):\n",
    "   \n",
    "    def __init__(self, column = SUBREDDITS_COLUMN):\n",
    "        self.column = column\n",
    "   \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self\n",
    "   \n",
    "    def transform(self, X, **transform_params):\n",
    "        return np.array([' '.join(x) for x in X[:,self.column]])\n",
    "   \n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature creator makes a term frequency matrix out of the list of subreddits that each requesting user contributes to. We decided to use a tree ensemble to test this method, because there may be interactive elements (presence of one subreddit and another). We also use L1 feature regularization as described in the text section above to reduce the feature set. We chose C such that we didn't get too many features (only 20) but didn't lose much explanatory power vs using lots more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators=200,\n",
    "                            max_depth=4,\n",
    "                            min_samples_split=15,\n",
    "                            random_state = rseed,\n",
    "                            class_weight='auto')\n",
    "l1 = LinearWeightFeatureThreshold(C=.15)\n",
    "tv_space = TfidfVectorizer(token_pattern = u'[^\\s]+', min_df=10)\n",
    "pipe = Pipeline([('sub',SubredditTransformer()), ('tv', tv_space), ('l1', l1), ('model',etc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer, verbose=1)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table += Subreddits + Parts of Speech\n",
    "\n",
    "The following table documents our results so far. The mean and median scores come from taking the averge of the ROC-AUC scores from 10 k-folds in the specified model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Interesting Word Tags in Request</td>\n",
    "<td>0.5509</td>\n",
    "<td>0.5582</td>\n",
    "<td>0.0363</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (LSVC)</td>\n",
    "<td>0.5706</td>\n",
    "<td>0.5800</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (ETC)</td>\n",
    "<td>0.5639</td>\n",
    "<td>0.5695</td>\n",
    "<td>0.0248</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Title and Body Length</td>\n",
    "<td>0.5787</td>\n",
    "<td>0.5813</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Location Features on Title and Body Length</td>\n",
    "<td>0.5207</td>\n",
    "<td>0.5157</td>\n",
    "<td>0.0252</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Parts of Speech</td>\n",
    "<td>0.5530</td>\n",
    "<td>0.5513</td>\n",
    "<td>0.0167</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Subreddits</td>\n",
    "<td>0.5586</td>\n",
    "<td>0.5581</td>\n",
    "<td>0.0262</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part11\"></a>\n",
    "## 11. Final, Composite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took two approaches to combining these features sets.\n",
    "In one, we checked which feature sets were the best and started with those. We then iteratively add additional feature sets to see whether or not they improve performance. If they don't add anything or subtract value, we remove them and keep going.\n",
    "In the other, we don't choose at all and leave it to the model to decide.\n",
    "\n",
    "Secondly, we try three different kinds of ensembled decision trees that get roughly similar results. Decision trees (a bunch of consecutive binary splits of the data based on variable values) can be a useful way to explore models where features my be nonlinear. Repeating a previous example, for the time features, hour 23.5 (late at night) and 0.5 (so early in the morning it's still late at night) may be treated similarly by a linear model, but a decision tree, can create a couple splites and capture it easily (hour > 23 and hour < 1).\n",
    "\n",
    "The \"ensemble\" part of \"tree ensemble\" means we constructing a ton of different decision trees, then average the prediction of each tree to inform our final prediction. This reduces the overfitting that can occur in a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats = {}\n",
    "all_feats={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lsvc = LinearSVC(class_weight='auto', random_state=rseed)\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "rfc = RandomForestClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with Extra Trees on BOW w/ L1 Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Try ExtraTreesClassifier for the BOW models:\n",
    "\n",
    "l1_bow = LinearWeightFeatureThreshold(C=0.15)\n",
    "tv_bow = TfidfVectorizer(tokenizer=SnowballStemTokenizer())\n",
    "\n",
    "all_feats['bow_l1'] = Pipeline([('extract', ExtractBody()), ('tv',tv_bow), ('features',l1_bow)])\n",
    "feats['bow_l1'] = all_feats['bow_l1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Tile and Body Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_feats['length'] = TextSummaryTransformer()\n",
    "feats['length'] = all_feats['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feats['length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Body & Title Length (NO ADDITION)</td>\n",
    "<td>0.5864</td>\n",
    "<td>0.5865</td>\n",
    "<td>0.0211</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding request quality to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incorpus = Pipeline([('all_text', ExtractAllText()),('concat', ConcatStringTransformer()),('in',InCorpusTransformer())])\n",
    "all_feats['incorpus']=incorpus\n",
    "feats['incorpus']=incorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del feats['incorpus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Request Quality (words from brown corpus)</td>\n",
    "<td>0.5903</td>\n",
    "<td>0.5992</td>\n",
    "<td>0.0274</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add subreddit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1 = LinearWeightFeatureThreshold(C=.15)\n",
    "tv_space = TfidfVectorizer(token_pattern = u'[^\\s]+', min_df=10)\n",
    "\n",
    "sub = Pipeline([('sub',SubredditTransformer()), ('tv', tv_space), ('l1', l1)])\n",
    "feats['sub'] = sub\n",
    "all_feats['sub'] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didn't help at all so will leave it out for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feats['sub']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.5903</td>\n",
    "<td>0.5992</td>\n",
    "<td>0.0274</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Subreddit Analysis</td>\n",
    "<td>0.5913</td>\n",
    "<td>0.5932</td>\n",
    "<td>0.0309</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add activity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acts = ExtractActivities()\n",
    "all_feats['activities'] = acts\n",
    "feats['activities'] = acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del feats['activities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.5903</td>\n",
    "<td>0.5992</td>\n",
    "<td>0.0273</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Activities</td>\n",
    "<td>0.5964</td>\n",
    "<td>0.5882</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv_space = TfidfVectorizer(ngram_range=(1,1),lowercase=True, token_pattern=u'[^\\s-]')\n",
    "all_text = Pipeline([('all_text', ExtractAllText()),('concat', ConcatStringTransformer())])\n",
    "\n",
    "pos_tags = Pipeline([\n",
    "    ('text',all_text),\n",
    "    ('word2tag', Word2TagTransformer()),\n",
    "    ('tv',tv_space),\n",
    "    ('desparse', DesparseTransformer())\n",
    "])\n",
    "\n",
    "all_feats['pos_tags'] = pos_tags\n",
    "feats['pos_tags'] = pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del feats['pos_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.5964</td>\n",
    "<td>0.5882</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Part of Speech Tags from Brown Corpus</td>\n",
    "<td>0.6024</td>\n",
    "<td>0.6116</td>\n",
    "<td>0.0198</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting word tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interesting = InterestingWordsTransformer(do_words=False)\n",
    "all_feats['interesting'] = interesting\n",
    "feats['interesting'] = interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('features', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse, toss it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feats['interesting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.6024</td>\n",
    "<td>0.6116</td>\n",
    "<td>0.0198</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+InterestingWords</td>\n",
    "<td>0.5958</td>\n",
    "<td>0.5916</td>\n",
    "<td>0.0147</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding time featues (month and hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = TimeTransformer(do_day=False, do_dow=False, do_hour=True,do_minute=False,do_second=False,do_month=True)\n",
    "feats['times'] = times\n",
    "all_feats['times'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('features', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feats['times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.6024</td>\n",
    "<td>0.6116</td>\n",
    "<td>0.0198</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Times</td>\n",
    "<td>0.5981</td>\n",
    "<td>0.6056</td>\n",
    "<td>0.0225</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding subreddit features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1 = LinearWeightFeatureThreshold(C=.15)\n",
    "tv_space = TfidfVectorizer(token_pattern = u'[^\\s]+', min_df=10)\n",
    "\n",
    "sub = Pipeline([('sub',SubredditTransformer()), ('tv', tv_space), ('l1', l1)])\n",
    "feats['sub'] = sub\n",
    "all_feats['sub'] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('features', FeatureUnion(feats.items())),('model',etc)])\n",
    "print_scores(cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse, toss it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feats['sub']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Prevailing Model</td>\n",
    "<td>0.6024</td>\n",
    "<td>0.6116</td>\n",
    "<td>0.0198</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>+Subreddit</td>\n",
    "<td>0.6009</td>\n",
    "<td>0.6079</td>\n",
    "<td>0.0280</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try all features with Extra Trees regularization then RandomForest prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest w/ All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(all_feats.items())),('model',rfc)])\n",
    "rfc_all = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
    "print_scores(rfc_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees Classifier With All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featues', FeatureUnion(all_feats.items())),('model',etc)])\n",
    "etc_all = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf, scoring=roc_scorer)\n",
    "print_scores(etc_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting with All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators = 200,\n",
    "                            learning_rate=0.01,\n",
    "                           max_depth = 3,\n",
    "                           min_samples_split=10,\n",
    "                           random_state = rseed)\n",
    "pipe = Pipeline([('featues', FeatureUnion(all_feats.items())),('model',gbc)])\n",
    "# note we use the oversampled k fold kf_over\n",
    "gbc = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer)\n",
    "print_scores(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Aggregated Models\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Best From Manual Stepwise Selection</td>\n",
    "<td>0.6024</td>\n",
    "<td>0.6116</td>\n",
    "<td>0.0198</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>All Features Gradient Boosting</td>\n",
    "<td>0.6114</td>\n",
    "<td>0.6070</td>\n",
    "<td>0.0325</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All Features Extra Trees</td>\n",
    "<td>0.6035</td>\n",
    "<td>0.6052</td>\n",
    "<td>0.0255</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All Features Random Forests</td>\n",
    "<td>0.6093</td>\n",
    "<td>0.6026</td>\n",
    "<td>0.0295</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "## Feature Models\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Method</th>\n",
    "<th>Mean ROC-AUC</th>\n",
    "<th>Median ROC-AUC</th>\n",
    "<th>Standard Deviation</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Activity Features with Reweighted Classes</td>\n",
    "<td>0.5589</td>\n",
    "<td>0.5570</td>\n",
    "<td>0.0213</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Simple BOW, Titles + Bodies, Snowball Stem Tokenizer</td>\n",
    "<td>0.5615</td>\n",
    "<td>0.5604</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees BOW w/ L1 Feature Reduction Bodies</td>\n",
    "<td>0.5870</td>\n",
    "<td>0.5920</td>\n",
    "<td>0.0271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Month, Hour</td>\n",
    "<td>0.5406</td>\n",
    "<td>0.5440</td>\n",
    "<td>0.0232</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Interesting Word Tags in Request</td>\n",
    "<td>0.5509</td>\n",
    "<td>0.5582</td>\n",
    "<td>0.0363</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (LSVC)</td>\n",
    "<td>0.5706</td>\n",
    "<td>0.5800</td>\n",
    "<td>0.0227</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request Quality (ETC)</td>\n",
    "<td>0.5639</td>\n",
    "<td>0.5695</td>\n",
    "<td>0.0248</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees on Title and Body Length</td>\n",
    "<td>0.5787</td>\n",
    "<td>0.5813</td>\n",
    "<td>0.0243</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Location Features on Title and Body Length</td>\n",
    "<td>0.5207</td>\n",
    "<td>0.5157</td>\n",
    "<td>0.0252</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Parts of Speech</td>\n",
    "<td>0.5530</td>\n",
    "<td>0.5513</td>\n",
    "<td>0.0167</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Extra Trees of Subreddits</td>\n",
    "<td>0.5586</td>\n",
    "<td>0.5581</td>\n",
    "<td>0.0262</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<a href=\"#top\">Return to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part12\"></a>\n",
    "## 12. Notes On Error Analysis\n",
    "\n",
    "We built tools for error analysis and a few of the features above came out of that analysis.\n",
    "For example, we noticed that the intresting words and incorpus counting features massively preferred long requests. This led to us dividing the counts by the length of requests, sort if in the fashion of TFIDF calculations, which led to better results for those features.\n",
    "\n",
    "We also noded that a lot of the succesful requests we failed to identify had location names in them and looking on the Reddit group noticed that location was a requirement for fulfillment (which, logistically, is obvious in hindsight). This inpsired us to create our geographic word identificaiton feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tv = TfidfVectorizer(tokenizer=SnowballStemTokenizer())\n",
    "l1 = LinearWeightFeatureThreshold(C=0.3)\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators = 200,\n",
    "                           max_depth = 4,\n",
    "                           min_samples_split=15,\n",
    "                           random_state=rseed,\n",
    "                           class_weight='auto')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('body', Pipeline([('extract', ExtractBody()),\n",
    "         ('tv',tv),\n",
    "         ('features',l1)\n",
    "         ])),\n",
    "    ('model', etc)\n",
    "])  \n",
    "                    \n",
    "\n",
    "# Number of errors to examine per fold. Keep the number negative to find the biggest error cases.\n",
    "# Keep in mind that this will print 10x this many error cases.\n",
    "\n",
    "TITLE_COLUMN = np.where(all_train_df.columns == 'request_title')[0][0]\n",
    "BODY_COLUMN = np.where(all_train_df.columns == 'request_text_edit_aware')[0][0]\n",
    "\n",
    "\n",
    "num_errors_per_fold = -1\n",
    "\n",
    "check_kf = kf\n",
    "\n",
    "for train_index, test_index in check_kf:\n",
    "    X_train, X_test = all_train_df.values[train_index], all_train_df.values[test_index]\n",
    "    y_train, y_test = all_train_labels[train_index], all_train_labels[test_index]\n",
    "    \n",
    "    print X_train.shape\n",
    "    print X_test.shape\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    cl_probs = pipe.predict_proba(X_test)\n",
    "\n",
    "    # Loop through this fold of test data and determine the R ratio for each one:\n",
    "    ratios = []\n",
    "    for i in range(0, cl_probs.shape[0]):\n",
    "        ratios.append(cl_probs[i].max() / cl_probs[i][y_test[i]])\n",
    "\n",
    "    # Find the 3 largest ratios and print them as error cases to examine:\n",
    "    ratios = np.asarray(ratios)\n",
    "    heavy_ratios = ratios.argsort()[num_errors_per_fold:][::-1]\n",
    "    for r in heavy_ratios:\n",
    "        print \"== We guessed %s for this, but it was actually %s. ==\" % (np.argmax(cl_probs[r]),\n",
    "                                                                         y_test[r])\n",
    "        print \"\\n\", X_test[r, TITLE_COLUMN]\n",
    "        print \"\\n\", X_test[r, BODY_COLUMN]\n",
    "        print \"\\n==========\\n\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part13\"></a>\n",
    "## 13. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Thinker\n",
    "\n",
    "We used this function to easily explore our text and generate ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TheThinker(n):\n",
    "    bodies = ExtractBody().transform(all_train_df)\n",
    "    titles = ExtractTitle().transform(all_train_df)\n",
    "    username = ExtractUser().transform(all_train_df)\n",
    "    y = all_train_labels\n",
    "\n",
    "    choices = np.random.choice(np.arange(n_all),n)\n",
    "    for i in choices:\n",
    "        print '###########################'\n",
    "        if y[i]:\n",
    "            print 'SUCCESS'\n",
    "        else:\n",
    "            print 'FAILURE'\n",
    "\n",
    "        print 'User:', username[i]\n",
    "\n",
    "        print 'Title:'\n",
    "        print titles[i]\n",
    "\n",
    "        print 'Body:'\n",
    "        print bodies[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TheThinker(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Theory\n",
    "\n",
    "We used this transformer to create simple models that look for the occurence of strings in text. This was useful for testing hunches about significance when exploring our data set and doing error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CheckWordsTransformer(TransformerMixin):\n",
    "    def __init__(self, words=[]):\n",
    "        self.words = words\n",
    "        \n",
    "    def find_words(self, words, text, lower=True):\n",
    "        word_dict = {}\n",
    "\n",
    "        for word in words:\n",
    "            if lower:\n",
    "                has_word = np.array([(1 if word in t.lower() else 0) for t in text])\n",
    "            else:\n",
    "                has_word = np.array([(1 if word in t else 0) for t in text])\n",
    "            word_dict[word] = has_word\n",
    "\n",
    "        return word_dict\n",
    "    \n",
    "    # manually create keywords with categories\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        words = self.words\n",
    "        find_words = self.find_words\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        if len(X.shape) > 1:\n",
    "            cols = X.shape[1]\n",
    "            for i in cols:\n",
    "                lens = lenArray(X[:,i])\n",
    "                features.append(pd.DataFrame(find_words(words, X[:,i])).values/lens)\n",
    "        else:\n",
    "            lens = lenArray(X)\n",
    "            features.append(pd.DataFrame(find_words(words, X)).values/lens)\n",
    "            \n",
    "        return np.hstack(tuple(features))\n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        #do nothing\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {'words': self.words}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the string 'thank' is pretty explanatory... is it really a magic word?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strings = ['thank']\n",
    "pipe = Pipeline([('text',all_text),('strings',CheckWordsTransformer(words=strings)),('model',etc)])\n",
    "strings_cv = cross_val_score(pipe, all_train_df.values, all_train_labels, cv=kf_over, scoring=roc_scorer)\n",
    "print_scores(strings_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
